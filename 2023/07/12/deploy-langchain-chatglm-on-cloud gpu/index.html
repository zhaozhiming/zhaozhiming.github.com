<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
    
	<title>部署 LangChain 和 ChatGLM2 来打造自有知识库问答系统 - Hacker and Geeker&#39;s Way</title>
    <meta name="author" content="">
    
	<meta name="description" content="在GPU服务器上部署 LangChain 和 ChatGLM2 来打造自有知识库问答系统"> <!-- TODO: truncate -->
	<meta name="keywords" content="langchain, chatglm2">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="atom.xml" rel="alternate" title="Hacker and Geeker&#39;s Way" type="application/atom+xml">
	<link href="/favicon.ico" rel="shortcut icon">
    <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/custom.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/hljs.css" media="screen, projection" rel="stylesheet" type="text/css">

    <link href='/stylesheets/font.css?family=Slackey' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Fjalla+One' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Amethysta+One' rel='stylesheet' type='text/css'>
	  <script src="/javascripts/jquery.min.js"></script>
    <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![}]-->

    <script type="text/javascript" src="/javascripts/jquery-tapir.js"></script>

    <!-- remove or comment it to disable ajaxification -->   
    <!-- <script src="/javascripts/ajaxify.js"></script> -->

    

    
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-100485541-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>


<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <div id="wrapper">
    <header id="header" class="inner"><!-- for more effects see _animate.scss -->
<h1 class="animated bounceInDown">
    <div id="headerbg">
        Hacker and Geeker&#39;s Way
    </div>
</h1>
<span class="subtitle"></span>
<br>

<ul id="social-links" style="text-align:center; clear:both;">
  
  <!-- GitHub -->
  <li>
  <a target="_blank" rel="noopener" href="https://github.com/zhaozhiming" class="github" title="Github"></a>
  </li>
  
  
  
  
  <!-- Twitter -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.twitter.com/kingzzm" class="twitter" title="Twitter"></a>
  </li>
  
  
  <!-- LinkedIn -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.linkedin.com/in/zhaozhiming" class="linkedin" title="LinkedIn"></a>
  </li>
  
  
  
  
  
  <!-- Stackoverflow -->
  <li>
  <a target="_blank" rel="noopener" href="http://stackoverflow.com/users/1954315/zhaozhiming" class="stackoverflow" title="Stackoverflow"></a>
  </li>
  
</ul>


<!-- use full url including 'index.html' for navigation bar if you are using ajax -->
<ul id="nav">
	<li id="ajax"><a href="/index.html">Home</a></li>
	<li id="ajax"><a href="/archives/index.html">Archives</a></li>
    <li><a href="/atom.xml">RSS</a></li>
    <li><a href="/about/index.html">About</a></li>
    
    <li>
    <div id="dark">
        <form action="//www.google.com.hk/search" method="get" accept-charset="UTF-8" id="search">
            <input type="hidden" name="sitesearch" value="https://zhaozhiming.github.io" />
            <input type="text" name="q" results="0" placeholder="Search..." x-webkit-speech />
        </form>
    </div>
    </li>
        
</ul>




</header>


<div id="toload">
<!-- begin toload -->
    <div id="content">
        <div class="inner">
<article class="post">
	<h2 class="title">部署 LangChain 和 ChatGLM2 来打造自有知识库问答系统</h2>
    <div class="meta">
        <div class="date">Published on: <time datetime="2023-07-12T08:35:10.000Z" itemprop="datePublished">7月 12, 2023</time>
</div>
        <div class="tags">Tags: 

<a href="/tags/langchain/">langchain</a> <a href="/tags/chatglm2/">chatglm2</a>
</div>
    </div>
	<div class="entry-content"><img src="/images/post/2023/07/embedding-docs-with-llm.png" class="" width="400" height="300">

<p>随着人工智能技术的迅猛发展，问答机器人在多个领域中展示了广泛的应用潜力。在这个信息爆炸的时代，许多领域都面临着海量的知识和信息，人们往往需要耗费大量的时间和精力来搜索和获取他们所需的信息。在这种情况下，垂直领域的 AI 问答机器人应运而生。OpenAI 的 GPT3.5 和 GPT4 无疑是目前最好的 LLM（大语言模型），借助 OpenAI 的 GPT 确实可以快速地打造出一个高质量的 AI 问答机器人，但是 GPT 在实际应用上存在着不少限制。比如 ChatGPT 的知识库是通用领域的，对于垂直领域的知识理解有限，而且对于不熟悉的知识还会存在<code>幻觉</code>的问题。另外 GPT 的训练语料大部分是英文的，对于中文的理解也存在一定的问题，这对于国内公司来说是一个很大的问题。本文将介绍如何使用中文 LLM—— ChatGLM 结合 LangChain 来打造一个垂直领域的知识库问答系统，并在云 GPU 服务上部署运行。</p>
<span id="more"></span>

<h2 id="GPU-服务器选择"><a href="#GPU-服务器选择" class="headerlink" title="GPU 服务器选择"></a>GPU 服务器选择</h2><p>如果想要在机器上跑自己部署的 LLM，那么你至少需要一台配置不错的 GPU 服务器，否则推理的速度会很慢。想要拥有 GPU 服务器，你可以选择购买或者租用。如果你有足够的资金，那么可以选择购买一台 GPU 服务器，如果你的资金有限，那么可以选择租用云 GPU 服务器。本文主要以租用云 GPU 服务器为例来讲解内容。</p>
<p>随着 AI 的火热，国内出现各种云 GPU 服务厂商，老牌的国内各大云厂商也都纷纷支持 GPU 服务器，经过笔者的试用，发现 AutoDL 和阿里云的 GPU 服务器性价比最高，而且系统也比较稳定且无需绑定额外的开发框架，所以笔者比较推荐这两个云服务器厂商。</p>
<h3 id="阿里云"><a href="#阿里云" class="headerlink" title="阿里云"></a>阿里云</h3><p>先说说为什么推荐阿里云的 GPU 服务器，这是因为他们推出了一个<a target="_blank" rel="noopener" href="https://free.aliyun.com/?searchKey=PAI&spm=5176.22772544.J_4237718650.1.14c32ea9Ojx8Dh">免费试用计划</a>，可以让开发者免费试用 <code>5000CU*H</code> 的 GPU，试用时间为 3 个月，支持 A10&#x2F;V100&#x2F;G6 3 种显卡机型。</p>
<img src="/images/post/2023/07/aliyun-pai.png" class="" width="1000" height="700">

<p>不免费的话阿里云的 GPU 服务器还是比较贵的，V100 16G 的服务器一个小时要大约 30 块钱，还不包括存储的费用。免费的额度使用完后，建议找其他的云服务器厂商。</p>
<h3 id="AutoDL"><a href="#AutoDL" class="headerlink" title="AutoDL"></a>AutoDL</h3><p><a target="_blank" rel="noopener" href="https://www.autodl.com/home">AutoDL</a>是一款新晋的云 GPU 深度学习环境出租平台，相比阿里云的 GPU 服务器价格，AutoDL 的价格可谓相当亲民。</p>
<p>以 V100 服务器为例，阿里云的 V100 16G 的服务器收费大概是 30 元每小时且不包存储，而 AutoDL 的 V100 32G 服务器仅需 2.4 元每小时，还送 50GB 的存储，价格是阿里云的十分之一不到，配置还更高。</p>
<img src="/images/post/2023/07/autodl.png" class="" width="1000" height="700">

<p>所以我建议的云 GPU 服务器使用策略是：先免费试用阿里云三个月的 GPU 服务器，然后再使用 AutoDL 的 GPU 服务器。如果你有更加划算的 GPU 服务器推荐，欢迎在评论区留言。</p>
<h2 id="创建实例"><a href="#创建实例" class="headerlink" title="创建实例"></a>创建实例</h2><p>无论你是使用阿里云还是 AutoDL，都需要先创建一个服务器实例，创建实例按照官方说明文档即可，需要注意的是实例的镜像选择，笔者选择的是 PyTorch 1.1x + Ubuntu 20.04 的镜像，这个镜像已经安装了 PyTorch 和 CUDA ，使用起来也比较稳定，其他的镜像部署过程中可能会出现各种问题。</p>
<p>另外建议选择显存至少为 16GB 的显卡，因为 ChatGLM2-6B 的推理需要至少 12GB 的显存，再加上 LangChain 其他模型的显存消耗，所以至少要保证 16GB 的显存才能进行后面的部署工作。</p>
<h2 id="部署-ChatGLM2"><a href="#部署-ChatGLM2" class="headerlink" title="部署 ChatGLM2"></a>部署 ChatGLM2</h2><p>我们先部署 ChatGLM2，ChatGLM2 是开源中英双语对话模型的第二代，对中文的处理能力要优于其他大模型，比第一代有了更强大的性能，更长的上下文，更高效的推理，更开放的协议。</p>
<h3 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h3><p>开始克隆仓库并安装依赖，在阿里云和 AutoDL 上安装 python 依赖包时，系统默认的是使用阿里的源，安装速度会比较慢，这里我推荐用百度的源，速度快非常多。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/THUDM/ChatGLM2-6B.git</span><br><span class="line"><span class="built_in">cd</span> ChatGLM2-6B</span><br><span class="line">pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple <span class="comment"># 用百度的源</span></span><br></pre></td></tr></table></figure>

<h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>接着是下载模型，这里我使用的是 ChatGLM2-6B 的模型，模型文件是存放在 HuggingFace 上面，最近国内连接 HuggingFace 经常会不通，不清楚是不是被墙了，所以如果下载过程中遇到问题，可以尝试<strong>使用科学上网</strong>进行代理。下面介绍两种下载方法。</p>
<h4 id="自动下载"><a href="#自动下载" class="headerlink" title="自动下载"></a>自动下载</h4><p>第一种是直接运行<code>web_demo.py</code>，这样程序在执行时会先检查本地是否有模型，没有的话就会自动下载模型，下载完成后再开始执行程序，下载下来的模型存放在这个目录：<code>~/.cache/huggingface/hub/models--THUDM--chatglm2-6b/blobs</code>，模型大小大概有 10G 左右。</p>
<h4 id="手动下载"><a href="#手动下载" class="headerlink" title="手动下载"></a>手动下载</h4><p>第二种是使用<code>git clone</code>下载模型，因为模型比较大，所以要先确认 git 的大文件下载功能是否已开启，执行命令<code>git lfs install</code>，如果显示<code>Git LFS initialized.</code>则说明已开启，否则需要先安装<code>Git LFS</code>，可以在<a target="_blank" rel="noopener" href="https://github.com/git-lfs/git-lfs?utm_source=gitlfs_site&utm_medium=installation_link&utm_campaign=gitlfs#installing">这个网站</a>查看如何安装。</p>
<p>然后执行<code>git clone</code>命令下载模型。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://huggingface.co/THUDM/chatglm2-6b</span><br></pre></td></tr></table></figure>

<p>手动下载模型需要修改<code>web_demo.py</code>文件中的模型路径，将<code>from_pretrained</code>中的模型路径修改为下载后的模型路径。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="deletion">-tokenizer = AutoTokenizer.from_pretrained(&quot;THUDM/chatglm2-6b&quot;, trust_remote_code=True)</span></span><br><span class="line"><span class="deletion">-model = AutoModel.from_pretrained(&quot;THUDM/chatglm2-6b&quot;, trust_remote_code=True, device=&#x27;cuda&#x27;)</span></span><br><span class="line"><span class="addition">+tokenizer = AutoTokenizer.from_pretrained(&quot;/mnt/workspace/chatglm2-6b&quot;, trust_remote_code=True)</span></span><br><span class="line"><span class="addition">+model = AutoModel.from_pretrained(&quot;/mnt/workspace/chatglm2-6b&quot;, trust_remote_code=True).quantize(4).cuda()</span></span><br></pre></td></tr></table></figure>

<h3 id="部署验证"><a href="#部署验证" class="headerlink" title="部署验证"></a>部署验证</h3><p>最后我们希望将 ChatGLM2 运行起来，看安装是否成功，ChatGLM2 提供了一个 web 程序，启动这个程序后我们可以在浏览器中访问它的功能，web 程序的访问端口是 7860，但我们的服务器不一定会开放这个端口对外提供访问，因此我们还需要再改下<code>web_demo.py</code>的代码，来让它可以提供外部访问的地址。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="deletion">-demo.queue().launch(share=False, inbrowser=True)</span></span><br><span class="line"><span class="addition">+demo.queue().launch(share=True, inbrowser=True)</span></span><br></pre></td></tr></table></figure>

<p>将<code>share</code>设置为 True，这样程序启动后会生成一个对外访问地址，我们在本地浏览器直接访问这个地址就可以了。执行命令<code>python web_demo.py</code>启动 web 程序。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/mnt/workspace/ChatGLM2-6B&gt; python web_demo.py</span><br><span class="line">[2023-07-13 11:20:33,304] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)</span><br><span class="line">Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████| 7/7 [01:25&lt;00:00, 12.25s/it]</span><br><span class="line">Running on <span class="built_in">local</span> URL:  http://127.0.0.1:7860</span><br><span class="line">Running on public URL: https://46841e4217313f9dbb.gradio.live</span><br><span class="line"></span><br><span class="line">This share <span class="built_in">link</span> expires <span class="keyword">in</span> 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces</span><br></pre></td></tr></table></figure>

<p><code>public URL</code>就是我们要访问的地址，需要注意的是这个地址不是一成不变的，而是每次启动都会生成新的地址。下面是 web 程序的界面。</p>
<img src="/images/post/2023/07/chatglm2.png" class="" width="600" height="400">

<h2 id="部署-LangChain-ChatGLM"><a href="#部署-LangChain-ChatGLM" class="headerlink" title="部署 LangChain-ChatGLM"></a>部署 LangChain-ChatGLM</h2><p>接下来我们再部署 LangChain-ChatGLM，首先介绍一下 LangChain 和 LangChain-ChatGLM 这 2 个项目。</p>
<p><a target="_blank" rel="noopener" href="https://docs.langchain.com/docs/">LangChain</a>是一个强大的框架，旨在帮助开发人员使用语言模型构建端到端的应用程序。它提供了一套工具、组件和接口，可简化创建由 LLM 提供支持的应用程序的过程。LangChain 可以轻松管理与语言模型的交互，将多个组件链接在一起，并集成额外的资源，例如 API 和数据库。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/imClumsyPanda/langchain-ChatGLM">Langchain-ChatGLM</a> 是一种利用 Langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。它对中文文档分隔，问题加上下文拼接上都做了相应的优化。</p>
<p>Langchain-ChatGLM 开始是基于 ChatGLM 第一代来开发的，后面慢慢支持更多的模型，ChatGLM2 推出后，项目也很快集成了 ChatGLM2。</p>
<h3 id="克隆仓库-1"><a href="#克隆仓库-1" class="headerlink" title="克隆仓库"></a>克隆仓库</h3><p>和ChatGLM2一样，我们首先要下载项目代码并安装依赖。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/imClumsyPanda/langchain-ChatGLM.git </span><br><span class="line"><span class="built_in">cd</span> langchain-ChatGLM </span><br><span class="line">pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple <span class="comment"># 用百度的源</span></span><br></pre></td></tr></table></figure>

<h3 id="下载-Embedding-模型"><a href="#下载-Embedding-模型" class="headerlink" title="下载 Embedding 模型"></a>下载 Embedding 模型</h3><p>Langchain-ChatGLM 除了要用到 ChatGLM2 模型外，还需要用到一个用来 Embedding 文档和 Prompt 的模型，默认是用的<code>text2vec-large-chinese</code>，但这个模型推理用的显存比较大，大概要 3G 多显存，我们可以使用一个小一点的模型<code>text2vec-base-chinese</code>。</p>
<p>同样地，我们只需要用<code>git clone</code>下载模型。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://huggingface.co/shibing624/text2vec-base-chinese</span><br></pre></td></tr></table></figure>

<h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>模型下载完成后修改配置文件<code>config/model_config.py</code>，将<code>embedding_model_dict</code>中的模型地址改为下载后的 Embedding 模型地址，同时修改默认的 Embedding 模型名称为<code>text2vec-base</code>。同时修改<code>llm_model_dict</code>中<code>chatglm2-6b</code>的模型地址为我们下载的 ChatGLM2 模型地址，并且修改默认的 LLM 模型为<code>chatglm2-6b</code>。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> embedding_model_dict = &#123;</span><br><span class="line">     &quot;ernie-tiny&quot;: &quot;nghuyong/ernie-3.0-nano-zh&quot;,</span><br><span class="line">     &quot;ernie-base&quot;: &quot;nghuyong/ernie-3.0-base-zh&quot;,</span><br><span class="line"><span class="deletion">-    &quot;text2vec-base&quot;: &quot;shibing624/text2vec-base-chinese&quot;,</span></span><br><span class="line"><span class="addition">+    &quot;text2vec-base&quot;: &quot;/mnt/workspace/text2vec-base-chinese&quot;,</span></span><br><span class="line"></span><br><span class="line"> # Embedding model name</span><br><span class="line"><span class="deletion">-EMBEDDING_MODEL = &quot;text2vec&quot;</span></span><br><span class="line"><span class="addition">+EMBEDDING_MODEL = &quot;text2vec-base&quot;</span></span><br><span class="line"></span><br><span class="line">     &quot;chatglm2-6b&quot;: &#123;</span><br><span class="line">         &quot;name&quot;: &quot;chatglm2-6b&quot;,</span><br><span class="line"><span class="deletion">-        &quot;pretrained_model_name&quot;: &quot;THUDM/chatglm2-6b&quot;,</span></span><br><span class="line"><span class="addition">+        &quot;pretrained_model_name&quot;: &quot;/mnt/workspace/chatglm2-6b&quot;,</span></span><br><span class="line"></span><br><span class="line"> # LLM 名称</span><br><span class="line"><span class="deletion">-LLM_MODEL = &quot;chatglm-6b&quot;</span></span><br><span class="line"><span class="addition">+LLM_MODEL = &quot;chatglm2-6b&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="针对-ChatGLM2-的修改"><a href="#针对-ChatGLM2-的修改" class="headerlink" title="针对 ChatGLM2 的修改"></a>针对 ChatGLM2 的修改</h3><p>因为 ChatGLM2 刚推出不久，Langchain-ChatGLM 对其适配还不太完善，需要手动再修改一些代码才能让 ChatGLM2 正常运行。修改的代码主要是<code>/models/loader/loader.py</code>文件中的<code>_load_model</code>方法，修改内容如下。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">                         LoaderClass.from_pretrained(checkpoint,</span><br><span class="line">                                                     config=self.model_config,</span><br><span class="line">                                                     torch_dtype=torch.bfloat16 if self.bf16 else torch.float16,</span><br><span class="line"><span class="deletion">-                                                    trust_remote_code=True)</span></span><br><span class="line"><span class="deletion">-                        .half()</span></span><br><span class="line"><span class="deletion">-                        .cuda()</span></span><br><span class="line"><span class="addition">+                                                    trust_remote_code=True, device=&#x27;cuda&#x27;)</span></span><br><span class="line"><span class="addition">+                        # .half()</span></span><br><span class="line"><span class="addition">+                        # .cuda()</span></span><br></pre></td></tr></table></figure>

<h3 id="部署运行"><a href="#部署运行" class="headerlink" title="部署运行"></a>部署运行</h3><p>最后跟 ChatGLM2 一样，我们修改 Langchain-ChatGLM 的 web 程序，以便我们可以在浏览器中访问到这个 web 程序。修改<code>webui.py</code>文件，将<code>share</code>设置为 True。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="deletion">-         share=False,</span></span><br><span class="line"><span class="addition">+         share=True,</span></span><br></pre></td></tr></table></figure>

<p>修改完后执行命令<code>python webui.py</code>启动 web 程序。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/mnt/workspace/langchain-ChatGLM&gt; python webui.py</span><br><span class="line">INFO  2023-07-13 14:37:52,071-1d:</span><br><span class="line">loading model config</span><br><span class="line">llm device: cuda</span><br><span class="line">embedding device: cuda</span><br><span class="line"><span class="built_in">dir</span>: /mnt/workspace/langchain-ChatGLM</span><br><span class="line">flagging username: e717a58b4ce9444e82491cefeb80bf56</span><br><span class="line"></span><br><span class="line">[2023-07-13 14:37:58,105] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)</span><br><span class="line">Loading /mnt/workspace/chatglm2-6b...</span><br><span class="line">Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████| 7/7 [01:29&lt;00:00, 12.80s/it]</span><br><span class="line">Loaded the model <span class="keyword">in</span> 91.42 seconds.</span><br><span class="line">&#123;<span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;你好！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。&#x27;</span>&#125;</span><br><span class="line">Running on <span class="built_in">local</span> URL:  http://0.0.0.0:7860</span><br><span class="line">Running on public URL: https://b0f2e23a9ea5b74b4a.gradio.live</span><br><span class="line"></span><br><span class="line">This share <span class="built_in">link</span> expires <span class="keyword">in</span> 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces</span><br></pre></td></tr></table></figure>

<p>访问<code>public URL</code>地址就可以看到我们的 web 程序了。</p>
<img src="/images/post/2023/07/langchain-chatglm.png" class="" width="600" height="400">

<h3 id="系统使用介绍"><a href="#系统使用介绍" class="headerlink" title="系统使用介绍"></a>系统使用介绍</h3><p>添加知识库文档步骤如下：</p>
<ul>
<li>先选新建知识库</li>
<li>输入知识库名字，点击“添加至知识选项”</li>
<li>上传文件，完了后点击“上传文件并加载知识库”</li>
</ul>
<p>然后就可以基于这个知识库进行问答了，操作十分简单。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了 GPU 服务器的选型，ChatGLM2 和 Langchain-ChatGLM 的部署和使用，如果想要了解更多关于 Langchain-ChatGLM 的实现原理，可以在他们 GitHub 仓库中查看更多的信息。如果你想打造属于自身业务的问答系统，可以参考本文的方法，希望可以帮助到你。</p>
<p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>
</div>

</article>
<section id="appreciates">
  <center>
	<h2>赞赏</h2>
    <div class="post-footer">
      <div>
	    <h4>如果文章对您有所帮助，可以捐赠我喝杯咖啡😌，捐赠方式：</h4>
        <div class="digital-wallet">
          <h6>BTC 地址：3LYgSyf7ddMALwGWPQr3PY4wzjsTDdg1oV</h6>
          <h6>ETH 地址：0x0C9b27c89A61aadb0aEC24CA8949910Cbf77Aa73</h6>
        </div>
        <div class="wechat_appreciates">
          <img src="/images/wechat_appreciates.png" alt="qrcode" width="250" >
        </div>
      </div>
      <div>
	    <h4>文章已同步更新公众号，欢迎关注</h4>
        <div class="wechat_appreciates">
          <img src="/images/wxgzh_qrcode.jpg" alt="qrcode" width="250" >
        </div>
      </div>
    </div>
  </center>
</section>



    
      <script type="text/javascript">
        var disqus_config = function () {
            this.page.url = 'https://zhaozhiming.github.io/2023/07/12/deploy-langchain-chatglm-on-cloud%20gpu/';
            this.page.identifier = 'https://zhaozhiming.github.io/2023/07/12/deploy-langchain-chatglm-on-cloud%20gpu/';
        };

        (function() {
          var d = document, s = d.createElement('script');
          s.src = '//zhaozhiming.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    



<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>

</div>

    </div>
    <footer id="footer">
    <div style="display:inline">
    Copyright &copy; 2025

    赵芝明
. Powered by <a href="http://zespia.tw/hexo/" target="_blank">Hexo</a> |
    Theme is <a target="_blank" rel="noopener" href="https://github.com/wd/hexo-fabric">hexo-fabric</a>, fork from <a target="_blank" rel="noopener" href="http://github.com/panks/fabric">fabric</a> by <a target="_blank" rel="noopener" href="http://panks.me">Pankaj Kumar</a>
</div>


    </footer>
    <script src="/javascripts/fabric.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script>
 <!-- Delete or comment this line to disable Fancybox -->



<!-- end toload --> 
</div>
</div>
<script src="/javascripts/jquery.ui.totop.js" type="text/javascript"></script>
<script type="text/javascript">
/*<![CDATA[*/
;(function($){$().UItoTop({easingType:'easeOutCirc'});})(jQuery); 
/*]]>*/
</script><!-- remove it to remove the scroll to top button -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5608723650603289" crossorigin="anonymous"></script>
</body>
</html>
