<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
    
	<title>使用 LLaMA Factory 进行大语言模型微调 - Hacker and Geeker&#39;s Way</title>
    <meta name="author" content="">
    
	<meta name="description" content="使用 LLaMA Factory 进行大语言模型微调"> <!-- TODO: truncate -->
	<meta name="keywords" content="llm, fine-tuning, llama_factory">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="atom.xml" rel="alternate" title="Hacker and Geeker&#39;s Way" type="application/atom+xml">
	<link href="/favicon.ico" rel="shortcut icon">
    <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/custom.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/hljs.css" media="screen, projection" rel="stylesheet" type="text/css">

    <link href='/stylesheets/font.css?family=Slackey' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Fjalla+One' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Amethysta+One' rel='stylesheet' type='text/css'>
	  <script src="/javascripts/jquery.min.js"></script>
    <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![}]-->

    <script type="text/javascript" src="/javascripts/jquery-tapir.js"></script>

    <!-- remove or comment it to disable ajaxification -->   
    <!-- <script src="/javascripts/ajaxify.js"></script> -->

    

    
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-100485541-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>


<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <div id="wrapper">
    <header id="header" class="inner"><!-- for more effects see _animate.scss -->
<h1 class="animated bounceInDown">
    <div id="headerbg">
        Hacker and Geeker&#39;s Way
    </div>
</h1>
<span class="subtitle"></span>
<br>

<ul id="social-links" style="text-align:center; clear:both;">
  
  <!-- GitHub -->
  <li>
  <a target="_blank" rel="noopener" href="https://github.com/zhaozhiming" class="github" title="Github"></a>
  </li>
  
  
  
  
  <!-- Twitter -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.twitter.com/kingzzm" class="twitter" title="Twitter"></a>
  </li>
  
  
  <!-- LinkedIn -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.linkedin.com/in/zhaozhiming" class="linkedin" title="LinkedIn"></a>
  </li>
  
  
  
  
  
  <!-- Stackoverflow -->
  <li>
  <a target="_blank" rel="noopener" href="http://stackoverflow.com/users/1954315/zhaozhiming" class="stackoverflow" title="Stackoverflow"></a>
  </li>
  
</ul>


<!-- use full url including 'index.html' for navigation bar if you are using ajax -->
<ul id="nav">
	<li id="ajax"><a href="/index.html">Home</a></li>
	<li id="ajax"><a href="/archives/index.html">Archives</a></li>
    <li><a href="/atom.xml">RSS</a></li>
    <li><a href="/about/index.html">About</a></li>
    
    <li>
    <div id="dark">
        <form action="//www.google.com.hk/search" method="get" accept-charset="UTF-8" id="search">
            <input type="hidden" name="sitesearch" value="https://zhaozhiming.github.io" />
            <input type="text" name="q" results="0" placeholder="Search..." x-webkit-speech />
        </form>
    </div>
    </li>
        
</ul>




</header>


<div id="toload">
<!-- begin toload -->
    <div id="content">
        <div class="inner">
<article class="post">
	<h2 class="title">使用 LLaMA Factory 进行大语言模型微调</h2>
    <div class="meta">
        <div class="date">Published on: <time datetime="2023-10-27T02:02:06.000Z" itemprop="datePublished">10月 27, 2023</time>
</div>
        <div class="tags">Tags: 

<a href="/tags/llm/">llm</a> <a href="/tags/fine-tuning/">fine-tuning</a> <a href="/tags/llama-factory/">llama_factory</a>
</div>
    </div>
	<div class="entry-content"><img src="/images/post/2023/10/llama-factory.png" class="" width="400" height="300">

<p>LLM（大语言模型）微调一直都是老大难问题，不仅因为微调需要大量的计算资源，而且微调的方法也很多，要去尝试每种方法的效果，需要安装大量的第三方库和依赖，甚至要接入一些框架，可能在还没开始微调就已经因为环境配置而放弃了。今天我们来介绍一个可以帮助大家快速进行 LLM 微调的工具——LLaMA Factory，它可以帮助大家快速进行 LLM 微调，而且还可以在微调过程中进行可视化，非常方便。</p>
<span id="more"></span>

<h2 id="什么是-LLM-微调"><a href="#什么是-LLM-微调" class="headerlink" title="什么是 LLM 微调"></a>什么是 LLM 微调</h2><p>LLM 微调，也叫做 Fine-tuning，是深度学习领域中常见的一种技术，用于将预先训练好的模型适配到特定的任务或数据集上。这个过程包括几个主要步骤：</p>
<ul>
<li>基础模型选择：选择一个通用文本数据的基础语言模型，使其能够理解基本的语言结构和语义。</li>
<li>准备训练数据集：选择一个与目标任务相关的较小数据集。</li>
<li>微调：在此数据集上训练模型，但通常使用较低的学习率，以保留基础模型学到的知识，同时学习目标任务的特定知识。</li>
<li>评估：在目标任务的验证集上评估模型的性能，需要准备评估数据集。</li>
<li>应用：如果性能满意，则可以将模型应用于实际任务。</li>
</ul>
<p>这种方法的优势在于，通过微调可以快速并且以较低的计算成本将模型适配到特定任务，而不需要从头开始训练模型。同时，由于预训练模型已经学到了很多通用的语言知识，微调通常能够获得不错的性能。</p>
<h2 id="前沿的微调策略"><a href="#前沿的微调策略" class="headerlink" title="前沿的微调策略"></a>前沿的微调策略</h2><ul>
<li>LoRA：LoRA 是一种用于微调大型语言模型的技术，通过低秩近似方法降低适应数十亿参数模型（如 GPT-3）到特定任务或领域的计算和财务成本。</li>
<li>QLoRA：QLoRA 是一种高效的大型语言模型微调方法，它显著降低了内存使用量，同时保持了全 16 位微调的性能。它通过在一个固定的、4 位量化的预训练语言模型中反向传播梯度到低秩适配器来实现这一目标。</li>
<li>PEFT：PEFT 是一种 NLP 技术，通过仅微调一小部分参数，高效地将预训练的语言模型适应到各种应用，降低计算和存储成本。它通过调整特定任务的关键参数来对抗灾难性遗忘，并在多种模式（如图像分类和稳定扩散梦展台）中提供与全微调相当的性能。这是一种在最少的可训练参数情况下实现高性能的有价值方法。</li>
</ul>
<p>目前 LLM 微调的最佳实践是采用 LoRA 或 QLoRA 策略进行 LLM 微调。</p>
<h2 id="LLaMA-Factory-介绍"><a href="#LLaMA-Factory-介绍" class="headerlink" title="LLaMA Factory 介绍"></a>LLaMA Factory 介绍</h2><p><a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">LLaMA Factory</a>是一个 LLM 微调工具，支持预训练（Pre-Training）、指令监督微调（Supervised Fine-Tuning）、奖励模型训练（Reward Modeling）等训练方式，每种方式都支持 LoRA 和 QLoRA 微调策略。它的前身是<a target="_blank" rel="noopener" href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning">ChatGLM-Efficient-Turning</a>，是基于 ChatGLM 模型做的一个微调工具，后面慢慢支持了更多的 LLM 模型，包括 BaiChuan，QWen，LLaMA 等，于是便诞生了 LLaMA Factory。</p>
<p>它的特点是支持的模型范围较广（主要包含大部分中文开源 LLM），集成业界前沿的微调方法，提供了微调过程中需要用到的常用数据集，最重要的一点是它提供了一个 WebUI 页面，让非开发人员也可以很方便地进行微调工作。</p>
<h2 id="部署安装"><a href="#部署安装" class="headerlink" title="部署安装"></a>部署安装</h2><p>LLaMA Factory 的部署安装非常简单，只需要按照官方仓库中的步骤执行即可，执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 克隆仓库</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/hiyouga/LLaMA-Factory.git</span><br><span class="line"><span class="comment"># 创建虚拟环境</span></span><br><span class="line">conda create -n llama_factory python=3.10</span><br><span class="line"><span class="comment"># 激活虚拟环境</span></span><br><span class="line">conda activate llama_factory</span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line"><span class="built_in">cd</span> LLaMA-Factory</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p>接下来是下载 LLM，可以选择自己常用的 LLM，包括 ChatGLM，BaiChuan，QWen，LLaMA 等，这里我们下载 BaiChuan 模型进行演示：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一：开启 git lfs 后直接 git clone 仓库</span></span><br><span class="line">git lfs install</span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二：先下载仓库基本信息，不下载大文件，然后再通过 huggingface 上的文件链接下载大文件</span></span><br><span class="line">GIT_LFS_SKIP_SMUDGE=1 git <span class="built_in">clone</span> https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat</span><br><span class="line"><span class="built_in">cd</span> Baichuan2-13B-Chat</span><br><span class="line">wget <span class="string">&quot;https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00001-of-00003.bin&quot;</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>方法一的方式会将仓库中的 git 记录一并下载，导致下载下来的文件比较大，建议是采用方法二的方式，速度更快整体文件更小。</p>
<p><strong>注意点：</strong></p>
<ul>
<li>如果你是使用 <a target="_blank" rel="noopener" href="https://www.autodl.com/home">AutoDL</a> 进行部署的话，在<code>conda activate llama_factory</code>之前需要先执行一下<code>conda init bash</code>命令来初始化一下 conda 环境，然后重新打开一个终端窗口，再执行<code>conda activate llama_factory</code>命令。</li>
<li>如果使用的是 BaiChuan 的模型，需要修改<code>transformers</code>的版本为<code>4.33.2</code>，否则会报<code>AttributeError: &#39;BaichuanTokenizer&#39; object has no attribute &#39;sp_model&#39;</code>的错误。</li>
</ul>
<h2 id="开始微调"><a href="#开始微调" class="headerlink" title="开始微调"></a>开始微调</h2><p>启动 LLaMA Factory 的 WebUI 页面，执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python src/train_web.py</span><br></pre></td></tr></table></figure>

<p>启动后的界面如下：</p>
<img src="/images/post/2023/10/llama-factory-web.png" class="" width="1000" height="600">

<p>界面分上下两部分，上半部分是模型训练的基本配置，有如下参数：</p>
<ul>
<li>模型名称：可以使用常用的模型，包括 ChatGLM，BaiChuan，QWen，LLaMA 等，我们根据下载的模型选择<code>Baichuan2-13B-Chat</code>。</li>
<li>模型路径：输入框填写我们之前下载的 Baichuan 模型的地址。</li>
<li>微调方法有三种：<ul>
<li>full：将整个模型都进行微调。</li>
<li>freeze：将模型的大部分参数冻结，只对部分参数进行微调。</li>
<li>lora：将模型的部分参数冻结，只对部分参数进行微调，但只在特定的层上进行微调。</li>
</ul>
</li>
<li>模型断点：在未开始微调前为空，微调一次后可以点击<code>刷新断点</code>按钮，会得到之前微调过的断点。</li>
<li>高级设置和模型设置可以不用管，使用默认值即可。</li>
</ul>
<p>下半部分是一个页签窗口，分为<code>Train</code>、<code>Evaluate</code>、<code>Chat</code>、<code>Export</code>四个页签，微调先看<code>Train</code>界面，有如下参数：</p>
<ul>
<li>训练阶段：选择训练阶段，分为预训练（Pre-Training）、指令监督微调（Supervised Fine-Tuning）、奖励模型训练（Reward Modeling）、PPO 、DPO 五种，这里我们选择指令监督微调（Supervised Fine-Tuning）。<ul>
<li>Pre-Training：在该阶段，模型会在一个大型数据集上进行预训练，学习基本的语义和概念。</li>
<li>Supervised Fine-Tuning：在该阶段，模型会在一个带标签的数据集上进行微调，以提高对特定任务的准确性。</li>
<li>Reward Modeling：在该阶段，模型会学习如何从环境中获得奖励，以便在未来做出更好的决策。</li>
<li>PPO Training：在该阶段，模型会使用策略梯度方法进行训练，以提高在环境中的表现。</li>
<li>DPO Training：在该阶段，模型会使用深度强化学习方法进行训练，以提高在环境中的表现。</li>
</ul>
</li>
<li>数据路径：指训练数据集文件所在的路径，这里的路径指的是 LLaMA Factory 目录下的文件夹路径，默认是<code>data</code>目录。</li>
<li>数据集：这里可以选择<code>数据路径</code>中的数据集文件，这里我们选择<code>self_cognition</code>数据集，这个数据集是用来调教 LLM 回答诸如<strong>你是谁</strong>、<strong>你由谁制造</strong>这类问题的，里面的数据比较少只有 80 条左右。在微调前我们需要先修改这个文件中的内容，将里面的<code>&lt;NAME&gt;</code>和<code>&lt;AUTHOR&gt;</code>替换成我们的 AI 机器人名称和公司名称。选择了数据集后，可以点击右边的<code>预览数据集</code>按钮来查看数据集的前面几行的内容。</li>
</ul>
<img src="/images/post/2023/10/llama-factory-dataset.png" class="" width="600" height="400">

<ul>
<li>学习率：学习率越大，模型的学习速度越快，但是学习率太大的话，可能会导致模型在寻找最优解时<strong>跳过</strong>最优解，学习率太小的话，模型学习速度会很慢，所以这个参数需要根据实际情况进行调整，这里我们使用默认值<code>5e-5</code>。</li>
<li>训练轮数：训练轮数越多，模型的学习效果越好，但是训练轮数太多的话，模型的训练时间会很长，因为我们的训练数据比较少，所以要适当增加训练轮数，这里将值设置为<code>30</code>。</li>
<li>最大样本数：每个数据集最多使用的样本数，因为我们的数据量很少只有 80 条，所以用默认值就可以了。</li>
<li>计算类型：这里的<code>fp16</code> 和 <code>bf16</code> 是指数字的数据表示格式，主要用于深度学习训练和推理过程中，以节省内存和加速计算，这里我们选择<code>bf16</code></li>
<li>学习率调节器：有以下选项可以选择，这里我们选择默认值<code>cosine</code>。<ul>
<li>linear（线性）: 随着训练的进行，学习率将以线性方式减少。</li>
<li>cosine（余弦）: 这是根据余弦函数来减少学习率的。在训练开始时，学习率较高，然后逐渐降低并在训练结束时达到最低值。</li>
<li>cosine_with_restarts（带重启的余弦）: 和余弦策略类似，但是在一段时间后会重新启动学习率，并多次这样做。</li>
<li>polynomial（多项式）: 学习率会根据一个多项式函数来减少，可以设定多项式的次数。</li>
<li>constant（常数）: 学习率始终保持不变。</li>
<li>constant_with_warmup（带预热的常数）: 开始时，学习率会慢慢上升到一个固定值，然后保持这个值。</li>
<li>inverse_sqrt（反平方根）: 学习率会随着训练的进行按照反平方根的方式减少。</li>
<li>reduce_lr_on_plateau（在平台上减少学习率）: 当模型的进展停滞时（例如，验证误差不再下降），学习率会自动减少。</li>
</ul>
</li>
<li>梯度累积和最大梯度范数：这两个参数通常可以一起使用，以保证在微调大型语言模型时，能够有效地处理大规模数据，同时保证模型训练的稳定性。梯度累积允许在有限的硬件资源上处理更大的数据集，而最大梯度范数则可以防止梯度爆炸，保证模型训练的稳定性，这里我们使用默认值即可。</li>
<li>断点名称：默认是用时间戳作为断点名称，可以自己修改。</li>
<li>其他参数使用默认值即可。</li>
</ul>
<p>参数设置完后点击<code>预览命令</code>按钮可以查看本次微调的命令，确认无误后点击<code>开始</code>按钮就开始微调了，因为数据量比较少，大概几分钟微调就完成了(具体时间还要视机器配置而定，笔者使用的是 A40 48G GPU），在界面的右下方还可以看到微调过程中损失函数曲线，损失函数的值越低，模型的预测效果通常越好。</p>
<img src="/images/post/2023/10/llama-factory-loss.png" class="" width="600" height="400">

<p>如果在微调过程中报<code>AttributeError: &#39;BaichuanTokenizer&#39; object has no attribute &#39;sp_model&#39;</code>这个错误，可以修改 BaiChuan 模型中的文件<code>tokenization_baichuan.py</code>，修改的内容如下：</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@@ -68,6 +68,11 @@</span> class BaichuanTokenizer(PreTrainedTokenizer):</span><br><span class="line">             if isinstance(pad_token, str)</span><br><span class="line">             else pad_token</span><br><span class="line">         )</span><br><span class="line"><span class="addition">+        self.vocab_file = vocab_file</span></span><br><span class="line"><span class="addition">+        self.add_bos_token = add_bos_token</span></span><br><span class="line"><span class="addition">+        self.add_eos_token = add_eos_token</span></span><br><span class="line"><span class="addition">+        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)</span></span><br><span class="line"><span class="addition">+        self.sp_model.Load(vocab_file)</span></span><br><span class="line">         super().__init__(</span><br><span class="line">            bos_token=bos_token,</span><br><span class="line">            eos_token=eos_token,</span><br><span class="line">            unk_token=unk_token,</span><br><span class="line">            pad_token=pad_token,</span><br><span class="line">            add_bos_token=add_bos_token,</span><br><span class="line">            add_eos_token=add_eos_token,</span><br><span class="line">            sp_model_kwargs=self.sp_model_kwargs,</span><br><span class="line">            clean_up_tokenization_spaces=clean_up_tokenization_spaces,</span><br><span class="line">            **kwargs,</span><br><span class="line">         )</span><br><span class="line"><span class="deletion">-        self.vocab_file = vocab_file</span></span><br><span class="line"><span class="deletion">-        self.add_bos_token = add_bos_token</span></span><br><span class="line"><span class="deletion">-        self.add_eos_token = add_eos_token</span></span><br><span class="line"><span class="deletion">-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)</span></span><br><span class="line"><span class="deletion">-        self.sp_model.Load(vocab_file)</span></span><br></pre></td></tr></table></figure>

<p>关于这个报错的更多信息可以参考这个<a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/Baichuan2/issues/204">issue</a>。</p>
<h2 id="模型试用"><a href="#模型试用" class="headerlink" title="模型试用"></a>模型试用</h2><p>微调完成后，进入<code>Chat</code>页签对微调模型进行试用。首先点击页面上的<code>刷新断点</code>按钮，然后选择我们最近微调的断点名称，再点击<code>加载模型</code>按钮，等待加载完成后就可以进行对话了，输入微调数据集中的问题，然后来看看微调后的 LLM 的回答吧。</p>
<img src="/images/post/2023/10/llama-factory-chat.png" class="" width="1000" height="800">

<h2 id="模型导出"><a href="#模型导出" class="headerlink" title="模型导出"></a>模型导出</h2><p>如果觉得微调的模型没有问题，就可以将模型导出并正式使用了，点击<code>Export</code>页签，在<code>导出目录</code>中输入导出的文件夹地址。一般模型文件会比较大，右边的<code>最大分块大小</code>参数用来将模型文件按照大小进行切分，默认是<code>10</code>GB，比如模型文件有 15G，那么切分后就变成 2 个文件，1 个 10G，1 个 5G。设置完成后点击<code>开始导出</code>按钮即可，等导出完成后，就可以在对应目录下看到导出的模型文件了。</p>
<img src="/images/post/2023/10/llama-factory-export.png" class="" width="1000" height="800">

<p>微调后的模型使用方法和原来的模型一样，可以参考我之前的文章来进行部署和使用——<a href="https://zhaozhiming.github.io/2023/08/22/use-fastchat-deploy-llm">《使用 FastChat 部署 LLM》</a>。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LLaMA Factory 是一个强大的 LLM 微调工具，今天我们只是简单地介绍了一下它的使用方法，真正的微调过程中还有很多工作要做，包括数据集的准备，微调的多个阶段，微调后的评估等，笔者也是刚接触微调领域，文中有不对的地方希望大家在评论区指出，一起学习讨论。</p>
<p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>
</div>

</article>
<section id="appreciates">
  <center>
	<h2>赞赏</h2>
    <div class="post-footer">
      <div>
	    <h4>如果文章对您有所帮助，可以捐赠我喝杯咖啡😌，捐赠方式：</h4>
        <div class="digital-wallet">
          <h6>BTC 地址：3LYgSyf7ddMALwGWPQr3PY4wzjsTDdg1oV</h6>
          <h6>ETH 地址：0x0C9b27c89A61aadb0aEC24CA8949910Cbf77Aa73</h6>
        </div>
        <div class="wechat_appreciates">
          <img src="/images/wechat_appreciates.png" alt="qrcode" width="250" >
        </div>
      </div>
      <div>
	    <h4>文章已同步更新公众号，欢迎关注</h4>
        <div class="wechat_appreciates">
          <img src="/images/wxgzh_qrcode.jpg" alt="qrcode" width="250" >
        </div>
      </div>
    </div>
  </center>
</section>



    
      <script type="text/javascript">
        var disqus_config = function () {
            this.page.url = 'https://zhaozhiming.github.io/2023/10/27/use-llama-factory-finetuning-llm/';
            this.page.identifier = 'https://zhaozhiming.github.io/2023/10/27/use-llama-factory-finetuning-llm/';
        };

        (function() {
          var d = document, s = d.createElement('script');
          s.src = '//zhaozhiming.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    



<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>

</div>

    </div>
    <footer id="footer">
    <div style="display:inline">
    Copyright &copy; 2024

    赵芝明
. Powered by <a href="http://zespia.tw/hexo/" target="_blank">Hexo</a> |
    Theme is <a target="_blank" rel="noopener" href="https://github.com/wd/hexo-fabric">hexo-fabric</a>, fork from <a target="_blank" rel="noopener" href="http://github.com/panks/fabric">fabric</a> by <a target="_blank" rel="noopener" href="http://panks.me">Pankaj Kumar</a>
</div>


    </footer>
    <script src="/javascripts/fabric.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script>
 <!-- Delete or comment this line to disable Fancybox -->



<!-- end toload --> 
</div>
</div>
<script src="/javascripts/jquery.ui.totop.js" type="text/javascript"></script>
<script type="text/javascript">
/*<![CDATA[*/
;(function($){$().UItoTop({easingType:'easeOutCirc'});})(jQuery); 
/*]]>*/
</script><!-- remove it to remove the scroll to top button -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5608723650603289" crossorigin="anonymous"></script>
</body>
</html>
