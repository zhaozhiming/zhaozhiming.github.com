<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
    
	<title>使用 Xinference 进行模型部署 - Hacker and Geeker&#39;s Way</title>
    <meta name="author" content="">
    
	<meta name="description" content="使用 Xinference 部署大语言模型"> <!-- TODO: truncate -->
	<meta name="keywords" content="llm, xinference">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="atom.xml" rel="alternate" title="Hacker and Geeker&#39;s Way" type="application/atom+xml">
	<link href="/favicon.ico" rel="shortcut icon">
    <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/custom.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/hljs.css" media="screen, projection" rel="stylesheet" type="text/css">

    <link href='/stylesheets/font.css?family=Slackey' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Fjalla+One' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Amethysta+One' rel='stylesheet' type='text/css'>
	  <script src="/javascripts/jquery.min.js"></script>
    <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![}]-->

    <script type="text/javascript" src="/javascripts/jquery-tapir.js"></script>

    <!-- remove or comment it to disable ajaxification -->   
    <!-- <script src="/javascripts/ajaxify.js"></script> -->

    

    
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-100485541-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>


<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <div id="wrapper">
    <header id="header" class="inner"><!-- for more effects see _animate.scss -->
<h1 class="animated bounceInDown">
    <div id="headerbg">
        Hacker and Geeker&#39;s Way
    </div>
</h1>
<span class="subtitle"></span>
<br>

<ul id="social-links" style="text-align:center; clear:both;">
  
  <!-- GitHub -->
  <li>
  <a target="_blank" rel="noopener" href="https://github.com/zhaozhiming" class="github" title="Github"></a>
  </li>
  
  
  
  
  <!-- Twitter -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.twitter.com/kingzzm" class="twitter" title="Twitter"></a>
  </li>
  
  
  <!-- LinkedIn -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.linkedin.com/in/zhaozhiming" class="linkedin" title="LinkedIn"></a>
  </li>
  
  
  
  
  
  <!-- Stackoverflow -->
  <li>
  <a target="_blank" rel="noopener" href="http://stackoverflow.com/users/1954315/zhaozhiming" class="stackoverflow" title="Stackoverflow"></a>
  </li>
  
</ul>


<!-- use full url including 'index.html' for navigation bar if you are using ajax -->
<ul id="nav">
	<li id="ajax"><a href="/index.html">Home</a></li>
	<li id="ajax"><a href="/archives/index.html">Archives</a></li>
    <li><a href="/atom.xml">RSS</a></li>
    <li><a href="/about/index.html">About</a></li>
    
    <li>
    <div id="dark">
        <form action="//www.google.com.hk/search" method="get" accept-charset="UTF-8" id="search">
            <input type="hidden" name="sitesearch" value="https://zhaozhiming.github.io" />
            <input type="text" name="q" results="0" placeholder="Search..." x-webkit-speech />
        </form>
    </div>
    </li>
        
</ul>




</header>


<div id="toload">
<!-- begin toload -->
    <div id="content">
        <div class="inner">
<article class="post">
	<h2 class="title">使用 Xinference 进行模型部署</h2>
    <div class="meta">
        <div class="date">Published on: <time datetime="2024-02-13T01:18:07.000Z" itemprop="datePublished">2月 13, 2024</time>
</div>
        <div class="tags">Tags: 

<a href="/tags/llm/">llm</a> <a href="/tags/xinference/">xinference</a>
</div>
    </div>
	<div class="entry-content"><img src="/images/post/2024/02/xinference.jpeg" class="" width="400" height="300">

<p>今天为大家介绍一款大语言模型（LLM）部署和推理工具——<a target="_blank" rel="noopener" href="https://github.com/xorbitsai/inference">Xinference</a>，其特点是部署快捷、使用简单、推理高效，并且支持多种形式的开源模型，还提供了 WebGUI 界面和 API 接口，方便用户进行模型部署和推理。现在就让我们一起来了解和使用 Xinference 吧！</p>
<span id="more"></span>

<h2 id="Xinference-介绍"><a href="#Xinference-介绍" class="headerlink" title="Xinference 介绍"></a>Xinference 介绍</h2><p>Xorbits Inference（Xinference）是一个性能强大且功能全面的分布式推理框架。可用于各种模型的推理。通过 Xinference，你可以轻松地一键部署你自己的模型或内置的前沿开源模型。无论你是研究者，开发者，或是数据科学家，都可以通过 Xinference 与最前沿的 AI 模型，发掘更多可能。下面是 Xinference 与其他模型部署推理工具的对比：</p>
<img src="/images/post/2024/02/xinference-why.png" class="" width="800" height="600">

<h2 id="Xinference-安装"><a href="#Xinference-安装" class="headerlink" title="Xinference 安装"></a>Xinference 安装</h2><p>Xinference 支持两种方式的安装，一种是使用 Docker 镜像安装，另外一种是直接在本地进行安装。想了解 Docker 安装方式的朋友可以参考官方的<a target="_blank" rel="noopener" href="https://inference.readthedocs.io/en/latest/getting_started/using_docker_image.html">Docker 安装文档</a>，我们这里主要介绍本地安装的方式。</p>
<p>首先安装 Xinference 的 Python 依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install <span class="string">&quot;xinference[all]&quot;</span></span><br></pre></td></tr></table></figure>

<p>Xinference 依赖的第三方库比较多，所以安装需要花费一些时间，等安装完成后，我们就可以启动 Xinference 服务了，启动命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xinference-local</span><br></pre></td></tr></table></figure>

<p>启动成功后，我们可以通过地址 <code>http://localhost:9777</code>来访问 Xinference 的 WebGUI 界面了。</p>
<img src="/images/post/2024/02/xinference-gui.png" class="" width="1000" height="600">

<p><strong>注意：</strong>在 Xinference 安装过程中，有可能会安装 PyTorch 的其他版本（其依赖的<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm">vllm</a>组件需要安装），从而导致 GPU 服务器无法正常使用，因此在安装完 Xinference 之后，可以执行以下命令看 PyTorch 是否正常：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(torch.cuda.is_available())&quot;</span></span><br></pre></td></tr></table></figure>

<p>如果输出结果为<code>True</code>，则表示 PyTorch 正常，否则需要重新安装 PyTorch，PyTorch 的安装方式可以参考<a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch 的页面</a>。</p>
<h2 id="模型部署与使用"><a href="#模型部署与使用" class="headerlink" title="模型部署与使用"></a>模型部署与使用</h2><p>在 Xinference 的 WebGUI 界面中，我们部署模型非常简单，下面我们来介绍如何部署 LLM 模型。</p>
<p>首先我们在<code>Launch Model</code>菜单中选择<code>LANGUAGE MODELS</code>标签，输入模型关键字<code>chatglm3</code>来搜索我们要部署的 ChatGLM3 模型。</p>
<img src="/images/post/2024/02/xinference-llm1.png" class="" width="1000" height="600">

<p>然后点击<code>chatglm3</code>卡片，会出现如下界面：</p>
<img src="/images/post/2024/02/xinference-llm2.png" class="" width="600" height="400">

<p>在部署 LLM 模型时，我们有以下参数可以进行选择：</p>
<ul>
<li>Model Format: 模型格式，可以选择量化和非量化的格式，非量化的格式是<code>pytorch</code>，量化格式有<code>ggml</code>、<code>gptq</code>等</li>
<li>Model Size：模型的参数量大小，如果是 ChatGLM3 的话就只有 6B 这个选项，而如果是 Llama2 的话，则有 7B、13B、70B 等选项</li>
<li>Quantization：量化精度，有 4bit、8bit 等量化精度选择</li>
<li>N-GPU：选择使用第几个 GPU</li>
<li>Model UID（可选）: 模型自定义名称，不填的话就默认用原始模型名称</li>
</ul>
<p>参数填写完成后，点击左边的火箭图标按钮即开始部署模型，后台会根据参数选择下载量化或非量化的 LLM 模型。部署完成后，界面会自动跳转到<code>Running Models</code>菜单，在<code>LANGUAGE MODELS</code>标签中，我们可以看到部署好的 ChatGLM3-6B 模型。</p>
<img src="/images/post/2024/02/xinference-llm3.png" class="" width="1000" height="600">

<p>我们如果点击上图的红色方框图标<code>Launch Web UI</code>，浏览器会弹出 LLM 模型的 Web 界面，在这个界面中，你可以与 LLM 模型进行对话，界面如下：</p>
<img src="/images/post/2024/02/xinference-llm4.png" class="" width="1000" height="600">

<h2 id="API-接口"><a href="#API-接口" class="headerlink" title="API 接口"></a>API 接口</h2><p>如果你不满足于使用 LLM 模型的 Web 界面，你也可以调用 API 接口来使用 LLM 模型，其实在 Xinference 服务部署好的时候，WebGUI 界面和 API 接口已经同时准备好了，在浏览器中访问<code>http://localhost:9997/docs/</code>就可以看到 API 接口列表。</p>
<img src="/images/post/2024/02/xinference-api.png" class="" width="1000" height="600">

<p>接口列表中包含了大量的接口，不仅有 LLM 模型的接口，还有其他模型（比如 Embedding 或 Rerank ）的接口，而且这些都是兼容 OpenAI API 的接口。以 LLM 的聊天功能为例，我们使用 Curl 工具来调用其接口，示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">curl -X <span class="string">&#x27;POST&#x27;</span> \</span><br><span class="line">  <span class="string">&#x27;http://localhost:9997/v1/chat/completions&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;accept: application/json&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">  -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">    &quot;model&quot;: &quot;chatglm3&quot;,</span></span><br><span class="line"><span class="string">    &quot;messages&quot;: [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        &quot;role&quot;: &quot;user&quot;,</span></span><br><span class="line"><span class="string">        &quot;content&quot;: &quot;hello&quot;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">  &#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回结果</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;chatglm3&quot;</span>,</span><br><span class="line">  <span class="string">&quot;object&quot;</span>: <span class="string">&quot;chat.completion&quot;</span>,</span><br><span class="line">  <span class="string">&quot;choices&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;index&quot;</span>: 0,</span><br><span class="line">      <span class="string">&quot;message&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Hello! How can I help you today?&quot;</span>,</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">&quot;finish_reason&quot;</span>: <span class="string">&quot;stop&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;usage&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;prompt_tokens&quot;</span>: 8,</span><br><span class="line">    <span class="string">&quot;total_tokens&quot;</span>: 29,</span><br><span class="line">    <span class="string">&quot;completion_tokens&quot;</span>: 37</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="多模态模型"><a href="#多模态模型" class="headerlink" title="多模态模型"></a>多模态模型</h2><p>我们再来部署多模态模型，多模态模型是指可以识别图片的 LLM 模型，部署方式与 LLM 模型类似。</p>
<p>首先选择<code>Launch Model</code>菜单，在<code>LANGUAGE MODELS</code>标签下的模型过滤器<code>Model Ability</code>中选择<code>vl-chat</code>，可以看到目前支持的 2 个多模态模型：</p>
<img src="/images/post/2024/02/xinference-vl1.png" class="" width="800" height="400">

<p>我们选择<code>qwen-vl-chat</code>这个模型进行部署，部署参数的选择和之前的 LLM 模型类似，选择好参数后，同样点击左边的火箭图标按钮进行部署，部署完成后会自动进入<code>Running Models</code>菜单，显示如下：</p>
<img src="/images/post/2024/02/xinference-vl2.png" class="" width="800" height="400">

<p>点击图中<code>Launch Web UI</code>的按钮，浏览器会弹出多模态模型的 Web 界面，在这个界面中，你可以使用图片和文字与多模态模型进行对话，界面如下：</p>
<img src="/images/post/2024/02/xinference-vl3.png" class="" width="1000" height="600">

<h2 id="Embedding-模型"><a href="#Embedding-模型" class="headerlink" title="Embedding 模型"></a>Embedding 模型</h2><p>Embedding 模型是用来将文本转换为向量的模型，使用 Xinference 部署的话更加简单，只需要在<code>Launch Model</code>菜单中选择<code>Embedding</code>标签，然后选择相应模型，不像 LLM 模型一样需要选择参数，只需直接部署模型即可，这里我们选择部署<code>bge-base-en-v1.5</code>这个 Embedding 模型。</p>
<img src="/images/post/2024/02/xinference-embedding1.png" class="" width="1000" height="600">

<img src="/images/post/2024/02/xinference-embedding2.png" class="" width="1000" height="600">

<p>我们通过 Curl 命令调用 API 接口来验证部署好的 Embedding 模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">curl -X <span class="string">&#x27;POST&#x27;</span> \</span><br><span class="line">  <span class="string">&#x27;http://localhost:9997/v1/embeddings&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;accept: application/json&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">  -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">  &quot;model&quot;: &quot;bge-base-en-v1.5&quot;,</span></span><br><span class="line"><span class="string">  &quot;input&quot;: &quot;hello&quot;</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;object&quot;</span>: <span class="string">&quot;list&quot;</span>,</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;bge-base-en-v1.5-1-0&quot;</span>,</span><br><span class="line">  <span class="string">&quot;data&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;index&quot;</span>: 0,</span><br><span class="line">      <span class="string">&quot;object&quot;</span>: <span class="string">&quot;embedding&quot;</span>,</span><br><span class="line">      <span class="string">&quot;embedding&quot;</span>: [0.0007792398682795465, …]</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;usage&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;prompt_tokens&quot;</span>: 37,</span><br><span class="line">    <span class="string">&quot;total_tokens&quot;</span>: 37</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Rerank-模型"><a href="#Rerank-模型" class="headerlink" title="Rerank 模型"></a>Rerank 模型</h2><p>Rerank 模型是用来对文本进行排序的模型，使用 Xinference 部署的话也很简单，方法和 Embedding 模型类似，部署步骤如下图所示，这里我们选择部署<code>bge-reranker-base</code>这个 Rerank 模型：</p>
<img src="/images/post/2024/02/xinference-rerank1.png" class="" width="1000" height="600">

<img src="/images/post/2024/02/xinference-rerank2.png" class="" width="1000" height="600">

<p>我们通过 Curl 命令调用 API 接口来验证部署好的 Rerank 模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">curl -X <span class="string">&#x27;POST&#x27;</span> \</span><br><span class="line">  <span class="string">&#x27;http://localhost:9997/v1/rerank&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;accept: application/json&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">  -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">  &quot;model&quot;: &quot;bge-reranker-base&quot;,</span></span><br><span class="line"><span class="string">  &quot;query&quot;: &quot;What is Deep Learning?&quot;,</span></span><br><span class="line"><span class="string">  &quot;documents&quot;: [</span></span><br><span class="line"><span class="string">    &quot;Deep Learning is ...&quot;,</span></span><br><span class="line"><span class="string">    &quot;hello&quot;</span></span><br><span class="line"><span class="string">  ]</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;id&quot;</span>: <span class="string">&quot;88177e80-cbeb-11ee-bfe5-0242ac110007&quot;</span>,</span><br><span class="line">  <span class="string">&quot;results&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;index&quot;</span>: 0,</span><br><span class="line">      <span class="string">&quot;relevance_score&quot;</span>: 0.9165927171707153,</span><br><span class="line">      <span class="string">&quot;document&quot;</span>: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;index&quot;</span>: 1,</span><br><span class="line">      <span class="string">&quot;relevance_score&quot;</span>: 0.00003880404983647168,</span><br><span class="line">      <span class="string">&quot;document&quot;</span>: null</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="图像模型"><a href="#图像模型" class="headerlink" title="图像模型"></a>图像模型</h2><p>Xinference 还支持图像模型，使用图像模型可以实现文生图、图生图等功能。Xinference 内置了几种图像模型，分别是 Stable Diffusion（SD）的各个版本。部署方式和文本模型类似，都是在 WebGUI 界面上启动模型即可，无需进行参数选择，但因为 SD 模型比较大，在部署图像模型前请确保服务器上有<strong>50GB</strong>以上的空间。这里我们选择部署<code>sdxl-turbo</code>图像模型，部署步骤截图如下：</p>
<img src="/images/post/2024/02/xinference-image1.png" class="" width="1000" height="600">

<img src="/images/post/2024/02/xinference-image2.png" class="" width="1000" height="600">

<p>我们可以使用 Python 代码调用的方式来使用图像模型生成图片，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xinference.client <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line">client = Client(<span class="string">&quot;http://localhost:9997&quot;</span>)</span><br><span class="line">model = client.get_model(<span class="string">&quot;sdxl-turbo&quot;</span>)</span><br><span class="line"></span><br><span class="line">model.text_to_image(<span class="string">&quot;An astronaut walking on the mars&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>这里我们使用了 Xinference 的客户端工具来实现文生图功能，生成的图片会自动保存在 Xinfercnce 的 Home 目录下的<code>image</code>文件夹中，Home 目录的默认地址是<code>~/.xinference</code>，我们也可以在启动 Xinference 服务时指定 Home 目录，启动命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">XINFERENCE_HOME=/tmp/xinference xinference-local</span><br></pre></td></tr></table></figure>

<h2 id="语音模型"><a href="#语音模型" class="headerlink" title="语音模型"></a>语音模型</h2><p>语音模型是 Xinference 最近新增的功能，使用语音模型可以实现语音转文字、语音翻译等功能。在部署语音模型之前，需要先安装<code>ffmpeg</code>组件，以 Ubuntu 操作系统为例，安装命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt update &amp;&amp; <span class="built_in">sudo</span> apt install ffmpeg</span><br></pre></td></tr></table></figure>

<p>目前 Xinference 还不支持在 WebGUI 界面上部署语音模型，需要通过命令行的方式来部署语音模型，在执行部署命令之前需要确保 Xinference 服务已经启动（xinference-local），部署命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xinference launch -u whisper-1 -n whisper-large-v3 -t audio</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-u</code>：表示模型 ID</li>
<li><code>-n</code>：表示模型名称</li>
<li><code>-t</code>：表示模型类型</li>
</ul>
<p>命令行部署的方式不仅适用语音模型，也同样适用于其他类型的模型。我们通过调用 API 接口来使用部署好的语音模型，接口兼容 OpenAI 的 Audio API 接口，因此我们也可以用 OpenAI 的 Python 包来使用语音模型，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"></span><br><span class="line"><span class="comment"># api key 可以随便写一个</span></span><br><span class="line">client = openai.Client(api_key=<span class="string">&quot;not empty&quot;</span>, base_url=<span class="string">&quot;http://127.0.0.1:9997/v1&quot;</span>)</span><br><span class="line">audio_file = <span class="built_in">open</span>(<span class="string">&quot;/your/audio/file.mp3&quot;</span>, <span class="string">&quot;rb&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 openai 的方法来调用语音模型</span></span><br><span class="line">completion = client.audio.transcriptions.create(model=<span class="string">&quot;whisper-1&quot;</span>, file=audio_file)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;completion: <span class="subst">&#123;completion&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">audio_file.close()</span><br></pre></td></tr></table></figure>

<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="模型来源"><a href="#模型来源" class="headerlink" title="模型来源"></a>模型来源</h3><p>Xinference 默认是从 HuggingFace 上下载模型，如果需要使用其他网站下载模型，可以通过设置环境变量<code>XINFERENCE_MODEL_SRC</code>来实现，使用以下代码启动 Xinference 服务后，部署模型时会从<a target="_blank" rel="noopener" href="https://modelscope.cn/">Modelscope</a>上下载模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">XINFERENCE_MODEL_SRC=modelscope xinference-local</span><br></pre></td></tr></table></figure>

<h3 id="模型独占-GPU"><a href="#模型独占-GPU" class="headerlink" title="模型独占 GPU"></a>模型独占 GPU</h3><p>在 Xinference 部署模型的过程中，如果你的服务器只有一个 GPU，那么你只能部署<strong>一个 LLM 模型</strong>或<strong>多模态模型</strong>或<strong>图像模型</strong>或<strong>语音模型</strong>，因为目前 Xinference 在部署这几种模型时只实现了一个模型独占一个 GPU 的方式，如果你想在一个 GPU 上同时部署多个以上模型，就会遇到这个错误：<code>No available slot found for the model</code>。</p>
<p>但如果是 Embedding 或者 Rerank 模型的话则没有这个限制，可以在同一个 GPU 上部署多个模型。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天给大家介绍了 Xinference 这个开源的部署推理工具，因为其部署方便，支持模型多等特点让我印象非常深刻，希望这篇文章可以让更多人了解这个工具，如果在使用的过程中遇到问题，也欢迎在评论区留言讨论。</p>
<p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>
</div>

</article>
<section id="appreciates">
  <center>
	<h2>赞赏</h2>
    <div class="post-footer">
      <div>
	    <h4>如果文章对您有所帮助，可以捐赠我喝杯咖啡😌，捐赠方式：</h4>
        <div class="digital-wallet">
          <h6>BTC 地址：3LYgSyf7ddMALwGWPQr3PY4wzjsTDdg1oV</h6>
          <h6>ETH 地址：0x0C9b27c89A61aadb0aEC24CA8949910Cbf77Aa73</h6>
        </div>
        <div class="wechat_appreciates">
          <img src="/images/wechat_appreciates.png" alt="qrcode" width="250" >
        </div>
      </div>
      <div>
	    <h4>文章已同步更新公众号，欢迎关注</h4>
        <div class="wechat_appreciates">
          <img src="/images/wxgzh_qrcode.jpg" alt="qrcode" width="250" >
        </div>
      </div>
    </div>
  </center>
</section>



    
      <script type="text/javascript">
        var disqus_config = function () {
            this.page.url = 'https://zhaozhiming.github.io/2024/02/13/use-xinference-deploy-llm/';
            this.page.identifier = 'https://zhaozhiming.github.io/2024/02/13/use-xinference-deploy-llm/';
        };

        (function() {
          var d = document, s = d.createElement('script');
          s.src = '//zhaozhiming.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    



<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>

</div>

    </div>
    <footer id="footer">
    <div style="display:inline">
    Copyright &copy; 2024

    赵芝明
. Powered by <a href="http://zespia.tw/hexo/" target="_blank">Hexo</a> |
    Theme is <a target="_blank" rel="noopener" href="https://github.com/wd/hexo-fabric">hexo-fabric</a>, fork from <a target="_blank" rel="noopener" href="http://github.com/panks/fabric">fabric</a> by <a target="_blank" rel="noopener" href="http://panks.me">Pankaj Kumar</a>
</div>


    </footer>
    <script src="/javascripts/fabric.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script>
 <!-- Delete or comment this line to disable Fancybox -->



<!-- end toload --> 
</div>
</div>
<script src="/javascripts/jquery.ui.totop.js" type="text/javascript"></script>
<script type="text/javascript">
/*<![CDATA[*/
;(function($){$().UItoTop({easingType:'easeOutCirc'});})(jQuery); 
/*]]>*/
</script><!-- remove it to remove the scroll to top button -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5608723650603289" crossorigin="anonymous"></script>
</body>
</html>
