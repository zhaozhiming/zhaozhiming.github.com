<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
    
	<title>对 Llama3 执行基准测试评估 - Hacker and Geeker&#39;s Way</title>
    <meta name="author" content="">
    
	<meta name="description" content="介绍 LLM 的评测指标及如何使用评测 Llama3"> <!-- TODO: truncate -->
	<meta name="keywords" content="llm, opencompass, llama3">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="atom.xml" rel="alternate" title="Hacker and Geeker&#39;s Way" type="application/atom+xml">
	<link href="/favicon.ico" rel="shortcut icon">
    <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/custom.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/hljs.css" media="screen, projection" rel="stylesheet" type="text/css">

    <link href='/stylesheets/font.css?family=Slackey' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Fjalla+One' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Amethysta+One' rel='stylesheet' type='text/css'>
	  <script src="/javascripts/jquery.min.js"></script>
    <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![}]-->

    <script type="text/javascript" src="/javascripts/jquery-tapir.js"></script>

    <!-- remove or comment it to disable ajaxification -->   
    <!-- <script src="/javascripts/ajaxify.js"></script> -->

    

    
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-100485541-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>


<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <div id="wrapper">
    <header id="header" class="inner"><!-- for more effects see _animate.scss -->
<h1 class="animated bounceInDown">
    <div id="headerbg">
        Hacker and Geeker&#39;s Way
    </div>
</h1>
<span class="subtitle"></span>
<br>

<ul id="social-links" style="text-align:center; clear:both;">
  
  <!-- GitHub -->
  <li>
  <a target="_blank" rel="noopener" href="https://github.com/zhaozhiming" class="github" title="Github"></a>
  </li>
  
  
  
  
  <!-- Twitter -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.twitter.com/kingzzm" class="twitter" title="Twitter"></a>
  </li>
  
  
  <!-- LinkedIn -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.linkedin.com/in/zhaozhiming" class="linkedin" title="LinkedIn"></a>
  </li>
  
  
  
  
  
  <!-- Stackoverflow -->
  <li>
  <a target="_blank" rel="noopener" href="http://stackoverflow.com/users/1954315/zhaozhiming" class="stackoverflow" title="Stackoverflow"></a>
  </li>
  
</ul>


<!-- use full url including 'index.html' for navigation bar if you are using ajax -->
<ul id="nav">
	<li id="ajax"><a href="/index.html">Home</a></li>
	<li id="ajax"><a href="/archives/index.html">Archives</a></li>
    <li><a href="/atom.xml">RSS</a></li>
    <li><a href="/about/index.html">About</a></li>
    
    <li>
    <div id="dark">
        <form action="//www.google.com.hk/search" method="get" accept-charset="UTF-8" id="search">
            <input type="hidden" name="sitesearch" value="https://zhaozhiming.github.io" />
            <input type="text" name="q" results="0" placeholder="Search..." x-webkit-speech />
        </form>
    </div>
    </li>
        
</ul>




</header>


<div id="toload">
<!-- begin toload -->
    <div id="content">
        <div class="inner">
<article class="post">
	<h2 class="title">对 Llama3 执行基准测试评估</h2>
    <div class="meta">
        <div class="date">Published on: <time datetime="2024-04-29T08:49:27.000Z" itemprop="datePublished">4月 29, 2024</time>
</div>
        <div class="tags">Tags: 

<a href="/tags/llm/">llm</a> <a href="/tags/llama3/">llama3</a> <a href="/tags/opencompass/">opencompass</a>
</div>
    </div>
	<div class="entry-content"><img src="/images/post/2024/05/llama3-evaluation.jpg" class="" width="400" height="300">

<p>近日 Meta 推出他们最新的开源 LLM（大语言模型）Llama3，吸引了众多科技领域业内人士的关注。Meta 同时也公布了 Llama3 的各项基准测试指标，Llama3 在各项指标的得分上表现优异，超过了目前市面上其他开源 LLM。今天我们就来聊聊 LLM 的基准测试指标，以及如何使用工具来评测 Llama3。</p>
<span id="more"></span>

<h2 id="LLama3-介绍"><a href="#LLama3-介绍" class="headerlink" title="LLama3 介绍"></a>LLama3 介绍</h2><p>Llama3 的发布无疑是人工智能领域的一个重磅消息，在 Meta 的官方介绍中罗列了 Llama3 的模型能力、训练参数等技术信息，有网友对这次发布进行了总结：</p>
<img src="/images/post/2024/05/llama3-features.jpeg" class="" width="1000" height="600">

<p>Llama3 开放了 8B 和 70B 两种参数的 LLM，各项能力比 Llama2 强很多，同时还有一个 400B 的 LLM 还在训练中。</p>
<h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>Llama3 可以在 HuggingFace 上进行下载，但在下载之前需要先提交申请，HuggingFace 上的申请页面如下：</p>
<img src="/images/post/2024/05/llama3-application.png" class="" width="1000" height="600">

<p>申请后大约等待 1 个小时就审批通过了，接着可以使用 HuggingFace 的 CLI 命令进行下载，需要在终端进行登录再下载，以下载 Llama3-8B 为例，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br><span class="line"><span class="comment"># 输入 HuggingFace 账号的 access token</span></span><br><span class="line">huggingface-cli download meta-llama/Meta-Llama-3-8B</span><br></pre></td></tr></table></figure>

<p>或者直接下载但需要带上 HuggingFace 的 token，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli download meta-llama/Meta-Llama-3-8B --token YOUR_TOKEN</span><br></pre></td></tr></table></figure>

<h2 id="LLM-基准测试"><a href="#LLM-基准测试" class="headerlink" title="LLM 基准测试"></a>LLM 基准测试</h2><p>LLM 基准测试就像是 LLM 的<strong>考试</strong>，这个考试会用一系列的数据集、问题和任务来考验模型的聪明程度，然后根据模型的表现给出一个分数，满分是 100 分。LLM 基准测试给了一个统一的标准来衡量不同 LLM 的性能，这样一来，不管是公司里的决策者、产品经理还是开发人员，都能更容易地比较和选择最适合他们需求的模型，开发者更好地了解 LLM 的长处和短板之后，就能针对性地改进模型，让模型变得更好更强大。</p>
<p>LLM 基准测试包括以下方面：</p>
<ul>
<li>通用能力：指的是模型的语言理解、对话理解等能力，常用的测试有 MMLU、MT-Bench 等</li>
<li>Agent 能力：包括工具调用、自我调试、根据反馈信息、探索环境等方面的能力</li>
<li>逻辑推理：指的是模型的数学、编码等能力，常用的测试有 HumanEval、GSM8K 等</li>
<li>长文本能力：指的是模型的长文本总结、问答等能力</li>
<li>特定自然语言处理任务：包括阅读理解、常识推理、世界知识、特定领域知识等方面的能力，常用的测试有 ARC、HellaSwag、SIQA、WinoGrande、TruthfulQA 等</li>
<li>真实问答：指的是模型生成真实答案的能力，也是减少幻觉的能力</li>
</ul>
<p>下面是 Llama3 8B 和 70B 模型的基准测试得分：</p>
<img src="/images/post/2024/05/llama3-benchmark.png" class="" width="1000" height="600">

<p>Meta 使用了 MMLU、HumanEval、GSM8K 等基准对 Llama3 8B 和 70B 的模型进行了评测，从图中可以看出，Llama3 在各项基准测试中的得分都很高，尤其是编码能力，HumanEval 和 GSM8K 的得分都比较高。</p>
<p>我们再看下 OpenAI 模型基准测试得分：</p>
<img src="/images/post/2024/05/gpt-benchmark.png" class="" width="1000" height="600">

<p>OpenAI 使用了 MMLU、HellaSwag、WinoGrande、ARC、HumanEval 等基准对 GPT-3.5 和 GPT-4 等模型进行了评测，但在 HumanEval 上的分数比 Llama3 低很多。</p>
<p>在 <a target="_blank" rel="noopener" href="https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a">HuggingFace</a> 上，一般是使用 ARC、 HellaSwag、MMLU、TruthQA、WinoGrande 和 GSM8K 这几个基准对 LLM 进行评测。</p>
<img src="/images/post/2024/05/huggingface-benchmark.png" class="" width="1000" height="600">

<h2 id="使用-OpenCompass-评测-Llama3"><a href="#使用-OpenCompass-评测-Llama3" class="headerlink" title="使用 OpenCompass 评测 Llama3"></a>使用 OpenCompass 评测 Llama3</h2><h3 id="OpenCompass-介绍"><a href="#OpenCompass-介绍" class="headerlink" title="OpenCompass 介绍"></a>OpenCompass 介绍</h3><p><a target="_blank" rel="noopener" href="https://github.com/open-compass/OpenCompass/">OpenCompass</a> 作为一个专业的大模型评测工具，为用户提供了全面、高效、灵活的评测解决方案，通过开源可复现的评测框架，支持对各类模型进行全面的能力评估。OpenCompass 将测评方向汇总为知识、语言、理解、推理、考试等五大能力维度，整合超过 70 个评测数据集，提供超过 40 万个模型评测问题，支持超过 70 种开源模型的评测，包括最新的 Llama3 模型。</p>
<h3 id="OpenCompass-安装"><a href="#OpenCompass-安装" class="headerlink" title="OpenCompass 安装"></a>OpenCompass 安装</h3><p>OpenCompass 的安装方式分为 GPU 环境和 CPU 环境，我们以 GPU 环境为例进行讲解，首先使用<code>conda</code>创建一个新的 Python 环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y</span><br><span class="line">conda activate opencompass</span><br></pre></td></tr></table></figure>

<p>然后下载 OpenCompass 的代码仓库并安装相关依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/open-compass/opencompass.git</span><br><span class="line"><span class="built_in">cd</span> opencompass</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure>

<p>因为我们后面还要做 HumanEval 的评测，所以还需要额外安装 HumanEval ：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/openai/human-eval.git</span><br><span class="line"><span class="built_in">cd</span> human-eval</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -e .</span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure>

<p>OpenCompass 提供了基准测试的数据集，包含 HuggingFace 和其他第三方的数据集，还有 OpenCompass 自己的数据集，下载这些数据集并解压到 OpenCompass 的目录下，解压完成后 OpenCompass 目录下会出现一个 data 目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> opencompass</span><br><span class="line">wget https://github.com/open-compass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-core-20240207.zip</span><br><span class="line">unzip OpenCompassData-core-20240207.zip</span><br><span class="line"><span class="built_in">rm</span> OpenCompassData-core-20240207.zip</span><br></pre></td></tr></table></figure>

<h3 id="Llama3-基准评测"><a href="#Llama3-基准评测" class="headerlink" title="Llama3 基准评测"></a>Llama3 基准评测</h3><p>在开始测评之前，先介绍一下硬件配置和评测模型：</p>
<ul>
<li>硬件配置：Nvidia 4090 24G 显存</li>
<li>评测模型：<a target="_blank" rel="noopener" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Meta-Llama-3-8B-Instruct</a></li>
</ul>
<p>同时设置好测试时所需的环境变量，如果不设置运行程序会报错：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MKL_SERVICE_FORCE_INTEL=1</span><br><span class="line"><span class="built_in">export</span> MKL_THREADING_LAYER=GNU</span><br><span class="line"><span class="built_in">export</span> TF_ENABLE_ONEDNN_OPTS=0</span><br></pre></td></tr></table></figure>

<ul>
<li>MKL_SERVICE_FORCE_INTEL&#x3D;1: 表示应用程序在执行使用英特尔数学核心库（MKL）的操作时，强制使用英特尔的服务层，即使在非英特尔的硬件上也是如此。</li>
<li>MKL_THREADING_LAYER&#x3D;GNU: 指定 MKL 使用的线程库为 GNU（GOMP，即 GNU OpenMP），选择正确的线程层可以优化并行性能，减少线程竞争和管理开销。</li>
<li>TF_ENABLE_ONEDNN_OPTS&#x3D;1: 这个环境变量是为 TensorFlow 设置的，表示启动一些基于 OneDNN（之前称为 MKL-DNN）的优化。</li>
</ul>
<p>我们首先进行 SocialIQA 和 WinoGrande 的评测，这 2 个评估指标是用于评估 LLM 理解社会常识和解决歧义问题能力的基准，测试代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python run.py --datasets siqa_gen winograd_ppl \</span><br><span class="line">--hf-path meta-llama/Meta-Llama-3-8B-Instruct \</span><br><span class="line">--model-kwargs device_map=<span class="string">&#x27;auto&#x27;</span> \</span><br><span class="line">--tokenizer-kwargs padding_side=<span class="string">&#x27;left&#x27;</span> truncation=<span class="string">&#x27;left&#x27;</span> trust_remote_code=True \</span><br><span class="line">--max-seq-len 2048 \</span><br><span class="line">--max-out-len 100 \</span><br><span class="line">--batch-size 64 \</span><br><span class="line">--num-gpus 1</span><br><span class="line">--debug</span><br></pre></td></tr></table></figure>

<ul>
<li>datasets: 数据集名称</li>
<li>hf-path: HuggingFace 模型地址，如果本地没有该模型会自动下载</li>
<li>model-kwargs: 构造 model 的参数</li>
<li>tokenizer-kwargs: 构造 tokenizer 的参数</li>
<li>max-seq-len: 模型能接受的最大序列长度</li>
<li>max-out-len: 最长生成 token 数</li>
<li>batch-size: 批次大小，如果运行过程中提示显存不足，可以适当调小 batch-size</li>
<li>num-gpus: 运行模型所需的最少 GPU 数量</li>
<li>debug: 是否开启 debug 模式，建议是开启调试模式，这样如果有报错会更容易定位问题</li>
</ul>
<p>如果程序执行成功，最后在终端会输出下面的信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataset    version    metric    mode      opencompass....-Llama-3-8B-Instruct</span><br><span class="line">---------  ---------  --------  ------  -------------------------------------</span><br><span class="line">siqa       e78df3     accuracy  gen                     38.59</span><br><span class="line">winograd   b6c7ed     accuracy  ppl                     57.19</span><br></pre></td></tr></table></figure>

<p>这就是 Llama3 在 SocialIQA 和 WinoGrande 评测中的得分，分别是 38.59 和 57.19。评测结果可以在 OpenCompass 的 <code>outputs/default/&#123;timestamp&#125;/summary</code>目录下的 CSV 文件中查看，内容如下所示：</p>
<img src="/images/post/2024/05/siqa-winograd.png" class="" width="1000" height="600">

<p>我们再来进行 HumanEval 基准测试，HumanEval 是 OpenAI 创建的基准测试，主要用于评估语言模型在代码生成任务上的能力。它包含多种编程问题，模型的任务是生成符合问题要求的代码。评估方式包括运行模型生成的代码并检测其输出是否正确，以及验证代码的功能是否符合问题的需求，测试代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python run.py --datasets humaneval_gen_8e312c \</span><br><span class="line">--hf-path meta-llama/Meta-Llama-3-8B-Instruct \</span><br><span class="line">--model-kwargs device_map=<span class="string">&#x27;auto&#x27;</span> \</span><br><span class="line">--tokenizer-kwargs padding_side=<span class="string">&#x27;left&#x27;</span> truncation=<span class="string">&#x27;left&#x27;</span> trust_remote_code=True \</span><br><span class="line">--max-seq-len 2048 \</span><br><span class="line">--max-out-len 100 \</span><br><span class="line">--batch-size 64 \</span><br><span class="line">--num-gpus 1</span><br><span class="line">--debug</span><br></pre></td></tr></table></figure>

<p>可以看到运行代码代与之前的基本一致，只是 datasets 参数改为 humaneval_gen_8e312c，数据集的名称可以在 OpenCompass 的 <code>configs/datasets</code>目录下找到，每个数据集对应一个 Python 文件，以 humaneval_gen_8e312c 数据集为例，对应的就是<code>configs/datasets/humaneval/humaneval_gen_8e312c.py</code>这个文件。</p>
<p>在执行 HumanEval 评测时，还需要修改之前下载的 human-eval 代码库中的 <code>human_eval/execution.py</code> 文件，将第 58 行的注释取消，启用代码执行评测，这样才能得到正确的评测结果。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># WARNING</span><br><span class="line"># This program exists to execute untrusted model-generated code. Although</span><br><span class="line"># it is highly unlikely that model-generated code will do something overtly</span><br><span class="line"># malicious in response to this test suite, model-generated code may act</span><br><span class="line"># destructively due to a lack of model capability or alignment.</span><br><span class="line"># Users are strongly encouraged to sandbox this evaluation suite so that it</span><br><span class="line"># does not perform destructive actions on their host or network. For more</span><br><span class="line"># information on how OpenAI sandboxes its code, see the accompanying paper.</span><br><span class="line"># Once you have read this disclaimer and taken appropriate precautions,</span><br><span class="line"># uncomment the following line and proceed at your own risk:</span><br><span class="line"><span class="deletion">-#                         exec(check_program, exec_globals)</span></span><br><span class="line"><span class="addition">+                         exec(check_program, exec_globals)</span></span><br><span class="line">                result.append(&quot;passed&quot;)</span><br></pre></td></tr></table></figure>

<p>执行完 HumanEval 评测后的结果如下：</p>
<img src="/images/post/2024/05/humaneval.png" class="" width="1000" height="600">

<p>这里 HumanEval 的分数是 51.83，而 Llama3 官方的分数是 62.2，相差比较多，可能是因为官方评测所用的提示词与 OpenCompass 的不相同，所以得分会有所差异。</p>
<p>最后再来进行 GSM8K 评测，GSM8K 是一个用于评估语言模型在数学推理和问题解决能力方面的基准。这个基准包含 8000 个基于小学数学的题目，涵盖了各种数学主题。评估主要看模型在解决这些数学问题时的准确性和推理能力，测试代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python run.py --datasets gsm8k_gen_1d7fe4 \</span><br><span class="line">--hf-path meta-llama/Meta-Llama-3-8B-Instruct \</span><br><span class="line">--model-kwargs device_map=<span class="string">&#x27;auto&#x27;</span> \</span><br><span class="line">--tokenizer-kwargs padding_side=<span class="string">&#x27;left&#x27;</span> truncation=<span class="string">&#x27;left&#x27;</span> trust_remote_code=True \</span><br><span class="line">--max-seq-len 2048 \</span><br><span class="line">--max-out-len 100 \</span><br><span class="line">--batch-size 64 \</span><br><span class="line">--num-gpus 1</span><br><span class="line">--debug</span><br></pre></td></tr></table></figure>

<p>执行完 GSM8K 评测后的结果如下：</p>
<img src="/images/post/2024/05/gsm8k.png" class="" width="1000" height="600">

<p>实际评测 GSM8K 的分数是 78.7，而 Llama3 官方得分是 79.6，这一次两者的得分比较接近。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了 LLM 的基准测试指标，以及如何使用 OpenCompass 评测最新的开源模型 Llama3 并得到指标得分。通过 OpenCompass 的评测，我们可以更全面地了解 Llama3 的性能表现。LLM 基准在一般情况下是有帮助的，但随着基准的日益普及，新的模型可以被训练或微调以获得基准测试高分，这使得模型的得分并不真实反映其在被评估方面的能力，期待这一问题能在日后可以得到改善。</p>
<p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>
</div>

</article>
<section id="appreciates">
  <center>
	<h2>赞赏</h2>
    <div class="post-footer">
      <div>
	    <h4>如果文章对您有所帮助，可以捐赠我喝杯咖啡😌，捐赠方式：</h4>
        <div class="digital-wallet">
          <h6>BTC 地址：3LYgSyf7ddMALwGWPQr3PY4wzjsTDdg1oV</h6>
          <h6>ETH 地址：0x0C9b27c89A61aadb0aEC24CA8949910Cbf77Aa73</h6>
        </div>
        <div class="wechat_appreciates">
          <img src="/images/wechat_appreciates.png" alt="qrcode" width="250" >
        </div>
      </div>
      <div>
	    <h4>文章已同步更新公众号，欢迎关注</h4>
        <div class="wechat_appreciates">
          <img src="/images/wxgzh_qrcode.jpg" alt="qrcode" width="250" >
        </div>
      </div>
    </div>
  </center>
</section>



    
      <script type="text/javascript">
        var disqus_config = function () {
            this.page.url = 'https://zhaozhiming.github.io/2024/04/29/use-opencompass-evaluate-llama3/';
            this.page.identifier = 'https://zhaozhiming.github.io/2024/04/29/use-opencompass-evaluate-llama3/';
        };

        (function() {
          var d = document, s = d.createElement('script');
          s.src = '//zhaozhiming.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    



<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>

</div>

    </div>
    <footer id="footer">
    <div style="display:inline">
    Copyright &copy; 2025

    赵芝明
. Powered by <a href="http://zespia.tw/hexo/" target="_blank">Hexo</a> |
    Theme is <a target="_blank" rel="noopener" href="https://github.com/wd/hexo-fabric">hexo-fabric</a>, fork from <a target="_blank" rel="noopener" href="http://github.com/panks/fabric">fabric</a> by <a target="_blank" rel="noopener" href="http://panks.me">Pankaj Kumar</a>
</div>


    </footer>
    <script src="/javascripts/fabric.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script>
 <!-- Delete or comment this line to disable Fancybox -->



<!-- end toload --> 
</div>
</div>
<script src="/javascripts/jquery.ui.totop.js" type="text/javascript"></script>
<script type="text/javascript">
/*<![CDATA[*/
;(function($){$().UItoTop({easingType:'easeOutCirc'});})(jQuery); 
/*]]>*/
</script><!-- remove it to remove the scroll to top button -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5608723650603289" crossorigin="anonymous"></script>
</body>
</html>
