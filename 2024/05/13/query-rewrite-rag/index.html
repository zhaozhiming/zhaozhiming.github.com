<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
    
	<title>高级 RAG 检索策略之查询重写 - Hacker and Geeker&#39;s Way</title>
    <meta name="author" content="">
    
	<meta name="description" content="介绍高级 RAG 检索中几种查询重写的策略"> <!-- TODO: truncate -->
	<meta name="keywords" content="rag, llamaindex, query-rewrite, hyde, stepback">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="atom.xml" rel="alternate" title="Hacker and Geeker&#39;s Way" type="application/atom+xml">
	<link href="/favicon.ico" rel="shortcut icon">
    <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/custom.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/hljs.css" media="screen, projection" rel="stylesheet" type="text/css">

    <link href='/stylesheets/font.css?family=Slackey' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Fjalla+One' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Amethysta+One' rel='stylesheet' type='text/css'>
	  <script src="/javascripts/jquery.min.js"></script>
    <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![}]-->

    <script type="text/javascript" src="/javascripts/jquery-tapir.js"></script>

    <!-- remove or comment it to disable ajaxification -->   
    <!-- <script src="/javascripts/ajaxify.js"></script> -->

    

    
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-100485541-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>


<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <div id="wrapper">
    <header id="header" class="inner"><!-- for more effects see _animate.scss -->
<h1 class="animated bounceInDown">
    <div id="headerbg">
        Hacker and Geeker&#39;s Way
    </div>
</h1>
<span class="subtitle"></span>
<br>

<ul id="social-links" style="text-align:center; clear:both;">
  
  <!-- GitHub -->
  <li>
  <a target="_blank" rel="noopener" href="https://github.com/zhaozhiming" class="github" title="Github"></a>
  </li>
  
  
  
  
  <!-- Twitter -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.twitter.com/kingzzm" class="twitter" title="Twitter"></a>
  </li>
  
  
  <!-- LinkedIn -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.linkedin.com/in/zhaozhiming" class="linkedin" title="LinkedIn"></a>
  </li>
  
  
  
  
  
  <!-- Stackoverflow -->
  <li>
  <a target="_blank" rel="noopener" href="http://stackoverflow.com/users/1954315/zhaozhiming" class="stackoverflow" title="Stackoverflow"></a>
  </li>
  
</ul>


<!-- use full url including 'index.html' for navigation bar if you are using ajax -->
<ul id="nav">
	<li id="ajax"><a href="/index.html">Home</a></li>
	<li id="ajax"><a href="/archives/index.html">Archives</a></li>
    <li><a href="/atom.xml">RSS</a></li>
    <li><a href="/about/index.html">About</a></li>
    
    <li>
    <div id="dark">
        <form action="//www.google.com.hk/search" method="get" accept-charset="UTF-8" id="search">
            <input type="hidden" name="sitesearch" value="https://zhaozhiming.github.io" />
            <input type="text" name="q" results="0" placeholder="Search..." x-webkit-speech />
        </form>
    </div>
    </li>
        
</ul>




</header>


<div id="toload">
<!-- begin toload -->
    <div id="content">
        <div class="inner">
<article class="post">
	<h2 class="title">高级 RAG 检索策略之查询重写</h2>
    <div class="meta">
        <div class="date">Published on: <time datetime="2024-05-13T14:18:11.000Z" itemprop="datePublished">5月 13, 2024</time>
</div>
        <div class="tags">Tags: 

<a href="/tags/llamaindex/">llamaindex</a> <a href="/tags/rag/">rag</a> <a href="/tags/query-rewrite/">query-rewrite</a> <a href="/tags/hyde/">hyde</a> <a href="/tags/stepback/">stepback</a>
</div>
    </div>
	<div class="entry-content"><img src="/images/post/2024/05/rag-query-rewrite.jpeg" class="" width="400" height="300">

<p>在 RAG（Retrieval Augmented Generation）应用中，文档检索是保证 RAG 应用高质量回答的关键环节，我们在之前的文章中也有所介绍，但除此之外，对用户问题的优化也同样重要，有时候用户的问题可能不够清晰或者不够具体，这时候就需要对用户问题进行查询重写，这样才能更好地提高检索的准确性。今天我们就来介绍一些 RAG 应用中查询重写的策略，以及了解如何在实际项目中使用它们。</p>
<span id="more"></span>

<h2 id="子问题查询"><a href="#子问题查询" class="headerlink" title="子问题查询"></a>子问题查询</h2><p>子问题策略，也称为子查询，是一种用于生成子问题的技术。子问题策略的核心思想是在问答过程中，为了更好地理解和回答主问题，系统会自动生成并提出与主问题相关的子问题。这些子问题通常具有更具体的细节，可以帮助系统更深入地理解主问题，从而进行更加准确的检索并提供正确的答案。</p>
<img src="/images/post/2024/05/sub-question-flow.png" class="" width="1000" height="400">

<ul>
<li>子问题策略首先将用户问题通过 LLM（大语言模型）生成多个子问题</li>
<li>然后将每个子问题经过 RAG 流程得到各自的答案（检索-生成）</li>
<li>最后将所有子问题的答案合并，得到最终的答案</li>
</ul>
<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>在<a target="_blank" rel="noopener" href="https://www.llamaindex.ai/">LlamaIndex</a>中已经对子问题查询进行了实现，但在查看子问题查询的效果之前，我们先看普通 RAG 检索对于复杂问题的效果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader</span><br><span class="line"></span><br><span class="line">question = <span class="string">&quot;哈莉·奎因和灭霸在《复仇者联盟》中是正义的角色吗？&quot;</span></span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">node_parser = VectorStoreIndex.from_documents(documents)</span><br><span class="line">query_engine = node_parser.as_query_engine()</span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;base query result: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">base query result: 不，哈莉·奎茵和灭霸在《复仇者联盟》系列中并非被描绘为正义的角色。</span><br></pre></td></tr></table></figure>

<p>以上代码是 LlamaIndex 的普通 RAG 检索过程，文档数据我们还是使用之前维基百科上的<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情来进行测试，这里我们问了一个复合问题，包括 2 个人物角色，一个是 DC 漫画中的<code>哈莉·奎茵</code>，另一个是漫威电影中的<code>灭霸</code>，问题是他们在复仇者联盟中是否为正义角色，查询结果虽然可以说正确，但并没有指出其中一个人物不是漫威电影中的人物。</p>
<p>我们再来看子问题查询的效果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.tools <span class="keyword">import</span> QueryEngineTool, ToolMetadata</span><br><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> SubQuestionQueryEngine</span><br><span class="line"></span><br><span class="line">query_engine_tools = [</span><br><span class="line">    QueryEngineTool(</span><br><span class="line">        query_engine=query_engine,</span><br><span class="line">        metadata=ToolMetadata(</span><br><span class="line">            name=<span class="string">&quot;Avengers&quot;</span>,</span><br><span class="line">            description=<span class="string">&quot;Marvel movie The Avengers&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line">query_engine = SubQuestionQueryEngine.from_defaults(</span><br><span class="line">    query_engine_tools=query_engine_tools</span><br><span class="line">)</span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;sub question query result: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Generated <span class="number">2</span> sub questions.</span><br><span class="line">[Avengers] Q: 哈莉·奎茵在《复仇者联盟》电影中扮演什么角色？</span><br><span class="line">[Avengers] Q: 灭霸在《复仇者联盟》电影中扮演什么角色？</span><br><span class="line">[Avengers] A: 在提供的有关《复仇者联盟》电影的背景中未提到哈莉·奎茵。</span><br><span class="line">[Avengers] A: 灭霸是《复仇者联盟》电影中的主要反派。他是一个强大的战争领主，试图利用无限宝石按照自己的愿景重塑宇宙。灭霸被描绘为强大而无情的敌人，对复仇者联盟和整个宇宙构成重大威胁。</span><br><span class="line">sub question query result: 在提供的有关《复仇者联盟》电影的背景中未提到哈莉·奎茵。灭霸是《复仇者联盟》电影中的主要反派，被描绘为一个强大而无情的敌人。</span><br></pre></td></tr></table></figure>

<ul>
<li>首先构建查询引擎工具，在工具中传入普通的查询引擎，并设置工具的元数据，元数据信息在 Debug 信息中会进行展示</li>
<li>使用<code>SubQuestionQueryEngine</code>类构建子问题查询引擎，传入查询引擎工具</li>
<li>查询结果中会显示生成的子问题以及子问题的答案，最终答案基于所有子问题的答案进行生成</li>
</ul>
<p>从上面的代码可以看出，对于复杂问题，子问题查询的效果要比普通查询更加准确。</p>
<p>上面的示例中，生成的子问题及答案是随着 Debug 信息展示出来的，我们也可以在检索过程中获取这些数据：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.callbacks <span class="keyword">import</span> (</span><br><span class="line">    CallbackManager,</span><br><span class="line">    LlamaDebugHandler,</span><br><span class="line">    CBEventType,</span><br><span class="line">    EventPayload,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line">llama_debug = LlamaDebugHandler(print_trace_on_end=<span class="literal">True</span>)</span><br><span class="line">callback_manager = CallbackManager([llama_debug])</span><br><span class="line">Settings.callback_manager = callback_manager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子问题查询代码</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (start_event, end_event) <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">    llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)</span><br><span class="line">):</span><br><span class="line">    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sub Question &quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&quot;: &quot;</span> + qa_pair.sub_q.sub_question.strip())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Answer: &quot;</span> + qa_pair.answer.strip())</span><br></pre></td></tr></table></figure>

<ul>
<li>在子问题查询的代码前面加上回调管理器，用来记录子问题查询的调试信息</li>
<li>在查询结束后，通过回调管理器获取子问题查询的调试信息，然后得到子问题和答案</li>
</ul>
<h3 id="提示词"><a href="#提示词" class="headerlink" title="提示词"></a>提示词</h3><p>LlamaIndex 使用单独的 Python 包<code>llama-index-question-gen-openai</code>来生成子问题，它的内部默认使用 OpenAI 的模型来生成子问题，提示词模板可以在 <a target="_blank" rel="noopener" href="https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/question_gen/llama-index-question-gen-openai/llama_index/question_gen/openai/base.py#L18-L45">LlamaIndex 的官方仓库</a>中查看。</p>
<p>我们也可以通过以下方法来打印 LlamaIndex 中的提示词，第一种方法是通过<code>get_prompts()</code>方法来打印，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prompts = query_engine.get_prompts()</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> prompts.keys():</span><br><span class="line">    sub_question_prompt = prompts[key]</span><br><span class="line">    template = sub_question_prompt.get_template()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;prompt: <span class="subst">&#123;template&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>首先通过<code>get_prompts()</code>方法获取查询引擎的 prompts 对象，基本上每个 LlamaIndex 对象都有这个方法</li>
<li>prompts 对象是个 JSON 对象，它的每个 Key 代表一个提示词模板</li>
<li>遍历 prompts 对象的每个 Key，获取每个 Key 对应的提示词模板，然后打印出来</li>
<li>子问题查询会包含 2 个提示词模板，一个是子问题生成的提示词模板，另一个是普通 RAG 的提示词模板</li>
</ul>
<p>另外一种方式是通过<code>set_global_handler</code>进行全局设置，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> set_global_handler</span><br><span class="line"></span><br><span class="line">set_global_handler(<span class="string">&quot;simple&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>在文件开头加上以上代码，这样在执行代码的过程中就会打印出 RAG 检索过程中的提示词，打印出的提示词不是提示词模板，而是加入了具体变量值之后的<strong>完整提示词</strong>。</p>
<h2 id="HyDE-查询转换"><a href="#HyDE-查询转换" class="headerlink" title="HyDE 查询转换"></a>HyDE 查询转换</h2><img src="/images/post/2024/05/hyde_paper.jpeg" class="" width="1000" height="400">

<p>HyDE（Hypothetical Document Embeddings）的本质是通过 LLM 对用户问题生成假设性文档，这些文档基于 LLM 本身的知识生成，可能存在错误或者不准确，但是跟 RAG 中知识库的文档相关联，然后通过假设性文档去检索向量相近的真实文档，通过这种方式来提高检索的准确性，HyDE 的论文可以参考<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.10496.pdf">这里</a>。</p>
<h3 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h3><p>在 LlamaIndex 中已经实现了 HyDE 的查询重写，我们先来看 LlamaIndex 如何生成假设性文档：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.indices.query.query_transform <span class="keyword">import</span> HyDEQueryTransform</span><br><span class="line"></span><br><span class="line">question = <span class="string">&quot;洛基使用了哪种神秘物品试图征服地球？&quot;</span></span><br><span class="line"></span><br><span class="line">hyde = HyDEQueryTransform(include_original=<span class="literal">True</span>)</span><br><span class="line">query_bundle = hyde(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;query_bundle embedding len: <span class="subst">&#123;<span class="built_in">len</span>(query_bundle.embedding_strs)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> idx, embedding <span class="keyword">in</span> <span class="built_in">enumerate</span>(query_bundle.embedding_strs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;embedding <span class="subst">&#123;idx&#125;</span>: <span class="subst">&#123;embedding[:<span class="number">100</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">query_bundle embedding <span class="built_in">len</span>: <span class="number">2</span></span><br><span class="line">embedding <span class="number">0</span>: 在他试图征服地球时，洛基使用了立方体，也被称为宇宙立方。这个神秘的...</span><br><span class="line">embedding <span class="number">1</span>: 洛基使用了哪种神秘物品试图征服地球？</span><br></pre></td></tr></table></figure>

<ul>
<li>首先构建<code>HyDEQueryTransform</code>对象，传入参数<code>include_original=True</code>，表示在生成的假设性文档中包含原始问题，其实<code>include_original</code>的默认值就是<code>True</code>，这里传入参数只是为了演示</li>
<li>然后调用<code>hyde</code>对象，传入问题，返回一个<code>QueryBundle</code>对象</li>
<li><code>QueryBundle</code>对象的<code>embedding_strs</code>属性值是一个数组，数组第一个元素是生成的假设性文档，如果<code>include_original</code>为<code>True</code>，那么数组的第 2 个元素会包含原始问题</li>
</ul>
<p>可以看到 LLM 基于自己的知识很好地回答了用户的问题，生成的假设性文档和电影剧情基本一致。</p>
<p>LlamaIndex 中生成假设性文档的提示词模板如下，大意就是为问题生成一段内容，其中<code>&#123;context_str&#125;</code>为用户问题：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">HYDE_TMPL = (</span><br><span class="line">    <span class="string">&quot;Please write a passage to answer the question\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Try to include as many key details as possible.\n&quot;</span></span><br><span class="line">    <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;&#123;context_str&#125;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="string">&#x27;Passage:&quot;&quot;&quot;\n&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>下面我们再用查询引擎对问题进行检索：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> TransformQueryEngine</span><br><span class="line"></span><br><span class="line">hyde_query_engine = TransformQueryEngine(query_engine, hyde)</span><br><span class="line">response = hyde_query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;hyde query result: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">hyde query result: 洛基在试图征服地球时使用了立方体，这是一个未知潜力的强大能源。</span><br></pre></td></tr></table></figure>

<ul>
<li>基于<code>HyDEQueryTransform</code>构建一个<code>TransformQueryEngine</code></li>
<li>查询引擎的<code>query</code>方法会先对原始问题生成假设性文档，然后用假设性文档进行检索并生成答案</li>
</ul>
<p>虽然我们得到了正确的结果，但我们不清楚 LlamaIndex 内部在检索过程中是否用假设性文档去检索，我们可以通过以下代码来验证：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.retrievers.transform_retriever <span class="keyword">import</span> TransformRetriever</span><br><span class="line"></span><br><span class="line">retriever = node_parser.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">hyde_retriever = TransformRetriever(retriever, hyde)</span><br><span class="line">nodes = hyde_retriever.retrieve(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;hyde retriever nodes len: <span class="subst">&#123;<span class="built_in">len</span>(nodes)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node id: <span class="subst">&#123;node.id_&#125;</span>, score: <span class="subst">&#123;node.get_score()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">50</span>)</span><br><span class="line">nodes = retriever.retrieve(<span class="string">&quot;\n&quot;</span>.join(<span class="string">f&quot;<span class="subst">&#123;n&#125;</span>&quot;</span> <span class="keyword">for</span> n <span class="keyword">in</span> query_bundle.embedding_strs))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;hyde documents retrieve len: <span class="subst">&#123;<span class="built_in">len</span>(nodes)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node id: <span class="subst">&#123;node.id_&#125;</span>, score: <span class="subst">&#123;node.get_score()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>上半部分使用<code>TransformRetriever</code>结合原始检索器和<code>HyDEQueryTransform</code>对象构建一个新的检索器</li>
<li>然后用<strong>新的检索器对用户问题</strong>进行检索，打印出检索到的文档 ID 和分数</li>
<li>下半部分使用<strong>原始检索器对假设性文档</strong>进行检索，假设性文档取自<code>QueryBundle</code>对象的<code>embedding_strs</code>属性，这里的<code>embedding_strs</code>有 2 个元素，一个是假设性文档，另一个是原始问题</li>
<li>打印出用假设性文档检索到的文档 ID 和分数</li>
</ul>
<p>下面是显示的结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hyde retriever nodes len: 2</span><br><span class="line">node <span class="built_in">id</span>: 51e9381a-ef93-49ee-ae22-d169eba95549, score: 0.8895532276574978</span><br><span class="line">node <span class="built_in">id</span>: 5ef8a87e-1a72-4551-9801-ae7e792fdad2, score: 0.8499209871867581</span><br><span class="line">==================================================</span><br><span class="line">hyde documents retrieve nodes len: 2</span><br><span class="line">node <span class="built_in">id</span>: 51e9381a-ef93-49ee-ae22-d169eba95549, score: 0.8842142746289462</span><br><span class="line">node <span class="built_in">id</span>: 5ef8a87e-1a72-4551-9801-ae7e792fdad2, score: 0.8460828835028101</span><br></pre></td></tr></table></figure>

<p>可以看到两者的结果基本一致，证明检索所用的<strong>输入</strong>是相似的，也就是假设性文档，我们再把<code>HyDEQueryTransform</code>对象中的<code>include_original</code>属性设置为<code>False</code>，这意味着生成的假设性文档不包含原始问题，然后再次运行代码，结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hyde retriever nodes len: 2</span><br><span class="line">node <span class="built_in">id</span>: cfaea328-16d8-4eb8-87ca-8eeccad28263, score: 0.7548985780343257</span><br><span class="line">node <span class="built_in">id</span>: f47bc6c7-d8e1-421f-b9b8-a8006e768c04, score: 0.7508234876205329</span><br><span class="line">==================================================</span><br><span class="line">hyde documents retrieve nodes len: 2</span><br><span class="line">node <span class="built_in">id</span>: 6c2bb8cc-3c7d-4f92-b039-db925dd60d53, score: 0.7498683385309097</span><br><span class="line">node <span class="built_in">id</span>: f47bc6c7-d8e1-421f-b9b8-a8006e768c04, score: 0.7496147322045141</span><br></pre></td></tr></table></figure>

<p>可以看到两者的结果也是基本一致，但是由于缺少原始问题，检索到的文档分数较低。</p>
<h3 id="HyDE-的限制"><a href="#HyDE-的限制" class="headerlink" title="HyDE 的限制"></a>HyDE 的限制</h3><p>HyDE 生成的假设性文档是基于 LLM 的知识生成的，可能存在错误或者不准确，LlamaIndex 在<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo/?h=hyde">官方文档</a>中指出 HyDE 可能会误导查询和引起偏见，所以在实际应用中需要谨慎使用。</p>
<h2 id="回溯提示（STEP-BACK-PROMPTING）"><a href="#回溯提示（STEP-BACK-PROMPTING）" class="headerlink" title="回溯提示（STEP-BACK PROMPTING）"></a>回溯提示（STEP-BACK PROMPTING）</h2><img src="/images/post/2024/05/stepback_paper.jpeg" class="" width="1000" height="400">

<p>回溯提示是一种简单的提示技术，通过抽象化来引导 LLM 从具体实例中提取高级概念和基本原理，利用这些概念和原理指导推理，可以显著提高 LLM 遵循正确推理路径解决问题的能力。<br>以上图中第一个的问题为例，原始问题是给定温度和体积求压强，在左边的回答中，不管是原始的回答还是思维链方式的回答，结果都不正确。而通过回溯提示的方式，先通过原始问题生成一个更为广泛的问题，比如求问题背后的物理公式，再通过广泛问题得到答案，最后将广泛问题的答案和原始问题一起提交给 LLM，从而得到正确的答案。回溯提示的论文可以参考<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.06117.pdf">这里</a>。</p>
<h3 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h3><p>回溯提示在 LlamaIndex 中没有具体的实现，但我们可以通过原始调用 LLM 结合 LlamaIndex 的方式来进行演示，首先我们来让 LLM 根据原始问题生成一个回溯的问题：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line">examples = [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;Who was the spouse of Anna Karina from 1968 to 1974?&quot;</span>,</span><br><span class="line">            <span class="string">&quot;output&quot;</span>: <span class="string">&quot;Who were the spouses of Anna Karina?&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;Estella Leopold went to whichschool between Aug 1954and Nov 1954?&quot;</span>,</span><br><span class="line">            <span class="string">&quot;output&quot;</span>: <span class="string">&quot;What was Estella Leopold&#x27;seducation history?&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    few_shot_examples = <span class="string">&quot;\n\n&quot;</span>.join(</span><br><span class="line">        [<span class="string">f&quot;human: <span class="subst">&#123;example[<span class="string">&#x27;input&#x27;</span>]&#125;</span>\nAI: <span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> examples]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    step_back_question_system_prompt = PromptTemplate(</span><br><span class="line">        <span class="string">&quot;You are an expert at world knowledge.&quot;</span></span><br><span class="line">        <span class="string">&quot;Your task is to step back and paraphrase a question to a more generic step-back question,&quot;</span></span><br><span class="line">        <span class="string">&quot;which is easier to answer. Here are a few examples:\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;few_shot_examples&#125;&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    completion = client.chat.completions.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">        temperature=<span class="number">0.1</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: step_back_question_system_prompt.<span class="built_in">format</span>(</span><br><span class="line">                    few_shot_examples=few_shot_examples</span><br><span class="line">                ),</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: question&#125;,</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line">    step_back_question = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;step_back_question: <span class="subst">&#123;step_back_question&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>首先我们定义了一些回溯问题的例子，将这些例子放到 LLM 的系统提示词中让 LLM 了解生成问题的规律</li>
<li>将用户问题和系统提示词一起传给 LLM，让 LLM 生成回溯问题</li>
</ul>
<p>生成了回溯问题后，我们再分别对原始问题和回溯问题进行检索，获取它们相关的文档：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">retrievals = retriever.retrieve(question)</span><br><span class="line">normal_context = <span class="string">&quot;\n\n&quot;</span>.join([<span class="string">f&quot;<span class="subst">&#123;n.text&#125;</span>&quot;</span> <span class="keyword">for</span> n <span class="keyword">in</span> retrievals])</span><br><span class="line">retrievals = retriever.retrieve(step_back_question)</span><br><span class="line">step_back_context = <span class="string">&quot;\n\n&quot;</span>.join([<span class="string">f&quot;<span class="subst">&#123;n.text&#125;</span>&quot;</span> <span class="keyword">for</span> n <span class="keyword">in</span> retrievals])</span><br></pre></td></tr></table></figure>

<p>得到了检索结果后，我们让 LLM 生成最终的答案：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">step_back_qa_prompt_template = PromptTemplate(</span><br><span class="line">        <span class="string">&quot;Context information is below.\n&quot;</span></span><br><span class="line">        <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;normal_context&#125;\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;step_back_context&#125;\n&quot;</span></span><br><span class="line">        <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">        <span class="string">&quot;Given the context information and not prior knowledge, &quot;</span></span><br><span class="line">        <span class="string">&quot;answer the question: &#123;question&#125;\n&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    completion = client.chat.completions.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">        temperature=<span class="number">0.1</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Always answer the question, even if the context isn&#x27;t helpful.&quot;</span>,</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: step_back_qa_prompt_template.<span class="built_in">format</span>(</span><br><span class="line">                    normal_context=normal_context,</span><br><span class="line">                    step_back_context=step_back_context,</span><br><span class="line">                    question=question,</span><br><span class="line">                ),</span><br><span class="line">            &#125;,</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line">    step_back_result = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;step_back_result: <span class="subst">&#123;step_back_result&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>在提示词模板中，我们将原始问题和回溯问题的文档信息传给 LLM，并结合原始问题让 LLM 生成答案</li>
</ul>
<p>最后我们看下普通 RAG 检索和使用回溯提示后的 RAG 检索两者的区别：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">question: 泰坦星球上有过一场大战吗？</span><br><span class="line">base_result: 没有，泰坦星球上没有发生过大战。它并不是任何已知重大冲突或战争的发生地。</span><br><span class="line">====================================================================================================</span><br><span class="line">step_back_question: 泰坦星球上发生过什么重要事件吗？</span><br><span class="line">step_back_result: 是的，在漫威电影宇宙中，泰坦星球上发生了一场重大的冲突。在《复仇者联盟：无限战争》中，泰坦被描绘成灭霸的毁灭故乡，泰坦上的战斗涉及一群英雄，包括钢铁侠（托尼·斯塔克）、蜘蛛侠（彼得·帕克）、奇异博士（斯蒂芬·斯特兰奇）以及银河护卫队，他们试图阻止灭霸实现他的目标。</span><br></pre></td></tr></table></figure>

<p>可以看到没有使用回溯提示的结果是错误的，而使用了回溯提示之后，我们得到了问题在知识库文档中的正确答案。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天我们介绍了 RAG 检索中几种查询重写的策略，包括子问题查询、HyDE 查询转换和回溯提示，并通过 LlamaIndex 对这几种策略进行了代码演示，在演示过程中还介绍了一些 LlamaIndex 的使用技巧。还有其他一些查询重写的策略没有在本文中介绍，随着 RAG 技术的发展，查询重写的策略也会越来越多，我们未来在合适的时候再对这一部分进行补充，希望这些内容对大家有所帮助。</p>
<p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>
</div>

</article>
<section id="appreciates">
  <center>
	<h2>赞赏</h2>
    <div class="post-footer">
      <div>
	    <h4>如果文章对您有所帮助，可以捐赠我喝杯咖啡😌，捐赠方式：</h4>
        <div class="digital-wallet">
          <h6>BTC 地址：3LYgSyf7ddMALwGWPQr3PY4wzjsTDdg1oV</h6>
          <h6>ETH 地址：0x0C9b27c89A61aadb0aEC24CA8949910Cbf77Aa73</h6>
        </div>
        <div class="wechat_appreciates">
          <img src="/images/wechat_appreciates.png" alt="qrcode" width="250" >
        </div>
      </div>
      <div>
	    <h4>文章已同步更新公众号，欢迎关注</h4>
        <div class="wechat_appreciates">
          <img src="/images/wxgzh_qrcode.jpg" alt="qrcode" width="250" >
        </div>
      </div>
    </div>
  </center>
</section>



    
      <script type="text/javascript">
        var disqus_config = function () {
            this.page.url = 'https://zhaozhiming.github.io/2024/05/13/query-rewrite-rag/';
            this.page.identifier = 'https://zhaozhiming.github.io/2024/05/13/query-rewrite-rag/';
        };

        (function() {
          var d = document, s = d.createElement('script');
          s.src = '//zhaozhiming.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    



<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>

</div>

    </div>
    <footer id="footer">
    <div style="display:inline">
    Copyright &copy; 2024

    赵芝明
. Powered by <a href="http://zespia.tw/hexo/" target="_blank">Hexo</a> |
    Theme is <a target="_blank" rel="noopener" href="https://github.com/wd/hexo-fabric">hexo-fabric</a>, fork from <a target="_blank" rel="noopener" href="http://github.com/panks/fabric">fabric</a> by <a target="_blank" rel="noopener" href="http://panks.me">Pankaj Kumar</a>
</div>


    </footer>
    <script src="/javascripts/fabric.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script>
 <!-- Delete or comment this line to disable Fancybox -->



<!-- end toload --> 
</div>
</div>
<script src="/javascripts/jquery.ui.totop.js" type="text/javascript"></script>
<script type="text/javascript">
/*<![CDATA[*/
;(function($){$().UItoTop({easingType:'easeOutCirc'});})(jQuery); 
/*]]>*/
</script><!-- remove it to remove the scroll to top button -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5608723650603289" crossorigin="anonymous"></script>
</body>
</html>
