<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
    
	<title>高级 RAG 检索策略之混合检索 - Hacker and Geeker&#39;s Way</title>
    <meta name="author" content="">
    
	<meta name="description" content="介绍如何用LlamaIndex、Llama3和ElasticSearch打造高效混合检索系统"> <!-- TODO: truncate -->
	<meta name="keywords" content="rag, llamaindex, llama3, elasticsearch, rrf">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="atom.xml" rel="alternate" title="Hacker and Geeker&#39;s Way" type="application/atom+xml">
	<link href="/favicon.ico" rel="shortcut icon">
    <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/custom.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/stylesheets/hljs.css" media="screen, projection" rel="stylesheet" type="text/css">

    <link href='/stylesheets/font.css?family=Slackey' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Fjalla+One' rel='stylesheet' type='text/css'>
    <link href='/stylesheets/font.css?family=Amethysta+One' rel='stylesheet' type='text/css'>
	  <script src="/javascripts/jquery.min.js"></script>
    <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![}]-->

    <script type="text/javascript" src="/javascripts/jquery-tapir.js"></script>

    <!-- remove or comment it to disable ajaxification -->   
    <!-- <script src="/javascripts/ajaxify.js"></script> -->

    

    
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-100485541-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>


<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <div id="wrapper">
    <header id="header" class="inner"><!-- for more effects see _animate.scss -->
<h1 class="animated bounceInDown">
    <div id="headerbg">
        Hacker and Geeker&#39;s Way
    </div>
</h1>
<span class="subtitle"></span>
<br>

<ul id="social-links" style="text-align:center; clear:both;">
  
  <!-- GitHub -->
  <li>
  <a target="_blank" rel="noopener" href="https://github.com/zhaozhiming" class="github" title="Github"></a>
  </li>
  
  
  
  
  <!-- Twitter -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.twitter.com/kingzzm" class="twitter" title="Twitter"></a>
  </li>
  
  
  <!-- LinkedIn -->
  <li>
  <a target="_blank" rel="noopener" href="http://www.linkedin.com/in/zhaozhiming" class="linkedin" title="LinkedIn"></a>
  </li>
  
  
  
  
  
  <!-- Stackoverflow -->
  <li>
  <a target="_blank" rel="noopener" href="http://stackoverflow.com/users/1954315/zhaozhiming" class="stackoverflow" title="Stackoverflow"></a>
  </li>
  
</ul>


<!-- use full url including 'index.html' for navigation bar if you are using ajax -->
<ul id="nav">
	<li id="ajax"><a href="/index.html">Home</a></li>
	<li id="ajax"><a href="/archives/index.html">Archives</a></li>
    <li><a href="/atom.xml">RSS</a></li>
    <li><a href="/about/index.html">About</a></li>
    
    <li>
    <div id="dark">
        <form action="//www.google.com.hk/search" method="get" accept-charset="UTF-8" id="search">
            <input type="hidden" name="sitesearch" value="https://zhaozhiming.github.io" />
            <input type="text" name="q" results="0" placeholder="Search..." x-webkit-speech />
        </form>
    </div>
    </li>
        
</ul>




</header>


<div id="toload">
<!-- begin toload -->
    <div id="content">
        <div class="inner">
<article class="post">
	<h2 class="title">高级 RAG 检索策略之混合检索</h2>
    <div class="meta">
        <div class="date">Published on: <time datetime="2024-06-01T01:14:39.000Z" itemprop="datePublished">6月 1, 2024</time>
</div>
        <div class="tags">Tags: 

<a href="/tags/elasticsearch/">elasticsearch</a> <a href="/tags/llamaindex/">llamaindex</a> <a href="/tags/rag/">rag</a> <a href="/tags/llama3/">llama3</a> <a href="/tags/rrf/">rrf</a>
</div>
    </div>
	<div class="entry-content"><img src="/images/post/2024/06/rag-hybrid-retrieve.jpg" class="" width="400" height="300">

<p>古人云：<strong>兼听则明，偏信则暗</strong>，意思是要同时听取各方面的意见，才能正确认识事物，只相信单方面的话，必然会犯片面性的错误。在 RAG（Retrieval Augmented Generation）应用中也是如此，如果我们可以同时从多个信息源中获取信息，那么我们的检索结果会更加全面和准确。今天我们就来介绍高级 RAG 检索策略中的混合检索，并在实际操作中结合 ElaticSearch 和 Llama3 来实现混合检索的效果。</p>
<span id="more"></span>

<h2 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h2><p>混合检索也叫融合检索，也叫多路召回，是指在检索过程中，同时使用多种检索方式，然后将多种检索结果进行融合，得到最终的检索结果。混合检索的优势在于可以充分利用多种检索方式的优势，弥补各种检索方式的不足，从而提高检索的准确性和效率，下面是混合检索的流程图：</p>
<img src="/images/post/2024/06/fusion-retrieve-flow.png" class="" width="1000" height="600">

<ul>
<li>首先是问题查询，这一过程的设计可以简单也可以复杂，简单的做法是直接将原始查询传递给检索器，而复杂一点的做法是通过 LLM（大语言模型）为原始查询生成子查询或相似查询，然后再将生成后的查询传递给检索器</li>
<li>然后是检索器执行检索，检索可以在同一数据源上进行不同维度的检索，比如向量检索和关键字检索，也可以是在不同数据源上进行检索，比如文档和数据库</li>
<li>检索过程从原来一个问题变成了多个问题检索，如果串行执行这些检索，那么检索的效率会大大降低，所以我们需要<strong>并行执行多个检索</strong>，这样才可以保证检索的效率</li>
<li>最后是融合检索结果，在这一过程中，我们需要对检索结果进行去重，因为在检索的多个结果中，有些结果可能是重复的，同时我们还需要对检索结果进行排序，排序方法一般采用 RRF（倒数排名融合），选出最匹配的检索结果</li>
</ul>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>为了更好地了解混合检索的原理和实现，今天我们将通过 LLM 应用框架<a target="_blank" rel="noopener" href="https://www.llamaindex.ai/">LlamaIndex</a>，结合 Meta 最新开源的模型<a target="_blank" rel="noopener" href="https://llama.meta.com/llama3/">Llama3</a>和开源搜索引擎<a target="_blank" rel="noopener" href="https://www.elastic.co/cn/elasticsearch/">ElasticSearch</a>，来实现一个高效的混合检索系统。在 RAG 检索过程中除了需要用到 LLM 的模型外，还需要用到 Embedding 模型和 Rerank 模型，这些模型我们也统一使用本地部署的模型，这样可以更好地了解各种模型的使用和部署。</p>
<h3 id="LlamaIndex-集成-Llama3"><a href="#LlamaIndex-集成-Llama3" class="headerlink" title="LlamaIndex 集成 Llama3"></a>LlamaIndex 集成 Llama3</h3><p>首先是进行 Llama3 的本地化部署，有多种工具可以部署 Llama3，比如 <a target="_blank" rel="noopener" href="https://ollama.com/">Ollama</a> 或 <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm">vllm</a>，而且这些工具都提供了兼容 OpenAI 的 API 接口，vllm 的部署方式可以参考我之前的<a href="https://zhaozhiming.github.io/2024/05/04/use-llama3-to-build-develop-team-copilot/">这篇文章</a>。</p>
<p>部署完成后，我们再看如何在 LlamaIndex 中集成 Llama3。虽然 LlamaIndex 提供了<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/">自定义 LLM</a>的功能，但继承自<code>CustomeLLM</code>类来实现自定义 LLM 的方式比较复杂，需要从头实现<code>complete</code>或<code>chat</code>等方法。这里推荐 LlamaInex 另外一个创建自定义 LLM 的方法，即使用<code>OpenAILike</code>类，这个类是对 <code>OpenAI</code> 类进行轻量级封装，只要有兼容 OpenAI 的 API 服务，就可以直接使用该类来获得 OpenAI LLM 的功能。</p>
<p>要使用<code>OpenAILike</code>类，首先需要安装相关依赖包<code>pip install llama-index-llms-openai-like</code>，然后使用以下代码进行集成：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.llms.openai_like <span class="keyword">import</span> OpenAILike</span><br><span class="line"><span class="keyword">from</span> llama_index.core.base.llms.types <span class="keyword">import</span> ChatMessage, MessageRole</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">llm = OpenAILike(</span><br><span class="line">    model=<span class="string">&quot;llama3&quot;</span>,</span><br><span class="line">    api_base=<span class="string">&quot;you-local-llama3-api&quot;</span>,</span><br><span class="line">    api_key=<span class="string">&quot;fake_key&quot;</span>,</span><br><span class="line">    is_chat_model=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">prompt_str = <span class="string">&quot;Please generate related movies to &#123;movie_name&#125;&quot;</span></span><br><span class="line">prompt_tmpl = PromptTemplate(prompt_str)</span><br><span class="line">response = llm.chat(</span><br><span class="line">    [</span><br><span class="line">        ChatMessage(</span><br><span class="line">            role=MessageRole.SYSTEM,</span><br><span class="line">            content=<span class="string">&quot;You are a helpful assistant.&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">        ChatMessage(</span><br><span class="line">            role=MessageRole.USER,</span><br><span class="line">            content=prompt_tmpl.<span class="built_in">format</span>(movie_name=<span class="string">&quot;Avengers&quot;</span>),</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;response: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">response: assistant: Here are some movie recommendations that are similar to the Avengers franchise:</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> **Guardians of the Galaxy** (<span class="number">2014</span>) - Another Marvel superhero team-up film, <span class="keyword">with</span> a fun <span class="keyword">and</span> quirky tone.</span><br><span class="line"><span class="number">2.</span> **The Justice League** (<span class="number">2017</span>) - A DC Comics adaptation featuring iconic superheroes like Superman, Batman, Wonder Woman, <span class="keyword">and</span> more.</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<ul>
<li>在<code>OpenAILike</code>对象中，参数<code>model</code>为模型名称，<code>api_base</code>为本地 Llama3 的 API 服务地址</li>
<li><code>api_key</code>可以随便填写，但不能不传这个参数，否则会出现连接超时的错误</li>
<li><code>is_chat_model</code>为是否是 chat 模型，因为 OpenAI 的模型分为 chat 模型和非 chat 模型</li>
<li>然后我们使用 LLM 对象进行了一个普通的对话，结果可以正常返回</li>
</ul>
<h3 id="LlamaIndex-集成-ElasticSearch"><a href="#LlamaIndex-集成-ElasticSearch" class="headerlink" title="LlamaIndex 集成 ElasticSearch"></a>LlamaIndex 集成 ElasticSearch</h3><p>在 RAG 应用中向量数据库是必不可少的一项功能，而 Elasticsearch 能够存储各种类型的数据，包括结构化和非结构化数据，并且支持全文检索和向量检索。ElasticSearch 本地环境的安装和部署可以参考我之前的<a href="https://zhaozhiming.github.io/2024/01/13/llamaindex-eleasticsearch-rga-practice/">这篇文章</a>。</p>
<p>部署完 ElasticSearch 后，还需要安装 LlamaIndex 的 Elasticsearch 依赖包<code>pip install llama-index-vector-stores-elasticsearch</code>，然后使用以下代码示例就可以集成 ElasticSearch：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> ElasticsearchStore</span><br><span class="line"></span><br><span class="line">es = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;my_index&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>index_name</code> 是 ElasticSearch 的索引名称，<code>es_url</code> 是 ElasticSearch 服务的地址</li>
</ul>
<h3 id="自定义-Embedding-和-Rerank-模型"><a href="#自定义-Embedding-和-Rerank-模型" class="headerlink" title="自定义 Embedding 和 Rerank 模型"></a>自定义 Embedding 和 Rerank 模型</h3><p>在高级 RAG 的检索过程中，需要用到 Embedding 模型来对文档和问题进行向量化，然后使用 Rerank 模型对检索结果进行重排序。同样有很多工具可以部署这 2 种模型，比如<a target="_blank" rel="noopener" href="https://github.com/huggingface/text-embeddings-inference">TEI</a> 和 <a target="_blank" rel="noopener" href="https://inference.readthedocs.io/en/latest/">Xinference</a>等。这里我们使用 TEI 来部署这 2 种模型，TEI 和模型的部署可以参考我之前的<a href="https://zhaozhiming.github.io/2024/01/18/rerank-model-deploy-and-usage/">这篇文章</a>。</p>
<p>Embedding 模型的启动命令如下，这里我们使用了<a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-base-en-v1.5">BAAI&#x2F;bge-base-en-v1.5</a>这个 Embeddings 模型，服务端口为 6006：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text-embeddings-router --model-id BAAI/bge-base-en-v1.5 --revision refs/pr/4 --port 6006</span><br></pre></td></tr></table></figure>

<p>Rerank 模型的启动命令如下，这里我们使用了<a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-reranker-base">BAAI&#x2F;bge-reranker-base</a>这个 Rerank 模型，服务端口为 7007：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text-embeddings-router --model-id BAAI/bge-reranker-base --revision refs/pr/4 --port 7007</span><br></pre></td></tr></table></figure>

<h2 id="多种检索方式"><a href="#多种检索方式" class="headerlink" title="多种检索方式"></a>多种检索方式</h2><h3 id="数据入库"><a href="#数据入库" class="headerlink" title="数据入库"></a>数据入库</h3><p>在介绍检索之前，我们先来了解下 LlamaIndex 如何使用 ElasticSearch 对文档进行解析和入库，这里的测试文档还是用维基百科上的<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> ElasticsearchStore</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader, StorageContext</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llms <span class="keyword">import</span> CustomEmbeddings</span><br><span class="line"></span><br><span class="line">store = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;avengers&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">)</span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">node_parser = SentenceSplitter(chunk_size=<span class="number">256</span>, chunk_overlap=<span class="number">50</span>)</span><br><span class="line">storage_context = StorageContext.from_defaults(vector_store=store)</span><br><span class="line">embed_model = CustomEmbeddings(</span><br><span class="line">    model=<span class="string">&quot;BAAI/bge-base-en-v1.5&quot;</span>, url=<span class="string">&quot;http://localhost:6006&quot;</span></span><br><span class="line">)</span><br><span class="line">VectorStoreIndex.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    transformations=[node_parser],</span><br><span class="line">    embed_model=embed_model,</span><br><span class="line">    storage_context=storage_context,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li>首先定义了一个 ElasticsearchStore 对象来连接 ElaticSearch 本地服务</li>
<li>然后使用 SimpleDirectoryReader 加载本地的文档数据</li>
<li>使用 SentenceSplitter 对文档进行分块处理，因为 TEI 的输入 Token 数最大只能 512，所以这里的 chunk_size 设置为 256，chunk_overlap 设置为 50</li>
<li>构建 StorageContext 对象，指定向量存储为之前定义的 ElasticsearchStore 对象</li>
<li>创建一个自定义 Embeddings 对象，使用的是 TEI 部署的 Embeddings 模型服务，这里<code>CustomEmbeddings</code>的代码可以参考<a href="https://zhaozhiming.github.io/2024/01/13/llamaindex-eleasticsearch-rga-practice/">这篇文章</a>中的代码</li>
<li>最后使用 VectorStoreIndex 对象将文档数据入库</li>
</ul>
<p>当执行完代码后，可以在 ElasticSearch 的<code>avengers</code>索引中看到文档数据，如下图所示：</p>
<img src="/images/post/2024/06/hybrid-search-avengers-index.png" class="" width="1000" height="600">

<h3 id="全文检索"><a href="#全文检索" class="headerlink" title="全文检索"></a>全文检索</h3><p>数据入库后，我们再来看下如何在 LlamaIndex 中使用 Elasticsearch 进行全文检索。</p>
<p>全文检索是 Elasticsearch 的基本功能，有时候也叫关键字检索，是指根据关键字在文档中进行检索，支持精确匹配，同时高级功能也支持模糊匹配、同义词替换、近义词搜索等。在 LlamaIndex 中使用 Elasticsearch 进行全文检索的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> AsyncBM25Strategy</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line">text_store = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;avengers&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">    retrieval_strategy=AsyncBM25Strategy(),</span><br><span class="line">)</span><br><span class="line">Settings.embed_model = embed_model</span><br><span class="line">text_index = VectorStoreIndex.from_vector_store(</span><br><span class="line">    vector_store=text_store,</span><br><span class="line">)</span><br><span class="line">text_retriever = text_index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>这里重新定义了一个 ElasticsearchStore 对象，但这次指定了检索策略为 BM25，如果要使用全文检索则必须指定这个检索策略</li>
<li>将<code>ElasticsearchStore</code>对象作为参数来创建<code>VectorStoreIndex</code> 对象</li>
<li>最后通过<code>VectorStoreIndex</code>对象创建全文检索的检索器，这里设置检索结果的数量为 2</li>
</ul>
<blockquote>
<p>BM25 是一种在信息检索领域广泛采用的排名函数，主要用于评估文档与用户查询的相关性。该算法的基本原理是将用户查询（query）分解为若干语素（qi），然后计算每个语素与搜索结果之间（document D）的相关性。通过累加这些相关性得分，BM25 最终得出查询与特定文档之间的总相关性评分。这种检索策略在现代搜索引擎中非常常见。</p>
</blockquote>
<h3 id="向量检索"><a href="#向量检索" class="headerlink" title="向量检索"></a>向量检索</h3><p>我们再来了解 LlamaIndex 中如何使用 Elasticsearch 进行向量检索。</p>
<p>向量检索是一种基于机器学习的信息检索技术，它使用数学向量来表示文档和查询。在 LlamaIndex 中使用 Elasticsearch 进行向量检索有 2 种检索策略，分别是<code>Dense</code>和<code>Sparse</code>，这两种策略的区别在于向量的稠密度，<code>Dense</code>检索的号码每一位都是有用的数字，就像一个充满数字的电话号码，而<code>Sparse</code>检索的号码大部分都是零，只有少数几个位置有数字，就像一个电话号码大部分是零，只有几个位置有数字。如果需要更精细、更复杂的检索方法，用<code>Dense</code>检索，如果需要简单快速的方法，用<code>Sparse</code>检索。<code>ElasicsearchStore</code>类默认的检索策略是<code>Dense</code>，下面是向量检索的代码示例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> AsyncDenseVectorStrategy, AsyncSparseVectorStrategy</span><br><span class="line"></span><br><span class="line">vector_store = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;avengers&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">    retrieval_strategy=AsyncDenseVectorStrategy(),</span><br><span class="line">    <span class="comment"># retrieval_strategy=AsyncSparseVectorStrategy(model_id=&quot;.elser_model_2&quot;),</span></span><br><span class="line">)</span><br><span class="line">Settings.embed_model = embed_model</span><br><span class="line">vector_index = VectorStoreIndex.from_vector_store(</span><br><span class="line">    vector_store=vector_store,</span><br><span class="line">)</span><br><span class="line">vector_retriever = vector_index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>向量检索的代码和全文检索的代码类似</li>
<li>如果是使用<code>Dense</code>检索策略，可以指定<code>retrieval_strategy=AsyncDenseVectorStrategy()</code>，也可以不指定<code>retrieval_strategy</code>参数</li>
<li>如果是使用<code>Sparse</code>检索策略，需要指定<code>retrieval_strategy=AsyncSparseVectorStrategy(model_id=&quot;.elser_model_2&quot;)</code>，这里需要额外部署 ElasticSearch 的 <a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html">ELSER 模型</a></li>
</ul>
<h3 id="混合检索"><a href="#混合检索" class="headerlink" title="混合检索"></a>混合检索</h3><p>定义好了 2 种检索器后，我们再来了解如何将这些检索进行融合，在 LlamaIndex 的 ElasticsearchStore 类中提供了混合检索的方法，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> AsyncDenseVectorStrategy</span><br><span class="line"></span><br><span class="line">vector_store = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;avengers&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">    retrieval_strategy=AsyncDenseVectorStrategy(hybrid=<span class="literal">True</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li>这里的检索策略还是使用<code>Dense</code>检索策略，但是指定了<code>hybrid=True</code>参数，表示使用混合检索</li>
</ul>
<p>设置了混合检索策略后，在融合检索结果时会自动使用 Elasicsearch 的 RRF 功能。</p>
<blockquote>
<p>RRF（倒数排名融合） 是一种融合检索算法，用于结合多个检索结果列表。每个结果列表中的每个文档被分配一个分数，分数基于文档在列表中的排名位置。该算法的基本思想是，通过对多个检索器的结果进行融合，来提高检索性能。</p>
</blockquote>
<p>但在 Elasticsearch 的免费版本中，这个功能是<strong>不可用</strong>的：</p>
<img src="/images/post/2024/06/es-rrf-support.png" class="" width="1000" height="600">

<p>因此我们需要自己实现 RRF 功能，RRF 的论文可以看<a target="_blank" rel="noopener" href="https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf">这里</a>，下面是 RRF 的代码实现：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="keyword">from</span> llama_index.core.schema <span class="keyword">import</span> NodeWithScore</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fuse_results</span>(<span class="params">results_dict, similarity_top_k: <span class="built_in">int</span> = <span class="number">2</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Fuse results.&quot;&quot;&quot;</span></span><br><span class="line">    k = <span class="number">60.0</span></span><br><span class="line">    fused_scores = &#123;&#125;</span><br><span class="line">    text_to_node = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算倒数排名分数</span></span><br><span class="line">    <span class="keyword">for</span> nodes_with_scores <span class="keyword">in</span> results_dict.values():</span><br><span class="line">        <span class="keyword">for</span> rank, node_with_score <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">            <span class="built_in">sorted</span>(</span><br><span class="line">                nodes_with_scores, key=<span class="keyword">lambda</span> x: x.score <span class="keyword">or</span> <span class="number">0.0</span>, reverse=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        ):</span><br><span class="line">            text = node_with_score.node.get_content()</span><br><span class="line">            text_to_node[text] = node_with_score</span><br><span class="line">            <span class="keyword">if</span> text <span class="keyword">not</span> <span class="keyword">in</span> fused_scores:</span><br><span class="line">                fused_scores[text] = <span class="number">0.0</span></span><br><span class="line">            fused_scores[text] += <span class="number">1.0</span> / (rank + k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结果按分数排序</span></span><br><span class="line">    reranked_results = <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">sorted</span>(fused_scores.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结果还原为节点集合</span></span><br><span class="line">    reranked_nodes: <span class="type">List</span>[NodeWithScore] = []</span><br><span class="line">    <span class="keyword">for</span> text, score <span class="keyword">in</span> reranked_results.items():</span><br><span class="line">        reranked_nodes.append(text_to_node[text])</span><br><span class="line">        reranked_nodes[-<span class="number">1</span>].score = score</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reranked_nodes[:similarity_top_k]</span><br></pre></td></tr></table></figure>

<ul>
<li>方法的参数<code>results_dict</code>是所有检索器的检索结果集合，<code>similarity_top_k</code>是最相似的结果数量</li>
<li>假设<code>results_dict</code>的值是<code>&#123;&#39;full-text&#39;: [nodes], &#39;vector&#39;: [nodes]&#125;</code>，这个方法方法的作用是将所有的检索结果节点进行融合，然后选出最相似的<code>similarity_top_k</code>个节点</li>
<li>方法开头是初始化一些变量，<code>k</code> 用于计算倒数排名分数，<code>fused_scores</code> 用于存储节点文本和融合后分数的映射，<code>text_to_node</code> 用于存储节点文本到节点的映射</li>
<li>然后是计算每个节点的倒数排名分数，先将 <code>results_dict</code> 中的每个节点按照分数进行排序，然后计算每个节点的倒数排名分数，将结果保存到 <code>fused_scores</code> 中，同时将节点文本和节点的关系保存到 <code>text_to_nodes</code> 中</li>
<li>接着再对 <code>fused_scores</code> 按照倒数排名分数进行排序，得到 <code>reranked_results</code></li>
<li>然后根据 <code>reranked_results</code> 将结果还原成节点集合的形式，并将节点的分数设置为融合后的分数，最终结果保存到 <code>reranked_nodes</code> 列表中</li>
<li>最后返回最相似的结果，返回 <code>reranked_nodes</code> 列表中的前 <code>similarity_top_k</code> 个节点</li>
</ul>
<p>定义好融合函数后，我们再定义一个方法来执行多个检索器，这个方法返回的结果就是融合函数的参数 <code>results_dict</code>，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.asyncio <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_queries</span>(<span class="params">query, retrievers</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run query against retrievers.&quot;&quot;&quot;</span></span><br><span class="line">    tasks = []</span><br><span class="line">    <span class="keyword">for</span> i, retriever <span class="keyword">in</span> <span class="built_in">enumerate</span>(retrievers):</span><br><span class="line">        tasks.append(retriever.aretrieve(query))</span><br><span class="line"></span><br><span class="line">    task_results = <span class="keyword">await</span> tqdm.gather(*tasks)</span><br><span class="line"></span><br><span class="line">    results_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, query_result <span class="keyword">in</span> <span class="built_in">enumerate</span>(task_results):</span><br><span class="line">        results_dict[(query, i)] = query_result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results_dict</span><br></pre></td></tr></table></figure>

<ul>
<li>方法的参数<code>query</code>是原始问题，<code>retrievers</code>是多个检索器的集合</li>
<li>将问题传给每个检索器，构建异步任务列表<code>tasks</code></li>
<li>然后使用<code>await tqdm.gather(*tasks)</code>来<strong>并行</strong>执行所有的检索器，并行执行可以提高检索效率</li>
<li>最后将检索结果保存到<code>results_dict</code>中，返回<code>results_dict</code></li>
</ul>
<p>因为我们使用了异步方式进行检索，原先的<code>CustomEmbeddings</code>中的方法也需要修改，示例代码如下：</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="addition">+import asyncio</span></span><br><span class="line"></span><br><span class="line"><span class="deletion">-    def _aget_query_embedding(self, query: str) -&gt; Embedding:</span></span><br><span class="line"><span class="deletion">-        return get_embedding(text=query, model=self._model, url=self._url)</span></span><br><span class="line"><span class="addition">+    async def _aget_query_embedding(self, query: str) -&gt; Embedding:</span></span><br><span class="line"><span class="addition">+        loop = asyncio.get_event_loop()</span></span><br><span class="line"><span class="addition">+        return await loop.run_in_executor(</span></span><br><span class="line"><span class="addition">+            None, get_embedding, query, self._model, self._url</span></span><br><span class="line"><span class="addition">+        )</span></span><br></pre></td></tr></table></figure>

<p>然后我们构建一个融合检索器来将上面定义的方法组合到一起，示例代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> QueryBundle</span><br><span class="line"><span class="keyword">from</span> llama_index.core.retrievers <span class="keyword">import</span> BaseRetriever</span><br><span class="line"><span class="keyword">from</span> llama_index.core.schema <span class="keyword">import</span> NodeWithScore</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FusionRetriever</span>(<span class="title class_ inherited__">BaseRetriever</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Ensemble retriever with fusion.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        retrievers: <span class="type">List</span>[BaseRetriever],</span></span><br><span class="line"><span class="params">        similarity_top_k: <span class="built_in">int</span> = <span class="number">2</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Init params.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>._retrievers = retrievers</span><br><span class="line">        <span class="variable language_">self</span>._similarity_top_k = similarity_top_k</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_retrieve</span>(<span class="params">self, query_bundle: QueryBundle</span>) -&gt; <span class="type">List</span>[NodeWithScore]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Retrieve.&quot;&quot;&quot;</span></span><br><span class="line">        results = asyncio.run(</span><br><span class="line">            run_queries(query_bundle.query_str, <span class="variable language_">self</span>._retrievers)</span><br><span class="line">        )</span><br><span class="line">        final_results = fuse_results(results, similarity_top_k=<span class="variable language_">self</span>._similarity_top_k)</span><br><span class="line">        <span class="keyword">return</span> final_results</span><br></pre></td></tr></table></figure>

<ul>
<li>这个融合检索器的类继承自<code>BaseRetriever</code>类，重写了<code>_retrieve</code>方法</li>
<li>构造方法中的参数<code>retrievers</code>是多个检索器的集合，<code>similarity_top_k</code>是最相似的结果数量</li>
<li>在<code>_retrieve</code>方法中，调用了<code>run_queries</code>方法来获取检索结果<code>results</code></li>
<li>然后调用了<code>fuse_results</code>方法来融合检索结果并返回</li>
</ul>
<p>我们来看融合检索器运行后的检索结果，代码示例如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">fusion_retriever = FusionRetriever(</span><br><span class="line">    [text_retriever, vector_retriever], similarity_top_k=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line">question = <span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span></span><br><span class="line">nodes = fusion_retriever.retrieve(question)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node content: <span class="subst">&#123;node.text[:<span class="number">100</span>]&#125;</span>...&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node score: <span class="subst">&#123;node.score&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">--------------------------------------------------</span><br><span class="line">node content: In the Eastern European country of Sokovia, the Avengers—Tony Stark, Thor, Bruce Banner, Steve Roger...</span><br><span class="line">node score: <span class="number">0.03306010928961749</span></span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">node content: Thor departs to consult <span class="keyword">with</span> Dr. Erik Selvig on the apocalyptic future he saw <span class="keyword">in</span> his hallucination, ...</span><br><span class="line">node score: <span class="number">0.016666666666666666</span></span><br></pre></td></tr></table></figure>

<ul>
<li>首先定义了一个 FusionRetriever 对象，传入了全文检索器和向量检索器，同时设置了最相似的结果数量为 2</li>
<li>然后传入了一个问题，获取检索结果</li>
</ul>
<p>从结果中可以看到，检索结果节点返回的分数是经过 RRF 融合后的分数，分数值比较低，与原始的 Rerank 分数值不太匹配，这时我们可以使用 Rerank 模型来对检索结果进行重排序。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"></span><br><span class="line">rerank = CustomRerank(</span><br><span class="line">    model=<span class="string">&quot;BAAI/bge-reranker-base&quot;</span>, url=<span class="string">&quot;http://localhost:7007&quot;</span>, top_n=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line">Settings.llm = llm</span><br><span class="line">query_engine = RetrieverQueryEngine(fusion_retriever, node_postprocessors=[rerank])</span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;response: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> response.source_nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node content: <span class="subst">&#123;node.text[:<span class="number">100</span>]&#125;</span>...&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node score: <span class="subst">&#123;node.score&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">response: Tony Stark <span class="keyword">and</span> Bruce Banner.</span><br><span class="line">--------------------------------------------------</span><br><span class="line">node content: In the Eastern European country of Sokovia, the Avengers—Tony Stark, Thor, Bruce Banner, Steve Roger...</span><br><span class="line">node score: <span class="number">0.8329173</span></span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">node content: Thor departs to consult <span class="keyword">with</span> Dr. Erik Selvig on the apocalyptic future he saw <span class="keyword">in</span> his hallucination, ...</span><br><span class="line">node score: <span class="number">0.24689633</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>CustomRerank</code>类是一个自定义的 Rerank 类，这个类的代码可以参考<a href="https://zhaozhiming.github.io/2024/01/18/rerank-model-deploy-and-usage/">这篇文章</a>中的代码</li>
<li>在系统设置中设置了 LLM 模型来生成答案</li>
<li>通过混合检索器构建查询引擎，并在<code>node_postprocessors</code>参数中传入了 Rerank 模型，表示在检索结果后使用 Rerank 模型对检索结果进行重排序</li>
<li>最后传入问题，获取检索结果</li>
</ul>
<p>从结果中可以看到，检索结果节点返回的分数是经过 Rerank 模型重排序后的分数，分数值比较高，这样我们的混合检索系统就构建完成了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>混合检索是一种在 RAG 应用中常用的检索策略，通过融合多种检索方式，可以提高检索的准确性和效率。今天我们通过 LlamaIndex 的代码实践，了解了构建混合检索系统的流程，同时也学习了如何使用 Llama3 和 ElasticSearch 来实现混合检索的效果，以及混合检索中一些常见的检索策略和排序算法。</p>
<p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>
</div>

</article>
<section id="appreciates">
  <center>
	<h2>赞赏</h2>
    <div class="post-footer">
      <div>
	    <h4>如果文章对您有所帮助，可以捐赠我喝杯咖啡😌，捐赠方式：</h4>
        <div class="digital-wallet">
          <h6>BTC 地址：3LYgSyf7ddMALwGWPQr3PY4wzjsTDdg1oV</h6>
          <h6>ETH 地址：0x0C9b27c89A61aadb0aEC24CA8949910Cbf77Aa73</h6>
        </div>
        <div class="wechat_appreciates">
          <img src="/images/wechat_appreciates.png" alt="qrcode" width="250" >
        </div>
      </div>
      <div>
	    <h4>文章已同步更新公众号，欢迎关注</h4>
        <div class="wechat_appreciates">
          <img src="/images/wxgzh_qrcode.jpg" alt="qrcode" width="250" >
        </div>
      </div>
    </div>
  </center>
</section>



    
      <script type="text/javascript">
        var disqus_config = function () {
            this.page.url = 'https://zhaozhiming.github.io/2024/06/01/llamaindex-llama3-es-hybrid-search/';
            this.page.identifier = 'https://zhaozhiming.github.io/2024/06/01/llamaindex-llama3-es-hybrid-search/';
        };

        (function() {
          var d = document, s = d.createElement('script');
          s.src = '//zhaozhiming.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    



<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>

</div>

    </div>
    <footer id="footer">
    <div style="display:inline">
    Copyright &copy; 2025

    赵芝明
. Powered by <a href="http://zespia.tw/hexo/" target="_blank">Hexo</a> |
    Theme is <a target="_blank" rel="noopener" href="https://github.com/wd/hexo-fabric">hexo-fabric</a>, fork from <a target="_blank" rel="noopener" href="http://github.com/panks/fabric">fabric</a> by <a target="_blank" rel="noopener" href="http://panks.me">Pankaj Kumar</a>
</div>


    </footer>
    <script src="/javascripts/fabric.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script>
 <!-- Delete or comment this line to disable Fancybox -->



<!-- end toload --> 
</div>
</div>
<script src="/javascripts/jquery.ui.totop.js" type="text/javascript"></script>
<script type="text/javascript">
/*<![CDATA[*/
;(function($){$().UItoTop({easingType:'easeOutCirc'});})(jQuery); 
/*]]>*/
</script><!-- remove it to remove the scroll to top button -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5608723650603289" crossorigin="anonymous"></script>
</body>
</html>
