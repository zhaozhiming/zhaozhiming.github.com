<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hacker and Geeker&#39;s Way</title>
  
  
  <link href="https://zhaozhiming.github.io/atom.xml" rel="self"/>
  
  <link href="https://zhaozhiming.github.io/"/>
  <updated>2024-10-19T09:17:34.760Z</updated>
  <id>https://zhaozhiming.github.io/</id>
  
  <author>
    <name>赵芝明</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Go 开发中你应该了解的 5 种 Mock 方法</title>
    <link href="https://zhaozhiming.github.io/2024/10/18/maybe-you-donot-known-5-way-to-mock-golang-programm/"/>
    <id>https://zhaozhiming.github.io/2024/10/18/maybe-you-donot-known-5-way-to-mock-golang-programm/</id>
    <published>2024-10-18T01:17:48.000Z</published>
    <updated>2024-10-19T09:17:34.760Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/10/golang-mock-tech.jpeg" class="" width="400" height="300"><p>在软件开发过程中，单元测试是确保代码质量的重要环节，而在编写单元测试时，我们通常需要隔离待测试的代码与其依赖的外部组件，例如引用的外部方法、数据库等。Mock 技术可以帮助我们模拟这些外部组件，控制它们的行为和输出，从而让我们可以专注于测试目标代码的逻辑。本文将介绍在 Golang 中常用的 5 种 Mock 方法，帮助你在编写单元测试时更加得心应手。</p><span id="more"></span><h2 id="Testify-Mock"><a href="#Testify-Mock" class="headerlink" title="Testify Mock"></a>Testify Mock</h2><p><a href="https://github.com/stretchr/testify">Testify</a> 是 Go 生态中一个非常流行的测试工具库，主要用于简化 Go 语言中的单元测试工作。它是一个包含多个包的集合，提供了断言、Mock 和其他便捷的测试功能。</p><p>Testify 的 Mock 是其中一个功能强大的模块，它提供了模拟接口和方法调用的能力，支持参数匹配、调用次数和顺序的验证。它允许开发者通过链式调用设置不同的返回值和行为，并能精确匹配参数或自定义参数验证，帮助模拟外部依赖如 API 调用，使其在复杂的逻辑测试中非常灵活和高效。它与 Go 原生测试框架无缝集成，提供直观易用的 API，简化了测试代码编写和维护。</p><p>下面我们通过代码示例介绍 Testify Mock 的功能，首先来看被测试的函数：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;context&quot;</span></span><br><span class="line"><span class="string">&quot;encoding/json&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/org/proj/server/third-party/amazonx&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/aws/aws-sdk-go-v2/aws&quot;</span></span><br><span class="line"><span class="string">&quot;github.com/aws/aws-sdk-go-v2/service/secretsmanager&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> (</span><br><span class="line">service <span class="keyword">struct</span> &#123;</span><br><span class="line">amazon amazonx.Amazon</span><br><span class="line">&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(svc *service)</span></span> GetTokenFromAWSSecret(c context.Context) (<span class="type">string</span>, <span class="type">error</span>) &#123;</span><br><span class="line">input := &amp;secretsmanager.GetSecretValueInput&#123;</span><br><span class="line">SecretId:     aws.String(<span class="string">&quot;SECRET_ID&quot;</span>),</span><br><span class="line">VersionStage: aws.String(<span class="string">&quot;AWSCURRENT&quot;</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resp, err := svc.amazon.GetSecretValue(c, input)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;&quot;</span>, err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resMap := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">string</span>]<span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line"><span class="keyword">if</span> err := json.Unmarshal([]<span class="type">byte</span>(*resp.SecretString), &amp;resMap); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;&quot;</span>, err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> resMap[<span class="string">&quot;token&quot;</span>].(<span class="type">string</span>), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>被测试方法 <code>GetTokenFromAWSSecret</code> 输入参数是一个 <code>context.Context</code> 对象，返回一个 <code>string</code> 类型的 <code>token</code> 和一个 <code>error</code> 对象</li><li><code>GetTokenFromAWSSecret</code> 方法依赖于 <code>amazonx.Amazon</code> 接口，通过 <code>svc.amazon.GetSecretValue</code> 方法获取 AWS Secret Value，并解析其中的 <code>token</code> 字段。</li></ul><p>了解完被测函数后， 我们来看如何使用 Testify Mock 对 <code>amazonx.Amazon</code> 接口进行 Mock：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;context&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;github.com/aws/aws-sdk-go-v2/service/secretsmanager&quot;</span></span><br><span class="line">    <span class="string">&quot;github.com/stretchr/testify/mock&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> MockAmazon <span class="keyword">struct</span> &#123;</span><br><span class="line">mock.Mock</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *MockAmazon)</span></span> GetSecretValue(ctx context.Context, input *secretsmanager.GetSecretValueInput) (*secretsmanager.GetSecretValueOutput, <span class="type">error</span>) &#123;</span><br><span class="line">args := m.Called(ctx, input)</span><br><span class="line"><span class="keyword">return</span> args.Get(<span class="number">0</span>).(*secretsmanager.GetSecretValueOutput), args.Error(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>首先我们使用 Testify Mock 定义了一个 <code>MockAmazon</code> 结构体</li><li>然后使用 Mock 对象来实现 <code>amazonx.Amazon</code> 接口的 <code>GetSecretValue</code> 方法</li></ul><p>接下来我们编写单元测试代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;context&quot;</span></span><br><span class="line"><span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/aws/aws-sdk-go-v2/service/secretsmanager&quot;</span></span><br><span class="line"><span class="string">&quot;github.com/stretchr/testify/assert&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestGetTokenFromAWSSecretSuccess</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line">mockAmazon := <span class="built_in">new</span>(MockAmazon)</span><br><span class="line">svc := &amp;service&#123;</span><br><span class="line">amazon: mockAmazon,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ctx := context.TODO()</span><br><span class="line">secretString := <span class="string">`&#123;&quot;token&quot;: &quot;test-token&quot;&#125;`</span></span><br><span class="line">mockAmazon.On(<span class="string">&quot;GetSecretValue&quot;</span>, ctx,</span><br><span class="line">    mock.AnythingOfType(<span class="string">&quot;*secretsmanager.GetSecretValueInput&quot;</span>)).</span><br><span class="line">Return(&amp;secretsmanager.GetSecretValueOutput&#123;</span><br><span class="line">    SecretString: &amp;secretString,</span><br><span class="line">&#125;, <span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line">token, err := svc.GetTokenFromAWSSecret(ctx)</span><br><span class="line">assert.NoError(t, err)</span><br><span class="line">assert.Equal(t, <span class="string">&quot;test-token&quot;</span>, token)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在测试函数中，我们首先创建了一个 <code>MockAmazon</code> 对象，并将其传入 <code>service</code> 结构体中</li><li>然后模拟 <code>GetSecretValue</code> 方法的行为，设置返回值为 <code>&#123; &quot;token&quot;: &quot;test-token&quot; &#125;</code></li><li>最后调用 <code>GetTokenFromAWSSecret</code> 方法，验证返回值是否符合预期</li></ul><p>这里需要注意的地方是，<code>mockAmazon</code> 对象需要模拟真实对象 <code>amazonx.Amazon</code> 的<strong>所有方法</strong>，如果有其中一个方法没有实现的话，就<strong>无法赋值</strong>给 <code>service</code> 结构体。一旦真实对象的接口方法很多，那么手动实现所有方法就会十分繁琐，这个时候如果有一种工具可以自动生成 Mock 方法就会非常方便，这就是我们接下来要介绍的 Mock 工具——GoMock。</p><h2 id="GoMock"><a href="#GoMock" class="headerlink" title="GoMock"></a>GoMock</h2><p><a href="https://github.com/uber-go/mock">GoMock</a> 是一个为 Go 语言提供模拟框架的工具库，由 Uber 维护，支持 Go 官方支持的最新两个版本。GoMock 允许开发者使用 <code>mockgen</code> 工具来生成用于测试的模拟对象，支持源文件模式和包模式生成模拟对象，并且与 Go 的内置 <code>testing</code> 包兼容，同时支持类型安全的模拟方法，并允许设置详细的期望调用。</p><p>使用 GoMock 首先需要安装 <code>mockgen</code> 工具：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go install github.com/golang/mock/mockgen@latest</span><br></pre></td></tr></table></figure><p>然后在要模拟的接口文件中添加接口生成注释，以之前的被测方法为例，我们需要在 <code>amazonx</code> 包中的 <code>amazon.go</code> 文件中添加如下注释：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//go:generate mockgen -destination ./amazon_mock.go -package amazonx -source amazon.go Amazon</span></span><br></pre></td></tr></table></figure><p>这个注释表示使用 <code>mockgen</code> 在当前目录下生成一个 <code>amazon_mock.go</code> 文件，包名为 <code>amazonx</code>，并且使用 <code>amazon.go</code> 文件中的 <code>Amazon</code> 接口生成 Mock 对象。一般支持 Golang 的编辑器都支持 <code>go:generate</code> 注释，可以直接在编辑器中执行生成 Mock 对象的命令。</p><img src="/images/post/2024/10/golang-mockgen.png" class="" width="1000" height="600"><p>生成后的 <code>amazon_mock.go</code> 文件内容大概是这个样子：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// MockAmazon is a mock of Amazon interface.</span></span><br><span class="line"><span class="keyword">type</span> MockAmazon <span class="keyword">struct</span> &#123;</span><br><span class="line">ctrl     *gomock.Controller</span><br><span class="line">recorder *MockAmazonMockRecorder</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MockAmazonMockRecorder is the mock recorder for MockAmazon.</span></span><br><span class="line"><span class="keyword">type</span> MockAmazonMockRecorder <span class="keyword">struct</span> &#123;</span><br><span class="line">mock *MockAmazon</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NewMockAmazon creates a new mock instance.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewMockAmazon</span><span class="params">(ctrl *gomock.Controller)</span></span> *MockAmazon &#123;</span><br><span class="line">mock := &amp;MockAmazon&#123;ctrl: ctrl&#125;</span><br><span class="line">mock.recorder = &amp;MockAmazonMockRecorder&#123;mock&#125;</span><br><span class="line"><span class="keyword">return</span> mock</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Other methods...</span></span><br></pre></td></tr></table></figure><p>我们可以使用 <code>NewMockAmazon</code> 方法来模拟 <code>amazonx.Amazon</code> 接口，下面来看下我们使用 GoMock 写的测试代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;context&quot;</span></span><br><span class="line"><span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/org/proj/server/third-party/amazonx&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/golang/mock/gomock&quot;</span></span><br><span class="line"><span class="string">&quot;github.com/stretchr/testify/assert&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestGetTokenFromAWSSecretByGomock</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line">ctrl := gomock.NewController(t)</span><br><span class="line"><span class="keyword">defer</span> ctrl.Finish()</span><br><span class="line"></span><br><span class="line">mockAmazon := amazonx.NewMockAmazon(ctrl)</span><br><span class="line">svc := &amp;service&#123;</span><br><span class="line">amazon: mockAmazon,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ctx := context.TODO()</span><br><span class="line"></span><br><span class="line">secretString := <span class="string">`&#123;&quot;token&quot;:&quot;test-token&quot;&#125;`</span></span><br><span class="line">mockAmazon.EXPECT().GetSecretValue(ctx, gomock.Any()).</span><br><span class="line">Return(&amp;secretsmanager.GetSecretValueOutput&#123;</span><br><span class="line">SecretString: &amp;secretString,</span><br><span class="line">&#125;, <span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line">token, err := svc.GetTokenFromAWSSecret(ctx)</span><br><span class="line">assert.NoError(t, err)</span><br><span class="line">assert.Equal(t, <span class="string">&quot;test-token&quot;</span>, token)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在测试函数中，我们首先创建了一个 GoMock 的 <code>controller</code> 对象，然后使用 <code>amazonx.NewMockAmazon</code> 方法创建一个 <code>MockAmazon</code> 对象</li><li>然后通过 <code>EXPECT</code> 方法设置 <code>GetSecretValue</code> 方法的行为和返回值</li><li>最后调用 <code>GetTokenFromAWSSecret</code> 方法，验证返回值是否符合预期</li></ul><p>使用 GoMock 写的测试代码跟 Testify Mock 类似，但是我们不再需要手动的编写 Mock 对象，而是通过 <code>mockgen</code> 工具生成 Mock 对象，这样就可以大大提高测试代码的编写效率。</p><h2 id="引用包-Mock"><a href="#引用包-Mock" class="headerlink" title="引用包 Mock"></a>引用包 Mock</h2><p>在单元测试中，我们经常会遇到被测试代码直接调用引用包方法的情况，举个例子，比如我们经常使用 Go 内置的 <code>os</code> 包进行文件读写，如果真实地去调用 <code>os</code> 包的方法，那么我们就需要在测试中构造测试文件，然后再对测试文件进行清理，这样会增加测试代码的复杂度。我们更希望可以直接 Mock <code>os</code> 包中的方法，这样就可以避免对文件系统的依赖，同时可以更加灵活地控制返回值。</p><p>下面我们就来介绍一下如何 Mock 引用包的方法，首先我们来看下被测试代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;os&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">GetEnvVariable</span><span class="params">()</span></span> <span class="type">string</span> &#123;</span><br><span class="line">    value := os.Getenv(<span class="string">&quot;MY_ENV_VAR&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> value == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;default&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> value</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>被测试代码中直接使用 <code>os</code> 包的 <code>Getenv</code> 方法获取环境变量，如果我们要 Mock <code>os</code> 包中的 <code>Getenv</code> 方法，我们需要对这个方法进行一些改造，修改后的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;os&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> getenv = os.Getenv</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">GetEnvVariable</span><span class="params">()</span></span> <span class="type">string</span> &#123;</span><br><span class="line">    value := getenv(<span class="string">&quot;MY_ENV_VAR&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> value == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;default&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> value</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们将 <code>os.Getenv</code> 方法赋值给了一个变量 <code>getenv</code>，这样我们就可以在测试代码中修改 <code>getenv</code> 的值，从而实现 Mock <code>os</code> 包中的 <code>Getenv</code> 方法。下面我们来看下测试代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;github.com/stretchr/testify/assert&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestGetEnvVariable</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line">    oldGetenv := getenv</span><br><span class="line">    <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; getenv = oldGetenv &#125;()</span><br><span class="line"></span><br><span class="line">    getenv = <span class="function"><span class="keyword">func</span><span class="params">(key <span class="type">string</span>)</span></span> <span class="type">string</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> key == <span class="string">&quot;MY_ENV_VAR&quot;</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;mock_value&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    result := GetEnvVariable()</span><br><span class="line">    assert.Equal(t, <span class="string">&quot;mock_value&quot;</span>, result)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在测试函数中，我们首先保存了原始的 <code>getenv</code> 方法</li><li>然后通过给 <code>getenv</code> 赋值的方式对 <code>os.Getenv</code> 方法进行 Mock</li><li>最后调用 <code>GetEnvVariable</code> 方法，验证返回值是否符合预期</li></ul><p>其实对于 <code>os.getenv</code> 的测试，我们也可以通过 <code>os.setenv</code> 方法来设置期望值，但我们现在不是讨论如何让这个测试案例通过，而是如何 Mock 引用包的方法。这种方法无需引用任何第三方库，只需要对被测试代码做一点简单的修改，就可以实现 Mock 引用包的目的，如果你遇到了引用其他包的情况，可以通过这种方式来进行 Mock。</p><h2 id="数据库-Mock"><a href="#数据库-Mock" class="headerlink" title="数据库 Mock"></a>数据库 Mock</h2><p>对于数据库的 Mock， 如果使用之前提到的 Testify Mock 或 GoMock 往往力不从心，你会发现当你写了一大堆 Mock 逻辑后，代码不是编译不通过就是运行起来各种报错。这个时候我们需要更专业的数据库 Mock 工具，比如 <a href="https://github.com/DATA-DOG/go-sqlmock">go-sqlmock</a>。</p><p>go-sqlmock 是一个用于 Go 语言的 SQL 驱动模拟库，旨在测试中模拟真实数据库交互，支持事务、多连接、上下文和命名 SQL 参数，无需修改源代码，且不依赖任何第三方库，它能够模拟任何 SQL 驱动方法的行为，并具有严格的预期顺序匹配。</p><p>下面我们来看下如何使用 go-sqlmock 对数据库进行 Mock，首先我们来看下被测试代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;context&quot;</span></span><br><span class="line"><span class="string">&quot;errors&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/org/proj/server/db&quot;</span></span><br><span class="line"><span class="string">&quot;github.com/org/proj/server/model&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;gorm.io/gorm&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> (</span><br><span class="line">userRepo <span class="keyword">struct</span> &#123;</span><br><span class="line">Instance *db.Instance</span><br><span class="line">&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ur *userRepo)</span></span> FindUser(c context.Context, id <span class="type">uint64</span>) (*model.User, <span class="type">error</span>) &#123;</span><br><span class="line">user := <span class="built_in">new</span>(model.User)</span><br><span class="line"><span class="keyword">if</span> err := ur.Instance.Conn(c).</span><br><span class="line">Table(model.TabNameUser()).</span><br><span class="line">Where(<span class="string">&quot;id = ?&quot;</span>, id).</span><br><span class="line">First(user).Error; err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> errors.Is(err, gorm.ErrRecordNotFound) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">&quot;user not found&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> user, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>被测试代码中的 <code>FindUser</code> 方法是一个查询用户信息的方法</li><li>它依赖于项目中的 <code>db.Instance</code> 对象，这个对象是一个 PostgreSQL 数据库连接对象</li><li>该方法使用 <code>grom</code> 库以 ORM 方式查询数据库信息，简化了 SQL 查询的操作</li><li>转换成 SQL 语句大概是这样：<code>SELECT * FROM users WHERE id = ?</code></li></ul><p>下面我们来看下如何创建一个数据库 Mock 对象：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;database/sql&quot;</span></span><br><span class="line"><span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/DATA-DOG/go-sqlmock&quot;</span></span><br><span class="line"><span class="string">&quot;gorm.io/driver/postgres&quot;</span></span><br><span class="line"><span class="string">&quot;gorm.io/gorm&quot;</span></span><br><span class="line"><span class="string">&quot;gorm.io/gorm/logger&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DbMock</span><span class="params">(t *testing.T)</span></span> (*sql.DB, *gorm.DB, sqlmock.Sqlmock) &#123;</span><br><span class="line">sqldb, mock, _ := sqlmock.New()</span><br><span class="line">dialector := postgres.New(postgres.Config&#123;</span><br><span class="line">Conn:       sqldb,</span><br><span class="line">DriverName: <span class="string">&quot;postgres&quot;</span>,</span><br><span class="line">&#125;)</span><br><span class="line">gormdb, _ := gorm.Open(dialector, &amp;gorm.Config&#123;</span><br><span class="line">Logger: logger.Default.LogMode(logger.Info),</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">return</span> sqldb, gormdb, mock</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>DbMock</code> 方法使用 <code>sqlmock.New()</code> 方法创建 <code>sqldb</code> 和 <code>mock</code> 对象</li><li><code>sqldb</code> 对象主要用来模拟数据库，<code>mock</code> 对象用来设置预期的查询和结果</li><li>使用 <code>gorm.Open</code> 方法创建一个 PostgreSQL 数据库连接对象 <code>gormdb</code></li></ul><p>接下来我们来看下测试代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/org/proj/server/db&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/DATA-DOG/go-sqlmock&quot;</span></span><br><span class="line"><span class="string">&quot;github.com/stretchr/testify/assert&quot;</span></span><br><span class="line"><span class="string">&quot;gorm.io/gorm&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestFindUserSuccess</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line">sqldb, gormdb, mock := DbMock(t)</span><br><span class="line"><span class="keyword">defer</span> sqldb.Close()</span><br><span class="line"></span><br><span class="line">testName := <span class="string">&quot;user1&quot;</span></span><br><span class="line">testOrg := <span class="string">&quot;org1&quot;</span></span><br><span class="line">rows := sqlmock.NewRows([]<span class="type">string</span>&#123;<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;organization&quot;</span>&#125;).AddRow(<span class="number">1</span>, testName, testOrg)</span><br><span class="line">mock.ExpectQuery(<span class="string">`SELECT`</span>).WillReturnRows(rows)</span><br><span class="line"></span><br><span class="line">ctx := context.TODO()</span><br><span class="line">repo := &amp;userRepo&#123;</span><br><span class="line">Instance: &amp;db.Instance&#123;DB: gormdb&#125;,</span><br><span class="line">&#125;</span><br><span class="line">user, err := repo.FindUser(ctx, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">assert.NoError(t, err)</span><br><span class="line">assert.Equal(t, testName, user.Name)</span><br><span class="line">assert.Equal(t, testOrg, user.Organization)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在测试函数中，我们首先调用 <code>DbMock</code> 方法创建返回几个数据库 Mock 对象</li><li>然后使用 <code>sqlmock</code> 设置数据库查询的预期结果，我们使用用户表的 3 个字段设置了一条记录</li><li>再使用 <code>mock</code> 对象模拟查询语句的执行结果，这里的查询语句使用了正则表达式的匹配方式，也就是说，这里的 <code>SELECT</code> 可以匹配任何以 <code>SELECT</code> 开头的 SQL 语句，包括 <code>SELECT * FROM users WHERE id = ?</code></li><li>使用模拟的 <code>gormdb</code> 实例化 <code>userRepo</code> 对象</li><li>最后调用 <code>FindUser</code> 方法，验证返回值是否符合预期</li></ul><h2 id="Http-请求-Mock"><a href="#Http-请求-Mock" class="headerlink" title="Http 请求 Mock"></a>Http 请求 Mock</h2><p>在 Web 开发中，我们经常会遇到对外部 HTTP 服务的调用，比如调用第三方 API、调用微服务等。在单元测试中，我们不可能对这些外部服务进行真实的调用，这样会使得单元测试变得缓慢且不稳定，我们需要对这些 HTTP 请求进行 Mock，这样就可以模拟外部服务的行为，从而使得单元测试更加高效和可靠。</p><p><a href="https://pkg.go.dev/net/http/httptest">httptest</a> 是 Go 语言标准库中提供的一个 HTTP 服务测试工具，它可以模拟 HTTP 请求和响应，用于测试 HTTP 服务的功能和性能。httptest 包中的 <code>NewRequest</code> 和 <code>NewRecorder</code> 方法可以模拟 HTTP 请求和响应，我们可以使用这两个方法来模拟 HTTP 请求和响应，从而实现对 HTTP 服务的 Mock。</p><p>下面我们来看下如何使用 httptest 对 HTTP 请求进行 Mock，以常用的 Web 开发框架 <a href="https://github.com/gin-gonic/gin">Gin</a> 为例，下面是一个 Gin 的 Controller 例子，Controller 中有一个 <code>Login</code> 方法，用于处理用户登录请求：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;net/http&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/org/proj/server/dto&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/gin-gonic/gin&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> (</span><br><span class="line">controller <span class="keyword">struct</span> &#123;</span><br><span class="line">service FooService</span><br><span class="line">&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(re *controller)</span></span> Login(c *gin.Context) &#123;</span><br><span class="line">req := <span class="built_in">new</span>(dto.ReqLogin)</span><br><span class="line"><span class="keyword">if</span> err := c.BindJSON(req); err != <span class="literal">nil</span> &#123;</span><br><span class="line">c.Error(err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resp, err := re.service.Login(c, *req)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">c.Error(err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">c.JSON(http.StatusOK, resp)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>Login</code> 方法从请求 Body 中获取数据并转换为 <code>ReqLogin</code> 对象</li><li>然后调用 service 中的 <code>Login</code> 方法处理登录逻辑</li><li>最后将 server 方法的返回结果序列化为 JSON 返回给客户端</li></ul><p>下面我们来看下如何对 Controller 的 <code>Login</code> 方法进行单元测试，这里我们需要使用 GoMock 对 <code>service</code> 进行 Mock，同时使用 httptest 对 HTTP 请求进行 Mock：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;bytes&quot;</span></span><br><span class="line"><span class="string">&quot;encoding/json&quot;</span></span><br><span class="line"><span class="string">&quot;net/http&quot;</span></span><br><span class="line"><span class="string">&quot;net/http/httptest&quot;</span></span><br><span class="line"><span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/org/proj/server/dto&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/gin-gonic/gin&quot;</span></span><br><span class="line"><span class="string">&quot;github.com/golang/mock/gomock&quot;</span></span><br><span class="line"><span class="string">&quot;github.com/stretchr/testify/assert&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestResourceLoginSuccess</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line">ctrl := gomock.NewController(t)</span><br><span class="line"><span class="keyword">defer</span> ctrl.Finish()</span><br><span class="line"></span><br><span class="line">jsonBody := []<span class="type">byte</span>(<span class="string">`&#123; &quot;account&quot;: &quot;foo&quot;, &quot;password&quot;: &quot;baz&quot; &#125;`</span>)</span><br><span class="line">req := httptest.NewRequest(<span class="string">&quot;POST&quot;</span>, <span class="string">&quot;/login&quot;</span>, bytes.NewBuffer(jsonBody))</span><br><span class="line">req.Header.Set(<span class="string">&quot;Content-Type&quot;</span>, <span class="string">&quot;application/json&quot;</span>)</span><br><span class="line"></span><br><span class="line">w := httptest.NewRecorder()</span><br><span class="line">ctx, _ := gin.CreateTestContext(w)</span><br><span class="line">ctx.Request = req</span><br><span class="line"></span><br><span class="line">mockService := NewMockOAuthService(ctrl)</span><br><span class="line">expectedResp := &amp;dto.RespLogin&#123;</span><br><span class="line">Account:      <span class="string">&quot;test-account&quot;</span>,</span><br><span class="line">Organization: <span class="string">&quot;test-org&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">mockService.EXPECT().Login(ctx, gomock.Any()).Return(expectedResp, <span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line">cd := &amp;controller&#123;</span><br><span class="line">service: mockService,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cd.Login(ctx)</span><br><span class="line"></span><br><span class="line">assert.Equal(t, http.StatusOK, ctx.Writer.Status())</span><br><span class="line">assert.Nil(t, ctx.Errors)</span><br><span class="line"></span><br><span class="line">resp := &amp;dto.RespLogin&#123;&#125;</span><br><span class="line">err := json.Unmarshal(w.Body.Bytes(), resp)</span><br><span class="line">assert.Nil(t, err)</span><br><span class="line">assert.Equal(t, expectedResp, resp)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>首先使用 <code>httptest.NewRequest</code> 方法创建一个 URL 为 <code>/login</code> 的 POST 请求，请求 Body 为一个我们设置好的 JSON 数据</li><li>然后使用 <code>httptest.NewRecorder</code> 方法创建一个 Response 对象，用于保存请求的返回结果，使用 <code>gin.CreateTestContext</code> 方法创建一个测试上下文对象，将 Request 对象赋值给上下文对象</li><li>接着使用 GoMock 对 <code>service</code> 的 <code>Login</code> 方法进行 Mock，设置预期的返回值</li><li>调用 Controller 的 <code>Login</code> 方法，传入上下文对象</li><li>在验证阶段，先验证返回状态码和 <code>ctx.Errors</code> 是否正确</li><li>最后验证被测方法中 <code>c.JSON(http.StatusOK, resp)</code> 返回的 JSON 数据是否符合预期，这个 JSON 在原方法中被添加到了 <code>ctx.Writer</code> 中，我们可以通过 <code>w.Body.Bytes()</code> 获取到这个 JSON 数据</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天我们介绍了在 Golang 中对不同测试对象进行 Mock 的 5 种方法，包括常用的测试工具库 Testify Mock 和 GoMock，以及如何 Mock 引用包、数据库和 HTTP 请求。通过这些 Mock 技术，我们能够灵活替换外部依赖，定制其行为和返回值，确保测试环境的可控性。掌握了这些 Mock 技术，以后无论是模拟任何数据或方法，都能让你的单元测试更加高效和可靠。</p><p>关注我，一起学习最新的开发编程新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍在 Golang 中对不同测试对象进行 Mock 的 5 种方法</summary>
    
    
    
    <category term="code" scheme="https://zhaozhiming.github.io/categories/code/"/>
    
    
    <category term="golang" scheme="https://zhaozhiming.github.io/tags/golang/"/>
    
    <category term="mock" scheme="https://zhaozhiming.github.io/tags/mock/"/>
    
    <category term="unit-test" scheme="https://zhaozhiming.github.io/tags/unit-test/"/>
    
    <category term="sql-mock" scheme="https://zhaozhiming.github.io/tags/sql-mock/"/>
    
    <category term="httpmock" scheme="https://zhaozhiming.github.io/tags/httpmock/"/>
    
  </entry>
  
  <entry>
    <title>部署 AWS ECS 超详细攻略</title>
    <link href="https://zhaozhiming.github.io/2024/10/04/use-github-action-to-deploy-aws-ecs/"/>
    <id>https://zhaozhiming.github.io/2024/10/04/use-github-action-to-deploy-aws-ecs/</id>
    <published>2024-10-04T14:10:05.000Z</published>
    <updated>2024-10-14T13:16:59.047Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/10/github-deploy-aws-ecs.jpg" class="" width="400" height="300"><p>随着云计算和容器化的广泛普及，越来越多的团队选择使用 <a href="https://aws.amazon.com/">AWS</a> ECS（Elastic Container Service）来运行他们的应用服务。同时，通过 GitHub Action 自动化 CI&#x2F;CD 流程可以极大地提高开发效率。本文将详细介绍如何结合 GitHub Action 和 AWS ECS，将代码从仓库无缝部署到生产环境中。</p><span id="more"></span><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><img src="/images/post/2024/10/github-action-deploy-ecs-flow.png" class="" width="1000" height="600"><p>在使用 GitHub Action 部署 AWS ECS 的过程中，整体流程可以分为以下几个步骤：</p><ul><li>创建 ECR 资源：首先需要在 AWS ECR（Elastic Container Registry）中创建镜像仓库，用于存储应用程序的 Docker 镜像。通过 ECR，我们可以轻松地管理镜像的版本和更新，确保每次部署都能拉取到最新的稳定镜像。</li><li>配置 IAM 角色和权限：然后是配置好必要的 IAM 角色和权限，以便在整个流程中授权不同的服务执行所需的操作。这些角色包括 ECS 任务角色、执行角色以及 GitHub Action 的执行角色。</li><li>创建 ECS 资源：接着需要在 AWS 中创建 ECS 资源，包括 ECS 集群、服务和任务定义。这些资源用于运行容器化的应用程序，确保应用程序能够在容器中稳定运行。</li><li>创建 GitHub 工作流：在代码仓库中编写 GitHub Action 工作流，用于自动化构建和部署流程，通过 YAML 文件定义工作流步骤。</li></ul><h2 id="创建-ECR-资源"><a href="#创建-ECR-资源" class="headerlink" title="创建 ECR 资源"></a>创建 ECR 资源</h2><p>在部署 ECS 容器应用时，我们需要一个地方存放容器镜像，AWS ECR 正是用于存放 Docker 镜像的服务。AWS ECR 是一个完全托管的 Docker 容器注册表，集成了 AWS 的身份验证与访问控制，确保镜像的安全性。</p><p>ECR 提供了镜像仓库的功能，可以将应用的 Docker 镜像推送到 ECR 中。在部署 ECS 服务时，ECR 充当镜像存储库，ECS 会从中拉取镜像来启动任务。通过 ECR，我们可以轻松地管理镜像的版本和更新，确保每次部署都能拉取到最新的稳定镜像。同时，ECR 支持生命周期策略，可以自动清理旧的和不再使用的镜像，以节省存储成本。</p><p>在 AWS 管理控制台上创建镜像仓库的步骤如下：</p><ul><li>登录到 AWS 管理控制台，然后导航到 Amazon ECR 服务页面。</li><li>在左侧导航栏中，选择 <code>Private registry</code> 或 <code>Public registry</code>，然后点击 <code>Repositories</code>。</li><li>点击右上角的 <code>Create repository</code> 按钮，如截图中所示。</li></ul><img src="/images/post/2024/10/aws-ecr-create-repository.png" class="" width="1000" height="600"><h2 id="配置-IAM-角色和权限"><a href="#配置-IAM-角色和权限" class="headerlink" title="配置 IAM 角色和权限"></a>配置 IAM 角色和权限</h2><p>在部署 AWS ECS 的过程中，有 3 种 IAM（Identity and Access Management） 角色需要配置：</p><ul><li>ECS 任务角色：用于授予 ECS 内部的任务权限，一般用来访问其他 AWS 服务，此角色应赋予容器内的应用程序所需的最小权限。</li><li>ECS 执行角色：用于授予 ESC 执行的任务权限，一般是从 ECR 拉取镜像和写入 CloudWatch 日志，此角色同样应赋予最小权限。</li><li>GitHub Action 执行角色：用于在 GitHub Action 中执行 AWS 操作，一般是推送镜像到 ECR 和更新 ECS 服务。</li></ul><h3 id="ECS-任务角色"><a href="#ECS-任务角色" class="headerlink" title="ECS 任务角色"></a>ECS 任务角色</h3><p>ECS 任务角色允许运行在 ECS 中的任务访问其他 AWS 服务。例如，如果任务中的容器需要访问 S3 存储，这时就需要配置合适的任务角色来访问 S3。任务角色的主要作用是授予容器内的应用程序权限，以便它们能够与其他 AWS 服务进行交互。为了提升安全性，任务角色应在满足应用程序需求的同时赋予最小权限。</p><p>以下是一个任务角色的最小权限策略示例，该角色允许操作 Lambda 函数和 Secrets Manager：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lambda:InvokeFunction&quot;</span><span class="punctuation">,</span> <span class="string">&quot;lambda:GetFunctionConfiguration&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;secretsmanager:GetSecretValue&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;secretsmanager:DescribeSecret&quot;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="ECS-执行角色"><a href="#ECS-执行角色" class="headerlink" title="ECS 执行角色"></a>ECS 执行角色</h3><p>ECS 执行角色是 ECS 用来执行任务时的角色，主要用于拉取镜像（例如从 ECR 拉取镜像）以及写入 CloudWatch 日志。执行角色确保 ECS 能够顺利执行任务的相关操作，正确配置执行角色可以确保任务运行过程中能够顺利访问必要的资源，特别是在自动化部署和扩展过程中起到关键作用。</p><p>为了配置执行角色的最小权限，可以使用 AWS 的预定义策略 <code>AmazonECSTaskExecutionRolePolicy</code>（权限策略如下所示），这样可以确保角色具备最低限度的权限来完成必要的任务：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;ecr:GetAuthorizationToken&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;ecr:BatchCheckLayerAvailability&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;ecr:GetDownloadUrlForLayer&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;ecr:BatchGetImage&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;logs:CreateLogStream&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;logs:PutLogEvents&quot;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="GitHub-Action-执行角色"><a href="#GitHub-Action-执行角色" class="headerlink" title="GitHub Action 执行角色"></a>GitHub Action 执行角色</h3><p>在一些 Github Action 部署文档中（包括 <a href="https://docs.github.com/en/actions/use-cases-and-examples/deploying/deploying-to-amazon-elastic-container-service">Github 官方的文档</a>），都会使用 AWS 的 IAM 用户来作为访问 AWS 的凭证（提供用户的 SECRET ACCESS KEY 和 SECRET ACCESS KEY），但<strong>这种方式并不是最佳实践</strong>，因为这样会暴露用户的凭证，更好的方式是创建一个角色来授权 Github Action 来访问 AWS 的资源。该角色需要有足够的权限来推送镜像到 ECR，并更新 ECS 服务的任务定义或服务。</p><p>以下是一个 GitHub Action 执行角色权限策略示例，并对示例中的策略进行说明：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Sid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;VisualEditor0&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ecs:RegisterTaskDefinition&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;iam:PassRole&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;arn:aws:iam::&lt;account-id&gt;:role/&lt;task-role&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;arn:aws:iam::&lt;account-id&gt;:role/&lt;task-execution-role&gt;&quot;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;ecs:DescribeServices&quot;</span><span class="punctuation">,</span> <span class="string">&quot;ecs:UpdateService&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:ecs:&lt;region&gt;:&lt;account-id&gt;:service/&lt;ecs-cluster&gt;/&lt;esc-service&gt;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;ecr:GetAuthorizationToken&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;ecr:BatchCheckLayerAvailability&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;ecr:InitiateLayerUpload&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;ecr:UploadLayerPart&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;ecr:CompleteLayerUpload&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;ecr:PutImage&quot;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>ECS 任务定义注册权限：允许注册 ECS 任务定义，主要用于创建或更新任务定义，使其能够包含新的应用配置和容器信息。</li><li>IAM 角色传递权限：允许将 IAM 角色传递给 ECS 服务。这意味着该角色可以在执行任务时传递其他 IAM 角色的权限，这里将权限传递给了我们之前创建的 2 个 ECS 角色——任务角色和执行角色，以便这些任务可以访问必要的 AWS 资源。</li><li>ECS 服务描述和更新权限：允许查看指定 ECS 服务的状态和配置信息，允许更新 ECS 服务配置，例如更改使用的任务定义，或调整服务的副本数量。这里的资源填写的是 ECS 服务的 ARN，ECS 服务我们在下面会介绍如何创建。</li><li>ECR 镜像操作权限：允许获取 ECR 访问令牌、检查镜像层是否存在、上传镜像层以及推送镜像到 ECR，以供 ECS 使用。</li></ul><h2 id="创建-ECS-资源"><a href="#创建-ECS-资源" class="headerlink" title="创建 ECS 资源"></a>创建 ECS 资源</h2><p>AWS ECS 是 AWS 提供的一种容器管理服务，它使得运行、停止和管理 Docker 容器变得更加简单。在部署 AWS ECS 服务之前，我们需要先创建 ECS 集群和服务。</p><h3 id="创建集群和服务"><a href="#创建集群和服务" class="headerlink" title="创建集群和服务"></a>创建集群和服务</h3><ul><li>进入 ECS 控制台，点击 <code>Create Cluster</code>，选择适合的运行类型（如 Fargate 或 EC2），配置集群的相关参数，确认后点击 <code>Create</code> 按钮完成集群的创建。</li><li>在集群创建完成后，点击 <code>Create Service</code>，选择服务类型，配置服务的相关参数，确认后点击 <code>Create</code> 按钮完成创建。</li></ul><img src="/images/post/2024/10/aws-ecs-create-cluster.png" class="" width="1000" height="600"><img src="/images/post/2024/10/aws-ecs-create-service.png" class="" width="1000" height="600"><h3 id="创建任务定义"><a href="#创建任务定义" class="headerlink" title="创建任务定义"></a>创建任务定义</h3><p>任务定义是 ECS 中的核心概念之一，可以看作是应用的蓝图。它定义了容器的配置，包括镜像的来源、CPU 和内存的资源需求、端口配置等。每次启动任务时，都会根据任务定义运行相应的容器。任务定义的创建需要非常细致和准确，以确保部署的应用程序符合需求，例如可以设置环境变量、挂载卷和定义日志记录等。正确配置任务定义是确保应用程序在容器中稳定运行的基础。</p><p>以下是一个任务定义的 JSON 文件示例，并对示例中的代码进行说明：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;containerDefinitions&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;my-task-definition-name&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;image&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;organization&gt;/&lt;image-name&gt;:&lt;image-tag&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;cpu&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;portMappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;supplier-8080-tcp-port&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;containerPort&quot;</span><span class="punctuation">:</span> <span class="number">8080</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;hostPort&quot;</span><span class="punctuation">:</span> <span class="number">8080</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;protocol&quot;</span><span class="punctuation">:</span> <span class="string">&quot;tcp&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;appProtocol&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;essential&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;environment&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;environmentFiles&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mountPoints&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;volumesFrom&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;ulimits&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;logConfiguration&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;logDriver&quot;</span><span class="punctuation">:</span> <span class="string">&quot;awslogs&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;options&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;awslogs-group&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/ecs/&lt;ecs-service&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;awslogs-create-group&quot;</span><span class="punctuation">:</span> <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;awslogs-region&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;region&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;awslogs-stream-prefix&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ecs&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;secretOptions&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;systemControls&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;family&quot;</span><span class="punctuation">:</span> <span class="string">&quot;myserver-defs&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;taskRoleArn&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:iam::&lt;account-id&gt;:role/&lt;task-role-name&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;executionRoleArn&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:iam::&lt;account-id&gt;:role/&lt;task-execution-role-name&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;networkMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;awsvpc&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;volumes&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;placementConstraints&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;requiresCompatibilities&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;FARGATE&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;cpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;512&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;memory&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2048&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;runtimePlatform&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;cpuArchitecture&quot;</span><span class="punctuation">:</span> <span class="string">&quot;X86_64&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;operatingSystemFamily&quot;</span><span class="punctuation">:</span> <span class="string">&quot;LINUX&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;tags&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>containerDefinitions 属性：定义了容器的详细配置，包括容器名称、镜像来源、CPU 和内存的分配、端口映射等。在此示例中，容器名称为 <code>my-task-definition-name</code>，镜像是我们刚才创建的 ECR 镜像仓库中的镜像名称，并将容器的 8080 端口映射到主机的 8080 端口。</li><li>logConfiguration 属性：指定了日志配置，使用 AWS CloudWatch Logs 记录容器的日志信息。日志组名称为 <code>/ecs/&lt;ecs-service&gt;</code>，并且在必要时会自动创建日志组。</li><li>taskRoleArn 和 executionRoleArn 属性：分别指定了 ECS 任务角色和执行角色的 ARN，也就是我们刚才创建的 2 个 ECS 角色，它们用于控制容器内的应用访问其他 AWS 服务的权限，以及任务执行过程中所需的权限。</li><li>networkMode 属性：指定网络模式为 <code>awsvpc</code>，表示容器将使用 AWS VPC 网络模式，这对于 Fargate 类型（无服务器计算引擎，用于简化容器部署）的任务是必需的。</li><li>requiresCompatibilities 属性：指定任务兼容性为 <code>FARGATE</code>，表示此任务将在 AWS Fargate 上运行。</li><li>cpu 和 memory 属性：定义了任务的 CPU 和内存配置。在此示例中，任务分配了 512 个 CPU 单位（0.5 个 vCPU）和 2048 MB 的内存。</li><li>runtimePlatform 属性：指定了运行平台的架构和操作系统类型，确保任务在合适的环境中运行。</li></ul><h2 id="创建-GitHub-工作流"><a href="#创建-GitHub-工作流" class="headerlink" title="创建 GitHub 工作流"></a>创建 GitHub 工作流</h2><p>GitHub Action 是 GitHub 提供的自动化工作流工具，可以用于构建、测试和部署代码。在我们的场景中，GitHub Action 可以被用来自动化构建 Docker 镜像并将其推送到 ECR，然后更新 ECS 服务。通过 GitHub Action，开发人员可以定义一套自动化的工作流，从代码提交到服务部署，全程无需人工干预。</p><p>GitHub Action 的工作流通过配置文件（通常是 <code>.github/workflows</code> 目录下的 YAML 文件）进行定义。</p><p>我们首先来看下构建和推送 Docker 镜像到 ECR 镜像仓库的工作流步骤示例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">build-push:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ECR</span> <span class="string">build</span> <span class="string">and</span> <span class="string">push</span></span><br><span class="line">  <span class="attr">needs:</span> <span class="string">test</span></span><br><span class="line">  <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">outputs:</span></span><br><span class="line">    <span class="attr">aws_ecr_image_tag:</span> <span class="string">$&#123;&#123;</span> <span class="string">steps.build-push-step.outputs.aws_ecr_image_tag</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">code</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@v4</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Configure</span> <span class="string">AWS</span> <span class="string">Credentials</span></span><br><span class="line">      <span class="attr">id:</span> <span class="string">aws-credentials</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">aws-actions/configure-aws-credentials@v4</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">role-to-assume:</span> <span class="string">$&#123;&#123;</span> <span class="string">vars.AWS_GITHUB_ACTION_ROLE</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">aws-region:</span> <span class="string">$&#123;&#123;</span> <span class="string">vars.AWS_REGION</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Log</span> <span class="string">in</span> <span class="string">to</span> <span class="string">Amazon</span> <span class="string">ECR</span></span><br><span class="line">      <span class="attr">id:</span> <span class="string">ecr-login</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">aws-actions/amazon-ecr-login@v2</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">mask-password:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">registry-type:</span> <span class="string">private</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Build</span> <span class="string">and</span> <span class="string">push</span></span><br><span class="line">      <span class="attr">id:</span> <span class="string">build-push-step</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="attr">AWS_ECR_REGISTRY:</span> <span class="string">$&#123;&#123;</span> <span class="string">steps.ecr-login.outputs.registry</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">AWS_ECR_REPOSITORY:</span> <span class="string">$&#123;&#123;</span> <span class="string">vars.AWS_ECR_REPOSITORY</span> <span class="string">&#125;&#125;</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        export AWS_ECR_IMAGE_TAG=$AWS_ECR_REGISTRY/$AWS_ECR_REPOSITORY:$(git rev-parse --short HEAD)</span></span><br><span class="line"><span class="string">        docker build -f ./server/build/Dockerfile -t $AWS_ECR_IMAGE_TAG ./server</span></span><br><span class="line"><span class="string">        docker push $AWS_ECR_IMAGE_TAG</span></span><br><span class="line"><span class="string">        echo &quot;aws_ecr_image_tag=$AWS_ECR_IMAGE_TAG&quot; &gt;&gt; &quot;$GITHUB_OUTPUT&quot;</span></span><br></pre></td></tr></table></figure><ul><li>从 GitHub 仓库中检出最新代码，以便进行构建。</li><li>配置 AWS 凭据，通过 <code>role-to-assume</code> 设定需要承担的 AWS 角色，以及目标区域。<code>role-to-assume</code> 填写的是我们之前创建的 Github Action 执行角色的 ARN，可以将其配置到 Github 仓库的 Variables 中，确保在工作流中可以使用这些信息。</li><li>登录到 AWS ECR，并确保凭据安全，使用的是上一步骤中配置的 AWS 凭据。</li><li>设置环境变量，如 ECR 注册表和镜像仓库名，环境变量的值同样可以配置到 Github 仓库的 Variables 中。然后生成包含 Git 提交短哈希的镜像标签，构建镜像并推送到 ECR，然后输出镜像标签供后续使用。</li></ul><p>下面是 Github 仓库的 Secret 和  Variables 配置页面：</p><img src="/images/post/2024/10/github-setting-secret.png" class="" width="1000" height="600"><p>我们再来看 ECS 部署的工作流步骤示例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ecs-deploy:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ECS</span> <span class="string">deployment</span></span><br><span class="line">  <span class="attr">needs:</span> <span class="string">build-push</span></span><br><span class="line">  <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">  <span class="attr">env:</span></span><br><span class="line">    <span class="attr">AWS_ECR_IMAGE_TAG:</span> <span class="string">$&#123;&#123;</span> <span class="string">needs.build-push.outputs.aws_ecr_image_tag</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@v4</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Configure</span> <span class="string">AWS</span> <span class="string">Credentials</span></span><br><span class="line">      <span class="attr">id:</span> <span class="string">aws-credentials</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">aws-actions/configure-aws-credentials@v4</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">role-to-assume:</span> <span class="string">$&#123;&#123;</span> <span class="string">vars.AWS_GITHUB_ACTION_ROLE</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">aws-region:</span> <span class="string">$&#123;&#123;</span> <span class="string">vars.AWS_REGION</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Render</span> <span class="string">Amazon</span> <span class="string">ECS</span> <span class="string">task</span> <span class="string">definition</span></span><br><span class="line">      <span class="attr">id:</span> <span class="string">render-task-def</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">aws-actions/amazon-ecs-render-task-definition@v1</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">task-definition:</span> <span class="string">your/task/definition/file.json</span></span><br><span class="line">        <span class="attr">container-name:</span> <span class="string">$&#123;&#123;</span> <span class="string">vars.AWS_ECS_CONTAINER</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">$&#123;&#123;</span> <span class="string">env.AWS_ECR_IMAGE_TAG</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">environment-variables:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          FOO=$&#123;&#123; vars.FOO &#125;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span> <span class="string">to</span> <span class="string">Amazon</span> <span class="string">ECS</span> <span class="string">task</span> <span class="string">definition</span></span><br><span class="line">      <span class="attr">id:</span> <span class="string">deploy-task-def</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">aws-actions/amazon-ecs-deploy-task-definition@v1</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">task-definition:</span> <span class="string">$&#123;&#123;</span> <span class="string">steps.render-task-def.outputs.task-definition</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">service:</span> <span class="string">$&#123;&#123;</span> <span class="string">vars.AWS_ECS_SERVICE</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">cluster:</span> <span class="string">$&#123;&#123;</span> <span class="string">vars.AWS_ECS_CLUSTER</span> <span class="string">&#125;&#125;</span></span><br></pre></td></tr></table></figure><ul><li>在 <code>ecs-deploy</code> 步骤中，将构建好的 Docker 镜像部署到 Amazon ECS 服务，依赖于前一步的 <code>build-push</code> 步骤，使用构建步骤的输出值 <code>aws_ecr_image_tag</code> 作为镜像标签。</li><li>检出代码，以便在部署过程中使用最新的代码版本。</li><li>配置 AWS 凭据，通过 <code>role-to-assume</code> 来承担之前配置好的 GitHub Action 执行角色，并指定 AWS 区域。</li><li>渲染任务定义文件，更新其中的容器镜像、环境变量等配置。这里的任务定义文件为 <code>your/task/definition/file.json</code>， 这个 JSON 文件相当于我们刚才介绍的任务定义示例文件，并替换容器镜像和其他必要的环境变量。</li><li>将渲染后的任务定义部署到指定的 ECS 服务和集群中，确保最新的代码和配置能够进行正确部署。这里用到了我们之前创建的 ECS Cluster 和 Service，可以将它们的名字配置到 Github 仓库的 Variables 中。</li></ul><p>通过这种方式，代码一旦合并到主分支，就可以触发 GitHub Action 完成从构建到部署的一系列操作，极大地简化了手动部署的复杂度，同时保证了每次部署的一致性和可重复性。工作流的配置可以根据团队的需求进行扩展，例如加入单元测试、静态代码分析、通知等，以确保代码质量和部署的可靠性。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了部署 AWS ECS 所需的资源和权限，以及通过自动化工作流实现无缝部署。整体流程包括创建 ECR 资源、配置 IAM 角色和权限、创建 ECS 资源，以及编写 GitHub Action 工作流，实现从代码检出到容器部署的完整自动化。通过这种方式，开发团队可以提高部署效率，确保每次部署的一致性和可重复性，同时增强安全性和代码质量控制。</p><p>关注我，一起学习最新的开发编程新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">详尽介绍使用 AWS 角色在 GitHub Action 中部署 AWS ECS 服务</summary>
    
    
    
    <category term="code" scheme="https://zhaozhiming.github.io/categories/code/"/>
    
    
    <category term="github" scheme="https://zhaozhiming.github.io/tags/github/"/>
    
    <category term="aws" scheme="https://zhaozhiming.github.io/tags/aws/"/>
    
    <category term="role" scheme="https://zhaozhiming.github.io/tags/role/"/>
    
    <category term="ecs" scheme="https://zhaozhiming.github.io/tags/ecs/"/>
    
    <category term="github-action" scheme="https://zhaozhiming.github.io/tags/github-action/"/>
    
  </entry>
  
  <entry>
    <title>使用 AWS 角色隔离资源和权限</title>
    <link href="https://zhaozhiming.github.io/2024/10/03/use-aws-role-to-isolate-resources-and-privilege/"/>
    <id>https://zhaozhiming.github.io/2024/10/03/use-aws-role-to-isolate-resources-and-privilege/</id>
    <published>2024-10-03T08:56:11.000Z</published>
    <updated>2024-10-06T10:30:25.508Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/10/aws-role.jpg" class="" width="400" height="300"><p>AWS 是目前全球最大的云服务提供商，提供了丰富的云服务，开发 Web 应用时经常会用到 AWS 的各种服务。在开发这种应用时，有时候开发人员为了图方便省事，只创建一个 AWS 角色来管理应用所涉及的所有 AWS 资源，这样不仅不利于 AWS 资源的管理，而且还会因为权限不当导致系统的安全性受到威胁。本文将介绍 AWS 角色中权限相关的内容，以及如何使用 AWS 角色策略对资源和权限进行隔离。</p><span id="more"></span><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>假设一家初创公司在开发他们的电商平台时，决定使用 AWS 提供的几种服务，包括 Amazon S3 存储产品图片，Amazon RDS 存储用户数据，以及 Amazon Lambda 处理后台任务。为了快速开发和部署，开发人员创建了一个单一的 AWS 角色，这个角色被赋予了完全访问 S3、RDS 和 Lambda 的权限，以便可以无障碍地调用这些服务。这意味着所有用户共享相同的权限，没有进行细粒度的权限控制，导致潜在的安全隐患。</p><img src="/images/post/2024/10/source-aws-design.png" class="" width="1000" height="600"><p>如果有坏人进入了这个应用程序，他们将能够访问所有的 AWS 资源，包括 S3、RDS 和 Lambda。这种情况下，坏人可以下载 S3 中所有的图片，删除 RDS 中的用户数据，或者通过 Lambda 执行恶意代码，造成严重的安全隐患。</p><img src="/images/post/2024/10/source-aws-design-danger.png" class="" width="1000" height="600"><p>在这种情况下，我们可以使用 AWS 角色策略对资源和权限进行隔离。用户通过应用程序使用各自的 AWS 角色来访问 AWS 资源。每个角色被分配了相对应最低限度的权限，每个角色只能访问其对应的资源，而不能访问其他角色的资源。通过这种方式，每个用户只能访问与其相关的资源，避免了权限的滥用和安全隐患。</p><img src="/images/post/2024/10/complete-aws-design.png" class="" width="1000" height="600"><h2 id="AWS-角色"><a href="#AWS-角色" class="headerlink" title="AWS 角色"></a>AWS 角色</h2><p>在 AWS 的 IAM（Identity and Access Management）服务中，角色是一个 AWS 身份，它不是与特定用户或组关联的，而是与 AWS 服务关联的。角色定义了一个实体，它可以代表一个用户、一个应用程序或者一个服务，以便可以访问 AWS 资源。角色可以通过权限策略来控制对资源的访问权限，以及信任关系来控制角色的使用者。</p><h3 id="权限属性"><a href="#权限属性" class="headerlink" title="权限属性"></a>权限属性</h3><p>在日常使用中，<em>权限策略（Permission Policy）</em>和<em>信任关系（Trust Relationship）</em>是 AWS 角色权限最常用和最核心的两个属性。</p><p><strong>权限策略</strong></p><ul><li>定义：权限策略是一个以 JSON 格式编写的文档，定义了角色被授予的具体权限。它描述了角色可以对哪些 AWS 资源执行哪些操作。</li><li>作用：控制角色在 AWS 中<em>可以做什么</em>。例如，允许角色访问特定的 S3 存储桶、启动 EC2 实例、或调用特定的 Lambda 函数等。</li></ul><p><strong>信任关系</strong></p><ul><li>定义：信任关系也是一个以 JSON 格式编写的文档，定义了哪些实体（如用户、角色、服务）被允许扮演（Assume） 该角色。它指定了可以获得该角色临时凭证的主体。</li><li>作用：控制<em>谁可以扮演这个角色</em>。例如，允许特定的 IAM 用户、其他角色或者 AWS 服务（如 EC2、Lambda）来扮演该角色。</li></ul><p>这两者共同构成了 IAM 角色的安全模型，权限策略确保角色只能执行被授予的操作，信任关系确保只有被信任的主体才能扮演该角色。</p><h3 id="操作资源"><a href="#操作资源" class="headerlink" title="操作资源"></a>操作资源</h3><p>AWS 角色可以被授予访问任何 AWS 服务和资源的权限，具体取决于添加到该角色的权限策略，可操作的资源包括但不限于：</p><ul><li>AWS Lambda：创建、更新、调用函数；配置触发器和日志等。</li><li>Amazon EventBridge Scheduler：可以按照预定的时间或间隔来调度任务或操作。</li><li>AWS KMS（密钥管理服务）：创建和管理加密密钥；控制密钥的使用权限等</li></ul><p>还有很多其他 AWS 服务和资源没有列出来，这里只列出比较常用的几种 AWS 资源，我们在下面的示例也会着重介绍如何通过角色策略来控制这些资源的访问权限。</p><h2 id="添加信任关系"><a href="#添加信任关系" class="headerlink" title="添加信任关系"></a>添加信任关系</h2><p>在 AWS 角色中，信任关系是一个非常重要的属性，它定义了哪些实体被允许扮演该角色。信任关系是一个以 JSON 格式编写的文档，它指定了可以获得该角色临时凭证的主体。这些实体可以是 AWS 服务或者 IAM 用户 。</p><p>下面的信任关系中添加了 <code>event</code>、<code>lambda</code> 和 <code>scheduler</code> 3 个 AWS 服务，这意味着这些服务可以扮演该角色。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Principal&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;Service&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="string">&quot;events.amazonaws.com&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="string">&quot;scheduler.amazonaws.com&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="string">&quot;lambda.amazonaws.com&quot;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sts:AssumeRole&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>下面的信任关系中添加了 IAM 用户 <code>user1</code> 作为扮演该角色的主体，这意味着 <code>user1</code> 可以扮演该角色。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Principal&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;AWS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:iam::123456789012:user/user1@org1.com&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sts:AssumeRole&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>其中 <code>Statement</code> 是一个数组，这意味着你可以将多个实体添加到信任关系中，比如同时添加其他 AWS 服务和 IAM 用户。</p><h2 id="添加权限策略"><a href="#添加权限策略" class="headerlink" title="添加权限策略"></a>添加权限策略</h2><p>在 AWS 角色中添加权限策略是控制角色对资源的访问权限的关键，我们可以通过以下两种方式来添加权限策略：</p><img src="/images/post/2024/10/aws-role-permission.png" class="" width="1000" height="600"><h3 id="添加预定义策略"><a href="#添加预定义策略" class="headerlink" title="添加预定义策略"></a>添加预定义策略</h3><p>AWS 提供了一些预定义的策略，可以直接添加到角色中，这些策略包括了一些常用的权限，比如只读、只写、完全访问等。以 Lambda 服务为例，你可以在 AWS 控制台中选择<code>AWSLambda_FullAccess</code>、<code>AWSLambda_ReadOnlyAccess</code>、<code>AWSLambdaBasicExecutionRole</code> 等预定义策略，然后将其添加到角色中。</p><p>添加预定义策略的好处是你不必了解所有的权限细节，比如你要添加 Lambda 函数的权限，你可以直接添加 <code>AWSLambda_FullAccess</code> 策略，这样就可以访问 Lambda 服务的所有权限。虽然这样做十分方便，，但是每个预定义策略的权限范围可能比你所需的权限更广泛或者更窄，甚至有时候为了添加几个不同资源的权限而添加了很多个预定义策略。</p><h3 id="创建自定义策略"><a href="#创建自定义策略" class="headerlink" title="创建自定义策略"></a>创建自定义策略</h3><p>自定义策略就比预定义策略灵活的多，你可以根据自己的需求将不同 AWS 服务的权限添加到一个策略中，比如下面这个例子：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lambda:InvokeFunction&quot;</span><span class="punctuation">,</span> <span class="string">&quot;lambda:GetFunction&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;secretsmanager:GetSecretValue&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;secretsmanager:DescribeSecret&quot;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>这个自定义策略允许角色执行 Lambda 函数的 <code>InvokeFunction</code> 和 <code>GetFunction</code> 操作，同时允许角色获取 Secrets Manager 中的 <code>GetSecretValue</code> 和 <code>DescribeSecret</code> 操作。不必添加诸如 AWS Lambda 和 Secrets Manager 的预定义策略，而且权限范围更加精准，更加符合你的实际需求。</p><h2 id="权限隔离"><a href="#权限隔离" class="headerlink" title="权限隔离"></a>权限隔离</h2><p>下面我们再来介绍一下如何为 AWS 角色或者资源进行权限隔离，以保证资源和权限的安全性，下面以 Lambda 函数和 Secrets Manager 这两种资源为例进行介绍。</p><h3 id="Lambda-函数"><a href="#Lambda-函数" class="headerlink" title="Lambda 函数"></a>Lambda 函数</h3><p>在刚才的示例中，我们为角色添加了访问 Lambda 函数的权限策略，这意味着角色可以调用 Lambda 函数，但如果我们想控制角色<strong>只能调用特定的 Lambda 函数</strong>，那么我们就需要在权限策略中添加特定的 Lambda 函数的 ARN（Amazon 资源名称）。下面是一个示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;lambda:InvokeFunction&quot;</span><span class="punctuation">,</span> <span class="string">&quot;lambda:GetFunction&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:lambda:us-west-1:123456789012:function:role1-*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>我们可以使用角色名称作为前缀来命名 Lambda 函数，然后在权限策略中添加限制，仅能调用以该前缀命名的 Lambda 函数，通过这种方式，可以保证该角色仅能调用属于自己的 Lambda 函数。假设我们的 AWS 角色名称为 <code>role1</code>，那么我们可以将 Lambda 函数的名称以 <code>role1-</code> 作为前缀，然后在角色的权限策略中添加只能调用以 <code>role1-</code> 为前缀的 Lambda 函数，这样就不会担心该角色调用其他 Lambda 函数。</p><p>还有另外一种方法，我们也可以在 Lambda 函数权限中设置可访问的角色，这样确保该 Lambda 函数只能被某个角色调用。在 AWS 上设置 Lambda 函数权限的具体路径为：<code>Lambda -&gt; Functions -&gt; Your Lambda Function -&gt; Configuration -&gt; Permissions -&gt; Add Permission</code>。</p><img src="/images/post/2024/10/aws-lambda-func-permission.png" class="" width="1000" height="600"><p>这两种方式都可以起到相同的作用，选择哪种方式取决于你的具体需求。当然你也可以同时使用这两种方式，这样可以让权限更加精确，但请确保这两者之间的权限不冲突。AWS 将同时评估两者，只有在两者都允许的情况下，访问才会被授予。</p><h3 id="Secret-Manager"><a href="#Secret-Manager" class="headerlink" title="Secret Manager"></a>Secret Manager</h3><p>为 Secrets Manager 添加权限策略也是一样的，我们在角色的权限策略中对资源进行限制，让其只能访问特定的 Secret，而不能访问其他 Secret。下面是一个示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;secretsmanager:GetSecretValue&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;secretsmanager:DescribeSecret&quot;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:secretsmanager:us-west-1:123456789012:secret:role1-*&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>我们将 Secret 的名字同样以角色名称为前缀，然后在权限策略中添加权限，只能访问以该前缀命名的 Secret，这样就可以确保该角色只能访问属于自己的 Secret。</p><p>同样地，我们也可以在 Secret 的权限中设置中可访问的角色，这样确保该 Secret 只能被某些角色访问。相比 Lambda 函数，Secret 的权限策略更加灵活，Secret 可以支持设置多个角色对其进行访问，而 Lambda 函数只能设置一个角色。下面是 Secret 的权限设置示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Principal&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;AWS&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;arn:aws:iam::123456789012:role/role1&quot;</span><span class="punctuation">]</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;secretsmanager:GetSecretValue&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;secretsmanager:DescribeSecret&quot;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:secretsmanager:us-west-1:123456789012:secret:role1-SEC-wBO2wW&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Deny&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Principal&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;secretsmanager:GetSecretValue&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:secretsmanager:us-west-1:123456789012:secret:role1-SEC-wBO2wW&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Condition&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;StringNotEquals&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;aws:PrincipalArn&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;arn:aws:iam::123456789012:role/role1&quot;</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>在这个 Secret 的权限设置中，我们允许 <code>role1</code> 角色访问该 Secret，同时拒绝其他角色访问该 Secret。这样就保证了只有 <code>role1</code> 角色可以访问该 Secret，其他角色无法访问。</p><h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><p>如果手动来创建角色并设置权限策略，可能会比较繁琐，特别是当需要创建多个角色和资源的时候。为了提高效率，我们可以使用 AWS SDK 来快速执行这些操作。</p><p>下面我们通过 Go 语言来演示如何使用 <a href="https://aws.amazon.com/sdk-for-go/">AWS SDK</a> 来创建角色、Lambda 函数和 Secret，并设置权限策略。</p><h3 id="创建角色并设置权限策略"><a href="#创建角色并设置权限策略" class="headerlink" title="创建角色并设置权限策略"></a>创建角色并设置权限策略</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> foo</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;context&quot;</span></span><br><span class="line">    <span class="string">&quot;errors&quot;</span></span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;github.com/aws/aws-sdk-go/aws&quot;</span></span><br><span class="line">    <span class="string">&quot;github.com/aws/aws-sdk-go/service/iam&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">CreateAwsRole</span><span class="params">(c context.Context, amazonConfig aws.Config)</span></span> (*iam.CreateRoleOutput, <span class="type">error</span>) &#123;</span><br><span class="line">roleName := <span class="string">&quot;role1&quot;</span></span><br><span class="line">assumeRolePolicyDocument := <span class="string">`&#123;</span></span><br><span class="line"><span class="string">&quot;Version&quot;: &quot;2012-10-17&quot;,</span></span><br><span class="line"><span class="string">&quot;Statement&quot;: [</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">&quot;Effect&quot;: &quot;Allow&quot;,</span></span><br><span class="line"><span class="string">&quot;Principal&quot;: &#123;</span></span><br><span class="line"><span class="string">&quot;Service&quot;: [</span></span><br><span class="line"><span class="string">&quot;lambda.amazonaws.com&quot;,</span></span><br><span class="line"><span class="string">]</span></span><br><span class="line"><span class="string">&#125;,</span></span><br><span class="line"><span class="string">&quot;Action&quot;: &quot;sts:AssumeRole&quot;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">]</span></span><br><span class="line"><span class="string">&#125;`</span></span><br><span class="line">createRoleInput := &amp;iam.CreateRoleInput&#123;</span><br><span class="line">RoleName:                 aws.String(roleName),</span><br><span class="line">AssumeRolePolicyDocument: aws.String(assumeRolePolicyDocument),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">iamClient := iam.NewFromConfig(amazonConfig)</span><br><span class="line">role, err := iamClient.CreateRole(c, createRoleInput)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, errors.New(fmt.Sprintf(<span class="string">&quot;create aws role occurred error: %s&quot;</span>, err.Error()))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> role, err</span><br></pre></td></tr></table></figure><ul><li>在 <code>CreateRole</code> 的方法中，我们创建了一个名为 <code>role1</code> 的 AWS 角色，该角色的信任关系允许 Lambda 服务扮演该角色，这意味着该角色下的 Lambda 函数拥有该角色资源权限，比如该角色下的 Lambda 函数可以访问该角色下的 Secret。</li><li>创建 AWS 角色需要 2 个参数，一个是角色名称 <code>role1</code>，另一个是信任关系文档 <code>assumeRolePolicyDocument</code>。</li></ul><p>然后我们再为这个角色添加权限策略：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">PutRolePolicy</span><span class="params">(c context.Context, amazonConfig aws.Config, roleName <span class="type">string</span>)</span></span> (<span class="type">error</span>) &#123;</span><br><span class="line">policyDocument := <span class="string">`&#123;</span></span><br><span class="line"><span class="string">&quot;Version&quot;: &quot;2012-10-17&quot;,</span></span><br><span class="line"><span class="string">&quot;Statement&quot;: [</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">&quot;Effect&quot;: &quot;Allow&quot;,</span></span><br><span class="line"><span class="string">&quot;Action&quot;: [</span></span><br><span class="line"><span class="string">&quot;lambda:InvokeFunction&quot;,</span></span><br><span class="line"><span class="string">&quot;lambda:GetFunction&quot;</span></span><br><span class="line"><span class="string">],</span></span><br><span class="line"><span class="string">&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-1:123456789012:function:role1-*&quot;</span></span><br><span class="line"><span class="string">&#125;,</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">&quot;Effect&quot;: &quot;Allow&quot;,</span></span><br><span class="line"><span class="string">&quot;Action&quot;: [</span></span><br><span class="line"><span class="string">&quot;secretsmanager:GetSecretValue&quot;,</span></span><br><span class="line"><span class="string">&quot;secretsmanager:DescribeSecret&quot;</span></span><br><span class="line"><span class="string">],</span></span><br><span class="line"><span class="string">            &quot;Resource&quot;: &quot;arn:aws:secretsmanager:us-west-1:123456789012:secret:role1-*&quot;</span></span><br><span class="line"><span class="string">]</span></span><br><span class="line"><span class="string">&#125;`</span>,</span><br><span class="line">putRolePolicyInput := &amp;iam.PutRolePolicyInput&#123;</span><br><span class="line">PolicyName:     aws.String(fmt.Sprintf(<span class="string">&quot;%s-policy&quot;</span>, roleName)),</span><br><span class="line">PolicyDocument: aws.String(policyDocument),</span><br><span class="line">RoleName:       aws.String(roleName),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">iamClient := iam.NewFromConfig(amazonConfig)</span><br><span class="line">_, err = iamClient.PutRolePolicy(c, putRolePolicyInput)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> errors.New(fmt.Sprintf(<span class="string">&quot;put role policy for aws role occurred error: %s&quot;</span>, err.Error()))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在 <code>PutRolePolicy</code> 的方法中，我们为 <code>role1</code> 角色添加了权限策略</li><li>权限策略中允许该角色调用名称以 <code>role1-</code> 为前缀的 Lambda 函数</li><li>权限策略中允许该角色访问名称以 <code>role1-</code> 为前缀的 Secret</li><li>创建权限策略需要 3 个参数，一个是角色名称 <code>role1</code>，另一个是权限策略文档 <code>policyDocument</code>，最后一个是权限策略名称，这里我们使用角色名称加上 <code>-policy</code> 作为权限策略名称。</li></ul><h3 id="创建-Lambda-函数并设置角色权限"><a href="#创建-Lambda-函数并设置角色权限" class="headerlink" title="创建 Lambda 函数并设置角色权限"></a>创建 Lambda 函数并设置角色权限</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;context&quot;</span></span><br><span class="line">    <span class="string">&quot;errors&quot;</span></span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;strings&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;github.com/aws/aws-sdk-go/aws&quot;</span></span><br><span class="line">    <span class="string">&quot;github.com/aws/aws-sdk-go/service/lambda&quot;</span></span><br><span class="line">    lambTypes <span class="string">&quot;github.com/aws/aws-sdk-go/service/lambda/types&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> ReqFile <span class="keyword">struct</span> &#123;</span><br><span class="line">    _     <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line">    Name  <span class="type">string</span></span><br><span class="line">    Bytes []<span class="type">byte</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">CreateLambdaFunction</span><span class="params">(c context.Context, file *ReqFile, roleName <span class="type">string</span>, roleARN <span class="type">string</span>, amazonConfig aws.Config)</span></span> (*lambda.CreateFunctionOutput, <span class="type">error</span>) &#123;</span><br><span class="line">splits := strings.Split(file.Name, <span class="string">&quot;.&quot;</span>)</span><br><span class="line">fileName := splits[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">lambdaClient := lambda.NewFromConfig(amazonConfig)</span><br><span class="line">lambdaFun, err := lambdaClient.RegisterLambda(</span><br><span class="line">c,</span><br><span class="line">&amp;lambda.CreateFunctionInput&#123;</span><br><span class="line">Code: &amp;lambTypes.FunctionCode&#123;</span><br><span class="line">ZipFile: file.Bytes,</span><br><span class="line">&#125;,</span><br><span class="line">FunctionName: aws.String(fmt.Sprintf(<span class="string">&quot;%s-%s&quot;</span>, roleName, fileName)),</span><br><span class="line">Role:        aws.String(roleARN),</span><br><span class="line">Runtime:     lambTypes.RuntimeNodejs20x,</span><br><span class="line">Timeout:     aws.Int32(<span class="number">30</span>),</span><br><span class="line">Description: <span class="literal">nil</span>,</span><br><span class="line">Handler:     aws.String(fmt.Sprintf(<span class="string">&quot;%s.handler&quot;</span>, fileName)),</span><br><span class="line">PackageType: lambTypes.PackageTypeZip,</span><br><span class="line">Publish:     <span class="literal">false</span>,</span><br><span class="line">&#125;,</span><br><span class="line"><span class="function"><span class="keyword">func</span><span class="params">(opt *lambda.Options)</span></span> &#123;&#125;,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, errors.New(fmt.Sprintf(<span class="string">&quot;failed to create lambda function: %s, err: %s&quot;</span>, fileName, err.Error()))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> lambdaFun, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在 <code>CreateLambdaFunction</code> 的方法中，我们创建了一个 Lambda 函数</li><li>AWS SDK 的 <code>RegisterLambda</code> 方法需要多个参数，其中 <code>Role</code> 参数是角色的 ARN，就是我们刚才创建的那个角色</li><li>Lambda 函数需要一个 Zip 格式的文件，这个文件包含了 Lambda 函数的代码，代码中的主要执行函数是 <code>handler</code>，这个函数需要在 <code>Handler</code> 参数中指定</li><li>Lambda 函数的名称是角色名称加上文件名作为前缀，这样就保证了 Lambda 函数只能被该角色调用</li></ul><h3 id="创建-Secret-并设置角色权限"><a href="#创建-Secret-并设置角色权限" class="headerlink" title="创建 Secret 并设置角色权限"></a>创建 Secret 并设置角色权限</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;context&quot;</span></span><br><span class="line">    <span class="string">&quot;errors&quot;</span></span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;github.com/aws/aws-sdk-go/aws&quot;</span></span><br><span class="line">    <span class="string">&quot;github.com/aws/aws-sdk-go/service/secretsmanager&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">CreateAwsSecretKey</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">c context.Context,</span></span></span><br><span class="line"><span class="params"><span class="function">awsRole *iam.CreateRoleOutput,</span></span></span><br><span class="line"><span class="params"><span class="function">amazonConfig aws.Config,</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span> (*secretsmanager.CreateSecretOutput, <span class="type">error</span>) &#123;</span><br><span class="line">secretName := fmt.Sprintf(<span class="string">&quot;%s-Sec&quot;</span>, awsRole.Name)</span><br><span class="line">secretValue := <span class="string">&quot;secret-value&quot;</span></span><br><span class="line">input := &amp;secretsmanager.CreateSecretInput&#123;</span><br><span class="line">Name:         aws.String(secretName),</span><br><span class="line">SecretString: aws.String(secretValue),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">secretManagerClient := secretsmanager.NewFromConfig(amazonConfig)</span><br><span class="line">awsSecretKey, err := secretManagerClient.CreateSecret(c, input)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, errors.New(fmt.Sprintf(<span class="string">&quot;create aws secret key occurred error: %s&quot;</span>, err.Error()))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> awsSecretKey, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在 <code>CreateAwsSecretKey</code> 的方法中，我们创建了一个 Secret</li><li>参数 <code>awsRole</code> 是我们刚才创建的 AWS 角色，包含了角色的名称和 ARN</li><li>创建 Secret 需要 2 个参数，一个是 Secret 名称，另一个是 Secret 值</li><li>创建 Secret 名称时，我们使用角色名称加上 <code>-Sec</code> 作为 Secret 名称，这样就保证了 Secret 只能被该角色访问</li></ul><p>然后我们再为这个 Secret 添加资源策略：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">PutResourcePolicyForAwsSecretKey</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">c context.Context,</span></span></span><br><span class="line"><span class="params"><span class="function">roleARN <span class="type">string</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">awsSecretKey *secretsmanager.CreateSecretOutput,</span></span></span><br><span class="line"><span class="params"><span class="function">amazonConfig aws.Config,</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">resourcePolicy := fmt.Sprintf(<span class="string">`&#123;</span></span><br><span class="line"><span class="string">&quot;Version&quot;: &quot;2012-10-17&quot;,</span></span><br><span class="line"><span class="string">&quot;Statement&quot;: [</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">&quot;Effect&quot;: &quot;Allow&quot;,</span></span><br><span class="line"><span class="string">&quot;Principal&quot;: &#123;</span></span><br><span class="line"><span class="string">&quot;AWS&quot;: [ &quot;%s&quot;  ]</span></span><br><span class="line"><span class="string">&#125;,</span></span><br><span class="line"><span class="string">&quot;Action&quot;: [ &quot;secretsmanager:GetSecretValue&quot;, &quot;secretsmanager:DescribeSecret&quot; ],</span></span><br><span class="line"><span class="string">&quot;Resource&quot;: &quot;%s&quot;</span></span><br><span class="line"><span class="string">&#125;,</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">&quot;Effect&quot;: &quot;Deny&quot;,</span></span><br><span class="line"><span class="string">&quot;Principal&quot; : &quot;*&quot;,</span></span><br><span class="line"><span class="string">&quot;Action&quot; : &quot;secretsmanager:GetSecretValue&quot;,</span></span><br><span class="line"><span class="string">&quot;Resource&quot; : &quot;%s&quot;,</span></span><br><span class="line"><span class="string">&quot;Condition&quot;: &#123;</span></span><br><span class="line"><span class="string">&quot;StringNotEquals&quot;: &#123;</span></span><br><span class="line"><span class="string">&quot;aws:PrincipalArn&quot;: [ &quot;%s&quot; ]</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">]</span></span><br><span class="line"><span class="string">&#125;`</span>,</span><br><span class="line">        roleARN,</span><br><span class="line">        awsSecretKey.ARN,</span><br><span class="line">        awsSecretKey.ARN,</span><br><span class="line">        roleARN,</span><br><span class="line">    )</span><br><span class="line">policyInput := &amp;secretsmanager.PutResourcePolicyInput&#123;</span><br><span class="line">SecretId:       aws.String(awsSecretKey.Name),</span><br><span class="line">ResourcePolicy: aws.String(resourcePolicy),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">secretManagerClient = secretsmanager.NewFromConfig(amazonConfig)</span><br><span class="line">_, err = secretManagerClient.PutResourcePolicy(c, policyInput)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> errors.New(fmt.Sprintf(<span class="string">&quot;put resource policy for aws secret key occurred error: %s&quot;</span>, err.Error()))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在 <code>PutResourcePolicyForAwsSecretKey</code> 的方法中，我们为 Secret 添加了资源策略</li><li>参数 <code>awsSecret</code> 是我们刚才创建的 AWS Secret，包含了 Secret 的名称和 ARN 等信息</li><li>权限策略中允许 <code>role1</code> 角色访问该 Secret，同时拒绝其他角色访问该 Secret</li><li>添加 Secret 权限策略需要 2 个参数，一个是 Secret 名字，另一个是权限策略文档 <code>resourcePolicy</code></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天我们讨论了在开发 AWS 应用时遇到的普遍问题，以及解决这一问题的主要方法。在这个过程中，我们还介绍了 AWS 角色的权限属性，包括权限策略和信任关系，以及如何使用这些属性来隔离资源。最后我们通过 Go 语言代码示例演示了如何创建 AWS 角色、Lambda 函数和 Secret，并设置权限策略。通过代码的方式，我们可以快速创建和设置 AWS 资源，保证资源和权限的安全性。</p><p>关注我，一起学习最新的开发编程新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍如何使用 AWS 角色策略对资源和权限进行隔离</summary>
    
    
    
    <category term="code" scheme="https://zhaozhiming.github.io/categories/code/"/>
    
    
    <category term="go-lang" scheme="https://zhaozhiming.github.io/tags/go-lang/"/>
    
    <category term="aws" scheme="https://zhaozhiming.github.io/tags/aws/"/>
    
    <category term="role" scheme="https://zhaozhiming.github.io/tags/role/"/>
    
    <category term="policy" scheme="https://zhaozhiming.github.io/tags/policy/"/>
    
    <category term="permission" scheme="https://zhaozhiming.github.io/tags/permission/"/>
    
  </entry>
  
  <entry>
    <title>Cursor 让我一天上手 Go 语言</title>
    <link href="https://zhaozhiming.github.io/2024/09/21/Cursor-helped-me-get-up-to-speed-with-Go-in-just-one-day/"/>
    <id>https://zhaozhiming.github.io/2024/09/21/Cursor-helped-me-get-up-to-speed-with-Go-in-just-one-day/</id>
    <published>2024-09-21T00:41:11.000Z</published>
    <updated>2024-10-03T08:49:29.889Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/09/cursor-ai-ide.jpg" class="" width="400" height="300"><p>最近看到网上不少人在讨论 <a href="https://www.cursor.com/">Cursor</a> 这款 AI 代码编辑器，我开始时还不以为然，因为在去年 Cursor 刚推出时我就试用了，当时并没有觉得特别惊艳，感觉和 Github 的 <a href="https://github.com/features/copilot">Copilot</a> 差不多，甚至还不如 Copilot 好用。但最近我被要求去开发一个新项目，我就想在新项目里尝试使用 Cursor 来开发，看看它是否有网上说的那么强大。今天我将为大家讲述我在这个新项目中使用 Cursor 的真实经历，并分享一些我的使用心得和体会。</p><span id="more"></span><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>最近被要求去开发一个新项目，这个项目是另外一位同事用 Go 语言开发的，已经开发了一段时间，但开发进度比原计划落后比较大，所以希望增加开发人手，希望我的加入能够在项目的里程碑节点完成开发工作。但尴尬的是，我之前并没有真正地写过 Go 语言。</p><img src="/images/post/2024/09/cursor-what.jpg" class="" width="600" height="400"><p>虽然我之前没有真正的写过 Go 语言，但或多或少看过一些 Go 语言的代码，而且我有其他语言的开发经验，我相信我可以很快上手 Go 语言，所以我决定接受这个挑战。</p><h2 id="决定使用-Cursor"><a href="#决定使用-Cursor" class="headerlink" title="决定使用 Cursor"></a>决定使用 Cursor</h2><p>在决定加入这个项目的这段时间，我经常在网上看到关于 Cursor 的宣传，我开始时并没有太在意，因为我觉得我已经有了 Github 的 Copilot，而且我去年也试用过 Cursor，没有觉得特别惊艳。但很多人跟我说 Cursor 和以前非常不一样了，强大了很多，被多次洗脑后，我决定尝试在新项目中使用 Cursor。</p><img src="/images/post/2024/09/cursor-strong.jpeg" class="" width="500" height="300"><h3 id="第一天的编码"><a href="#第一天的编码" class="headerlink" title="第一天的编码"></a>第一天的编码</h3><p>在使用 Cursor 之前，我看过一些关于 Cursor 的介绍，其中它的一个功能让我印象深刻，就是它在回答我们问题时可以基于整个代码库的代码来回答，而不是像 Copilot 那样只能基于当前文件的代码来回答。于是我使用 Cursor 开发新项目的第一件事，就是让它帮我介绍这个项目，然后它开始扫描项目的每个文件，给我了下面的回答：</p><img src="/images/post/2024/09/cursor-chat-with-codebase.png" class="" width="1000" height="600"><p>这使我我了解到项目的基本信息：</p><ul><li>项目分为客户端和服务端，都是用 Go 语言开发的</li><li>服务端使用了 <a href="https://github.com/gin-gonic/gin">Gin</a> 开发框架，数据库使用的是 <a href="https://www.postgresql.org/">PostgresSQL</a></li><li>客户端使用了 <a href="https://github.com/spf13/cobra">Cobra</a> 库，用于命令行工具的开发</li></ul><img src="/images/post/2024/09/cursor-wow.png" class="" width="600" height="400"><p>了解了项目的基本情况后，我开始尝试开发一个删除用户的小功能，首先我找到要修改的文件，然后在我要修改的地方按了一下回车，接着神奇地发现 Cursor 已经帮我补全了方法签名，并且 Cursor 知道这是我要添加的功能，然后我再按下 Tab 键，方法的实现代码就出现了，而且是根据项目的代码风格来的，这让我非常惊讶。</p><img src="/images/post/2024/09/cursor-code-complete.png" class="" width="1000" height="600"><p>我怀疑是我在修改代码之前查看了其他关于删除用户的代码，或者是在检索代码片段时使用了一些关键字，让 Cursor 知道了我要添加的功能是什么。</p><p>当然，你不可能所有事情都依赖 Cursor，在修改代码之前，你需要了解项目的基本结构、需要改动哪些文件、需求的业务逻辑等，然后再让 Cursor 帮你补全代码。当代码补全后，我们需要对代码进行检查，看看是否是我们想要的功能，如果有错误，我们需要对代码进行手动修改。</p><p>于是我就用这种方式开始了我的 Go 语言开发之旅，找到修改代码的位置，然后让 Cursor 帮我补全代码，然后我再对代码进行检查，如果有问题则修改，没有问题则进行测试，一直重复这个过程，直到功能开发完成为止。</p><p>这种方式能让我快速地使用 Go 语言开发功能，而且能够快速的学习 Go 语言，在开发过程中，如果对以前的代码或者生成的代码不清楚，我就随时和 Cursor 进行对话，让它帮我解答问题。</p><img src="/images/post/2024/09/cursor-perfect.jpg" class="" width="600" height="400"><h3 id="短暂的快乐"><a href="#短暂的快乐" class="headerlink" title="短暂的快乐"></a>短暂的快乐</h3><p>愉快的时光总是短暂的，在我的频繁使用下很快就超过了 Cursor 的免费限制，当 Cursor 提示我无法再进行代码补全时，我开始考虑是否要充值。因为 Cursor 的价格整整是 Copilot 的 2 倍（Copilot 是 10 美元一个月，而 Cursor 是 20 美元一个月），而且我前段时间刚购买了一年的 Copilot，我真的需要两个 AI 代码编辑器吗？</p><img src="/images/post/2024/09/cursor-choice.png" class="" width="400" height="600"><p>经过反复权衡和艰难抉择，我还是决定充值一个月的 Cursor 试试，因为短暂的使用让我感觉到了 Cursor 的强大，我相信它能帮我更快地完成这个项目。同时我也有点后悔购买了一年的 Copilot，应该按月购买，这样可以灵活更换到其他服务，虽然这样会贵一点，但至少不会浪费。</p><p>如果让我重新选择的话，我会选择 Cursor，因为 Copilot 有的功能 Cursor 都有，而 Cursor 有的功能 Copilot 并没有，而且在整体使用体验上，我更喜欢 Cursor。</p><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>到目前为止，我已经使用了 Cursor 2 个星期了，项目进度进展顺利，Go 语言的基础也掌握的差不多了，现在我已经能独立开发一个完整且比较复杂的前后端功能。在这个过程中，我之所以能够快速掌握 Go 语言，我想有这么几个原因：</p><ul><li>我有过其他语言开发经验，这样阅读 Go 语言的代码并不会觉得特别困难</li><li>项目之前的代码写的比较清晰规范，让人阅读起来比较容易</li><li>Cursor 真的给我很大的帮助，不管是代码补全还是 AI 问答，都能让我快速地学习</li></ul><img src="/images/post/2024/09/cursor-happy-end.jpg" class="" width="600" height="400"><hr><h2 id="使用心得"><a href="#使用心得" class="headerlink" title="使用心得"></a>使用心得</h2><p>这里介绍一下我在使用 Cursor 这段时间的一些心得和体会，因为我之前也使用过 Copilot，所以我会和 Copilot 进行一些对比。</p><h3 id="速度快"><a href="#速度快" class="headerlink" title="速度快"></a>速度快</h3><p>Cursor 给我第一个直观的感受就是<strong>快</strong>，在代码补全方面基本上是秒出结果，而我在使用 Copilot 时，经常看到右下角的 Copilot 图标在转圈，需要让我等待。我认为速度是代码编辑器的一个关键因素，因为在编码过程中，我们希望工具能够高效响应，避免任何卡顿或延迟，以保持开发效率。</p><h3 id="兼容-VSCode"><a href="#兼容-VSCode" class="headerlink" title="兼容 VSCode"></a>兼容 VSCode</h3><p>Cursor 兼容 <a href="https://code.visualstudio.com/">VSCode</a>，可以导入 VSCode 的所有配置信息，包括快捷键设置、代码模板、插件等，因为我之前主要使用 VSCode 进行开发，所以 Cursor 能让我无缝切换到它上面。</p><h3 id="每个项目有单独的对话历史"><a href="#每个项目有单独的对话历史" class="headerlink" title="每个项目有单独的对话历史"></a>每个项目有单独的对话历史</h3><p>在 Cursor 中与 AI 进行对话后，Cursor 会保存每一次对话记录，这样你可以随时查看之前的对话记录，而且这些对话记录是按照项目来保存的，这样不会将不同的项目的对话记录混在一起，非常方便。后面我看了一下 Copilot，发现 Copilot 也有这个功能，但我之前并没有发现。</p><h3 id="代码补全在中间也可以补全"><a href="#代码补全在中间也可以补全" class="headerlink" title="代码补全在中间也可以补全"></a>代码补全在中间也可以补全</h3><p>在 Cursor 中，代码补全不仅可以在代码的末尾补全，还可以在代码的中间补全，而这一点 Copolit 就无能为力了，后者只能在代码的末尾补全。</p><img src="/images/post/2024/09/cursor-middle-complete.png" class="" width="1000" height="600"><h3 id="可以设置系统提示词"><a href="#可以设置系统提示词" class="headerlink" title="可以设置系统提示词"></a>可以设置系统提示词</h3><p>Cursor 可以设置 AI 问答的系统提示词，比如像我们母语为中文的开发人员，我们可以让 AI 都用中文来回答问题，而这一点 Copilot 也是不支持的。</p><img src="/images/post/2024/09/cursor-rule.png" class="" width="1000" height="600"><h3 id="随时随地和-AI-聊天"><a href="#随时随地和-AI-聊天" class="headerlink" title="随时随地和 AI 聊天"></a>随时随地和 AI 聊天</h3><p>Cursor 可以随时随地和 AI 进行对话，不管你是否有在编码，它主要通过以下几种方式进行对话：</p><ul><li>使用快捷键 CMD + L 随时开启对话（同样的快捷键也可以关闭对话），如果选中代码再按快捷键，选中的代码会作为问题的上下文</li><li>使用快捷键 CMD + SHIFT + L 可以将选中的代码追加到当前对话中，并进行提问</li><li>如果代码有报错或者警告信息，将鼠标移动到代码处，Cursor 会提示咨询 AI，这样可以将报错或警告信息作为问题的上下文，然后进行提问</li></ul><img src="/images/post/2024/09/cursor-fix-chat.png" class="" width="1000" height="600"><h3 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h3><p>在 VSCode 中有代码重构的功能，但操作起来比较复杂，而 Cursor 利用代码补全功能，可以很方便地进行重构。比如重命名一个变量，只需要简单的改名字，然后一路按 Tab 键就可以了，如果在补全过程中发现代码格式不对，按 Tab 键时还可以自动格式化代码。</p><img src="/images/post/2024/09/cursor-refactor.png" class="" width="1000" height="600"><hr><h2 id="并非无所不能"><a href="#并非无所不能" class="headerlink" title="并非无所不能"></a>并非无所不能</h2><p>Cursor 并非无所不能，也有很多东西它是做不到的。之前我尝试了一个比较复杂的问题，我将前端项目和后端项目一起放到 Cursor 中，然后在 AI 问答中上传了前端页面的图，<strong>咨询 AI 图中的某个按钮对应的后端 API 代码在哪个文件</strong>，基于整个代码库的方式进行提问。</p><p>结果给 Cursor 不仅找不到对应的后端文件，还编造了一些后端文件。然后我降低了问题的难度，只让 Cursor 找出图片中按钮的前端代码，同样地，Cursor 也没有找到对应的前端文件，还是编造了一些前端文件。</p><p>从目前情况看，Cursor 可能在<strong>图片解析方面还有问题，或者在大而复杂的问题上不够聪明</strong>，并不能像人类一样理解问题的意图，但我相信随着 AI 技术的发展，这些问题都会逐渐解决。</p><h2 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h2><p>未来的编程趋势将显著受到 AI 技术的影响，随着 AI 技术的快速发展，越来越多的编程工具开始集成智能代码生成能力。AI 生成代码不仅能够加快开发速度，还能帮助开发者减少重复性劳动，使他们能够将更多时间投入到创新和复杂问题的解决中。但 AI 生成的代码可能存在逻辑错误、安全漏洞或不符合项目特定需求的问题，因此开发者需要对 AI 生成的代码进行仔细的审查和测试，确保其质量和可靠性。所以以后的编码过程可能是<strong>代码完全由 AI 生成，人类对生成的代码进行审查</strong>，然后让 AI 不断按照人类的要求进行修改，直到生成最终的代码。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我分享了我在使用 Cursor 开发新项目时的一些心得和体会，我认为 Cursor 是一个非常强大的 AI 代码编辑器，它可以帮助开发者快速地学习新技术和开发新项目，提高开发效率。虽然 Cursor 也有一些不足之处，但我相信随着 AI 技术的发展，这些问题都会逐渐解决。我希望未来的编程工具能够更加智能，让开发者能够更加高效地进行编程工作。</p><p>关注我，一起学习各种人工智能和 GenAI 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍在使用 Cursor 开发新项目时的一些心得和体会</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/tags/ai/"/>
    
    <category term="cursor" scheme="https://zhaozhiming.github.io/tags/cursor/"/>
    
    <category term="go-lang" scheme="https://zhaozhiming.github.io/tags/go-lang/"/>
    
  </entry>
  
  <entry>
    <title>一步一步发布公司的 NPM 包</title>
    <link href="https://zhaozhiming.github.io/2024/09/15/publish-company-npm-package-guide/"/>
    <id>https://zhaozhiming.github.io/2024/09/15/publish-company-npm-package-guide/</id>
    <published>2024-09-15T07:18:54.000Z</published>
    <updated>2024-10-03T08:49:29.889Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/09/publish-company-npm.jpeg" class="" width="400" height="300"><p>在项目开发中，有时候需要将自研的 NPM 包会发布到 NPM 的<a href="https://registry.npmjs.org/">公共注册表</a>让别人进行使用，NPM 的公共注册表是一个全球性的包管理库，任何用户都可以访问和下载你发布的公开包。如果你发布的是个人的 NPM 包，那么直接发布就可以了，但是如果你发布的是公司的 NPM 包，那么就需要通过一些流程和规范，来和公司其他同事进行协同合作。本文将为你介绍如何从零开始一步一步发布公司的 NPM 包，如果你的公司现在或以后也有这种需求，那么请跟我一起来学习吧。</p><span id="more"></span><h2 id="什么是-NPM"><a href="#什么是-NPM" class="headerlink" title="什么是 NPM"></a>什么是 NPM</h2><p>NPM 是 Node Package Manager 的缩写，它是一个针对 NPM 编程语言的包管理工具。它是随 Node.js 一起安装的，用于管理和分享 NPM 代码包（modules）。NPM 允许开发者轻松下载、安装、更新、删除各种开源的 NPM 包，并且可以轻松管理项目中的依赖关系。</p><h2 id="人员角色"><a href="#人员角色" class="headerlink" title="人员角色"></a>人员角色</h2><p>在发布公司 NPM 包的过程中，有以下人员或角色，他们主要负责以下工作：</p><ul><li>管理员：负责管理公司 NPM 组织，添加和删除 NPM 组织成员，处理 NPM 包发布权限申请等，可能会有多个管理员。</li><li>开发人员: 指公司内部所有的开发人员，负责开发公司的 NPM 包，发布公司的 NPM 包到 NPM 包管理库。</li></ul><img src="/images/post/2024/09/npm-flow.png" class="" width="1000" height="600"><h2 id="创建公司邮箱群组"><a href="#创建公司邮箱群组" class="headerlink" title="创建公司邮箱群组"></a>创建公司邮箱群组</h2><p>发布 NPM 包需要 NPM 账号，而注册 NPM 账号需要邮箱，如果是发布个人 NPM 包的话直接使用个人邮箱注册就可以了，但是如果是发布公司的 NPM 包，那么要注册 NPM 账号的邮箱就不能是个人邮箱，而应该是一个公司邮箱群组。邮箱群组是一个包含多个员工电子邮箱地址的统一地址，用于向所有成员同时发送邮件。</p><p>使用邮箱群组是出于多方面的考虑，一方面是安全，注册 NPM 账号的邮箱主要是用来接收 NPM 的通知邮件，比如发布 NPM 包成功后的通知。假设有人用公司的名义发了一个恶意的 NPM 包，那么公司的邮箱群组会收到 NPM 的通知邮件，这样公司的管理员们可以及时发现问题，如果是个人邮箱的话，那么就只有个人知道，这样就会增加安全风险。</p><p>另一方面单点问题，设想一下，如果是个人邮箱管理公司的 NPM 包，那么如果这个人离职了，那么这个人的邮箱就作废了，从而导致 NPM 上的组织也作废了，公司需要重新使用新邮箱注册 NPM 账号，重新在 NPM 上面创建组织。而最麻烦的是，以前的 NPM 包公司已经无法管理了，无法再对老的 NPM 包进行升级或者删除。</p><p>所以建议公司在注册 NPM 账号时，使用公司邮箱群组来注册，这样可以避免上述问题。邮箱群组的名字建议叫 <code>npm@公司域名</code>，比如 <code>npm@your-company.com</code>，这样可以清晰的表明这个邮箱群组是用来注册 NPM 账号的。</p><h2 id="管理员创建-NPM-账号与组织"><a href="#管理员创建-NPM-账号与组织" class="headerlink" title="管理员创建 NPM 账号与组织"></a>管理员创建 NPM 账号与组织</h2><p>有了邮箱群组后，管理员首先到 <a href="https://www.npmjs.com/">NPM 网站</a>上创建一个账号，NPM 账号创建可以<a href="https://docs.npmjs.com/creating-a-new-npm-user-account">参考 NPM 的官方文档</a>进行创建。</p><p>创建完 NPM 账号后，管理员需要创建一个 NPM 组织，NPM 组织是一个用于管理公司 NPM 包的组织，公司内部的所有 NPM 包都会发布到这个组织下。NPM 组织的创建可以<a href="https://docs.npmjs.com/creating-an-organization">参考 NPM 的官方文档</a>进行创建。</p><h2 id="开启双因子认证"><a href="#开启双因子认证" class="headerlink" title="开启双因子认证"></a>开启双因子认证</h2><p>对于公司的 NPM 账号，安全性是非常重要的，谁也不想看到有天公司的 NPM 包被别人恶意篡改了。为了增加账号的安全性，建议开启<a href="https://docs.npmjs.com/about-two-factor-authentication">双因子认证</a>。</p><blockquote><p>双因子认证（Two-Factor Authentication，简称 2FA）是一种增强账户安全性的验证方法，通过要求用户在登录或进行敏感操作时提供两种不同类型的认证因素，从而确保用户身份的真实性。与仅依赖单一密码相比，双因子认证显著提高了账户的安全性，减少了未经授权访问的风险。</p></blockquote><p>NPM 上的双因子认证可以通过安全密钥或 TOTP（Time-based One-time Password）来实现。</p><ul><li>安全密钥允许使用生物识别设备（ 比如苹果的 <a href="https://support.apple.com/en-gb/HT204587">Touch ID</a>）进行认证</li><li>TOTP 需要在移动设备上安装认证应用程序（比如 <a href="https://support.google.com/accounts/answer/1066447">Google Authenticator</a>） 来生成验证码</li></ul><h3 id="多管理员设置双因子认证"><a href="#多管理员设置双因子认证" class="headerlink" title="多管理员设置双因子认证"></a>多管理员设置双因子认证</h3><p>对于多管理员的 NPM 账号来说，使用 TOTP 不太方便，因为每个管理员都需要在自己的手机上安装认证应用程序，这样就会增加管理的复杂度。所以建议使用安全密钥的方式来开启双因子认证。</p><p><strong>首个</strong>管理员开启双因子认证后，需要创建<strong>恢复码</strong>，然后将恢复码分享给<strong>其他</strong>管理员，其他管理员<strong>首次登录</strong>可以使用恢复码来登录 NPM 网站，然后设置自己的安全密钥。</p><p>在账号的<code>Profile</code> -&gt; <code>Account</code> -&gt; <code>Two-factor authentication</code> 页面可以设置安全密钥和恢复码。</p><img src="/images/post/2024/09/npm-2fa.png" class="" width="1000" height="600"><p><strong>注意</strong>：NPM 每次可以生成 5 个恢复码，这些恢复码通常用于在无法进行双因子认证时登录账户。一旦您使用某个恢复码成功登录，系统将自动使该恢复码<strong>无效</strong>以防止重复使用。所以当其他管理员使用了恢复码登录后，建议尽快生成新的恢复码，并告知其他管理员。</p><img src="/images/post/2024/09/npm-recover-code.png" class="" width="1000" height="600"><p>其他管理员使用恢复码登录后，可以设置自己的安全密钥，比如使用 Apple Touch ID 作为安全密钥，这样在下次登录时就可以直接使用 Touch ID 而不需要输入恢复码了。</p><h2 id="邀请组织成员"><a href="#邀请组织成员" class="headerlink" title="邀请组织成员"></a>邀请组织成员</h2><p>创建好组织后，就可以邀请开发人员加入组织了，邀请流程如下：</p><ul><li>进入用户设置页面，选择组织名称，比如<code>your-organization</code></li><li>选择<code>成员</code>标签，然后点击<code>邀请成员</code></li><li>输入开发人员的邮箱并点击<code>邀请</code>按钮，NPM 将向开发人员发送一封邮件</li><li>开发人员点击邮件中的链接后，将加入 NPM 组织</li></ul><img src="/images/post/2024/09/npm-invite-member.png" class="" width="1000" height="600"><h2 id="开发人员注册账号"><a href="#开发人员注册账号" class="headerlink" title="开发人员注册账号"></a>开发人员注册账号</h2><p>开发人员也需要到 NPM 网站上创建一个账号，发布 NPM 包需要使用账号进行发布，有以下注意事项：</p><ul><li>建议使用公司邮箱进行注册</li><li>创建账号完成后，开启双因子认证</li></ul><h2 id="发布权限申请"><a href="#发布权限申请" class="headerlink" title="发布权限申请"></a>发布权限申请</h2><p>创建完 NPM 账号后，开发人员需要申请公司 NPM 包发布权限，否则无法发布公司的 NPM 包，申请流程为：</p><ul><li>找管理员将开发人员的 NPM 账号添加到公司的 NPM 组织</li><li>开发人员需要提供 NPM 账号名或注册邮箱给管理员</li><li>管理员邀请开发人员的 NPM 账号到公司的 NPM 组织后，开发人员的邮箱会收到一封邀请邮件</li><li>开发人员点击邮件中的链接后，将加入 NPM 组织</li></ul><p>这样开发人员就拥有了发布公司 NPM 包的权限。</p><img src="/images/post/2024/09/npm-join-org.png" class="" width="1000" height="600"><h2 id="发布-NPM-包"><a href="#发布-NPM-包" class="headerlink" title="发布 NPM 包"></a>发布 NPM 包</h2><p>开发人员申请了公司 NPM 包发布权限后，就可以发布公司的 NPM 包了。</p><h3 id="修改包名"><a href="#修改包名" class="headerlink" title="修改包名"></a>修改包名</h3><p>首先需要修改 NPM 包名称，公司的 NPM 包将以公司组织名称开头，比如 <code>@your-organization</code>。修改你项目中的 <code>package.json</code> 文件，将其中的 <code>name</code> 加上公司的 NPM 组织名称，假设你原来的 NPM 包名为 <code>foo</code>，那么修改后的内容如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;@you-organization/foo&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="使用-NPM-CLI-登录"><a href="#使用-NPM-CLI-登录" class="headerlink" title="使用 NPM CLI 登录"></a>使用 NPM CLI 登录</h3><p>再使用 NPM 命令行工具在终端进行登录，在终端执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm login</span><br></pre></td></tr></table></figure><p>命令执行后，会提示你按回车键打开浏览器链接，打开浏览器后会进入到 NPM 网站。如果你没有在 NPM 网站上登录，会提示输入用户名和密码进行登录。如果你设置了双因子认证，网页会提示你输入安全密钥或验证码。</p><img src="/images/post/2024/09/npm-cli-login.png" class="" width="1000" height="600"><p>完成以上操作后终端会提示你登录成功。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ npm login</span><br><span class="line">npm notice Log <span class="keyword">in</span> on https://registry.npmjs.org/</span><br><span class="line">Login at:</span><br><span class="line">https://www.npmjs.com/login?next=/login/cli/14939f9b-26fa-48df-ad2a-f4ac972897f9</span><br><span class="line">Press ENTER to open <span class="keyword">in</span> the browser...</span><br><span class="line"></span><br><span class="line">Logged <span class="keyword">in</span> on https://registry.npmjs.org/.</span><br></pre></td></tr></table></figure><h3 id="执行发布命令"><a href="#执行发布命令" class="headerlink" title="执行发布命令"></a>执行发布命令</h3><p>在终端登录了 NPM 账号后，执行以下命令进行 NPM 包发布：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm publish --access public</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：发布命令加上 <code>--access public</code> 参数，表示将 NPM 包发布到 NPM 的<strong>公开</strong>包管理库。如果不添加该参数的话，NPM 会默认将你的包发布到 NPM 的<strong>私有</strong>包管理库，这种类型的包管理库是 NPM 的收费服务，如果公司没有购买这种服务，那么发布到私有包管理库会导致发布失败。</p><p>开发人员也可以在 <code>pacakge.json</code> 文件中添加以下内容，然后直接通过 <code>npm publish</code> 命令进行发布，这种方法也会发布到 NPM 公开包管理库。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;publishConfig&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;access&quot;</span><span class="punctuation">:</span> <span class="string">&quot;public&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>执行了发布命令后，跟登录命令一样，会提示你进入 NPM 网站进行双因子认证，认证成功后将发布 NPM 包到 NPM 的公开包管理库。</p><p>发布成功后可以在 NPM 官网的公司组织下查看包是否上传成功，以 <code>next</code> 组织为例，可以在浏览器地址 <code>https://www.npmjs.com/org/next</code> 上查看该组织的 NPM 包信息。</p><img src="/images/post/2024/09/npm-org-packages.png" class="" width="1000" height="600"><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了发布公司 NPM 包的一系列流程和建议，包括作为管理员或者普通开发人员需要执行的一些操作，以及在安全方面需要注意的一些事项。此流程不仅适用于 NPM 包，也适用于其他包管理工具，比如 PIP 或者 Maven 等。如果你也正在发布公司的 NPM 包，那么希望这篇文章对你有所帮助。</p><p>关注我，一起学习各种人工智能和 GenAI 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍管理员如何管理公司的 NPM 账号，以及指导开发人员如何发布公司的 NPM 包</summary>
    
    
    
    <category term="code" scheme="https://zhaozhiming.github.io/categories/code/"/>
    
    
    <category term="javascript" scheme="https://zhaozhiming.github.io/tags/javascript/"/>
    
    <category term="npm" scheme="https://zhaozhiming.github.io/tags/npm/"/>
    
  </entry>
  
  <entry>
    <title>高级 RAG 检索策略之查询路由</title>
    <link href="https://zhaozhiming.github.io/2024/08/06/rag-query-router/"/>
    <id>https://zhaozhiming.github.io/2024/08/06/rag-query-router/</id>
    <published>2024-08-06T01:10:16.000Z</published>
    <updated>2024-10-03T08:49:29.889Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/08/rag-query-router.jpg" class="" width="400" height="300"><p>之前介绍 Self-RAG 的时候提到了其中的按需检索功能，就是根据用户的问题来判断是否需要进行文档检索，如果不需要检索的话则直接返回 LLM（大语言模型）生成的结果，这样不仅可以提升系统的性能，还可以提高用户的体验。在 Self-RAG 中按需检索是通过特殊训练后的 LLM 来实现的，但是在高级 RAG（Retrieval Augmented Generation）检索中我们可以使用<strong>查询路由</strong>来实现这个功能，借助查询路由我们可以轻松实现类似代码中的 If&#x2F;Else 功能。今天我们就来介绍查询路由的原理以及实现方式，并通过代码示例来了解查询路由在实际项目中的使用。</p><span id="more"></span><h2 id="查询路由"><a href="#查询路由" class="headerlink" title="查询路由"></a>查询路由</h2><p>查询路由是 RAG 中的一种智能查询分发功能，能够根据用户输入的语义内容，从多个选项中选择最合适的处理方式或数据源。查询路由能够显著提高 RAG 检索的相关性和效率，适用于各种复杂的信息检索场景，如将用户查询分发到不同的知识库。查询路由的灵活性和智能性使其成为构建高效 RAG 系统的关键组件。</p><img src="/images/post/2024/08/rag-router-flow.png" class="" width="1000" height="600"><h3 id="查询路由的类型"><a href="#查询路由的类型" class="headerlink" title="查询路由的类型"></a>查询路由的类型</h3><p>根据查询路由的实现原理我们可以将其分为两种类型：</p><ul><li>LLM Router：通过构建有效的提示词来让 LLM 判断用户问题的意图，现有的实现有 LlamaIndex Router 等。</li><li>Embedding Router: 通过 Embedding 模型将用户问题转为向量，然后通过相似性检索来判断用户问题的意图，现有的实现有 Semantic Router 等。</li></ul><p>下面我们就来了解这两种查询路由具体的实现原理。</p><h2 id="LLM-Router"><a href="#LLM-Router" class="headerlink" title="LLM Router"></a>LLM Router</h2><p>使用 LLM 来判断用户的意图目前是 RAG 中一种常见的路由方法，首先在提示词中列出问题的所有类别，然后让 LLM 将问题进行分类，最后根据分类结果来选择相应的处理方式。</p><p>LLM 应用框架 <a href="https://www.llamaindex.ai/">LlamaIndex</a> 使用的就是 LLM Router。在 LlamaIndex 中有几种查询路由的实现，比如路由检索器 <code>RouterRetriever</code>、路由查询引擎 <code>RouterQueryEngine</code>、流水线路由模块 <code>RouterComponent</code>，它们的实现原理基本一致，初始化时需要一个选择器和一个工具组件列表，通过选择器来得到工具组件序号，然后根据序号来选择相应的工具组件，最后执行工具组件的处理逻辑。以 <code>RouterQueryEngine</code> 为例，其示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> RouterQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.core.selectors <span class="keyword">import</span> LLMSingleSelector</span><br><span class="line"><span class="keyword">from</span> llama_index.core.tools <span class="keyword">import</span> QueryEngineTool</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize tools</span></span><br><span class="line">list_tool = QueryEngineTool.from_defaults(</span><br><span class="line">    query_engine=list_query_engine,</span><br><span class="line">    description=<span class="string">&quot;Useful for summarization questions related to the data source&quot;</span>,</span><br><span class="line">)</span><br><span class="line">vector_tool = QueryEngineTool.from_defaults(</span><br><span class="line">    query_engine=vector_query_engine,</span><br><span class="line">    description=<span class="string">&quot;Useful for retrieving specific context related to the data source&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize router query engine (single selection, llm)</span></span><br><span class="line">query_engine = RouterQueryEngine(</span><br><span class="line">    selector=LLMSingleSelector.from_defaults(),</span><br><span class="line">    query_engine_tools=[</span><br><span class="line">        list_tool,</span><br><span class="line">        vector_tool,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line">query_engine.query(<span class="string">&quot;&lt;query&gt;&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>首先我们构建 2 个工具 <code>list_tool</code> 和 <code>vector_tool</code>，分别用于总结问题和向量查询，<code>list_tool</code>使用 <code>SummaryIndex</code>来构建检索引擎，<code>vector_tool</code>使用 <code>VectorStoreIndex</code> 来构建检索引擎</li><li>然后初始化 <code>RouterQueryEngine</code>，传入选择器和工具列表</li><li>这里的选择器是 <code>LLMSingleSelector</code>，该选择器使用 LLM 判断用户问题意图并返回单个选择结果</li><li>最后调用 <code>query_engine.query</code> 方法传入用户问题，<code>RouterQueryEngine</code> 根据问题选择相应的工具并执行</li></ul><p>下面是 LlamaIndex Router 的流程图：</p><img src="/images/post/2024/08/llamaindex-router-flow.png" class="" width="400" height="600"><ul><li>首先选择器根据用户问题得到选择结果</li><li>对选择结果进行数据提取，得到工具组件序号</li><li>根据序号选择工具列表中的组件并执行</li></ul><p>在 LlamaIndex 中选择器有 4 种，如下图所示：</p><img src="/images/post/2024/08/llamaindex-selector.png" class="" width="1000" height="600"><p>这 4 种选择器都是通过 LLM 来判断用户问题的意图，按选择结果可以分为单个结果选择器和多个结果选择器，单个结果选择器只返回一个选择结果，多个结果选择器返回多个选择结果，然后会将多个结果合并为一个最终结果。</p><p>按解析结果可以分为文本结果选择器和对象结果选择器，文本结果选择器使用的是 LLM 的 completion API 来生成文本类型的选择结果，格式为：<code>&lt;index&gt;. &lt;reason&gt;</code>，<code>index</code>为选择结果的序号，<code>reason</code>为选择结果的原因。对象结果选择器使用的是 LLM 的 Function Calling API，将选择结果解析成一个 Python 对象，默认的对象为 <code>SingleSelection</code>，其定义如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SingleSelection</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;A single selection of a choice.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    index: <span class="built_in">int</span></span><br><span class="line">    reason: <span class="built_in">str</span></span><br></pre></td></tr></table></figure><p>2 种解析结果示例如下所示：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Text selector</span></span><br><span class="line">2. Useful <span class="keyword">for</span> questions related to oranges</span><br><span class="line"></span><br><span class="line"><span class="comment"># Object selector</span></span><br><span class="line">SingleSelection(index=2, reason=<span class="string">&quot;Useful for questions related to oranges&quot;</span>)</span><br></pre></td></tr></table></figure><p>使用文本结果选择器得到选择结果后，还需要进行额外处理，比如提取出结果中的序号，而使用对象结果选择器则不需要额外处理，可以直接使用对象的属性得到结果。</p><p>我们再来看下选择器的提示词模板：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_SINGLE_SELECT_PROMPT_TMPL = (</span><br><span class="line">    <span class="string">&quot;Some choices are given below. It is provided in a numbered list &quot;</span></span><br><span class="line">    <span class="string">&quot;(1 to &#123;num_choices&#125;), &quot;</span></span><br><span class="line">    <span class="string">&quot;where each item in the list corresponds to a summary.\n&quot;</span></span><br><span class="line">    <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">    <span class="string">&quot;&#123;context_list&#125;&quot;</span></span><br><span class="line">    <span class="string">&quot;\n---------------------\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Using only the choices above and not prior knowledge, return &quot;</span></span><br><span class="line">    <span class="string">&quot;the choice that is most relevant to the question: &#x27;&#123;query_str&#125;&#x27;\n&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>这是 <code>LLMSingleSelector</code> 的默认提示词模板</li><li><code>&#123;num_choices&#125;</code> 为选择结果的数量</li><li><code>&#123;context_list&#125;</code> 为工具组件列表的文本描述，包括序号和描述</li><li><code>&#123;query_str&#125;</code> 为用户问题</li></ul><p>使用 LLM Router 的一个关键就是构建有效的提示词，如果使用的 LLM 足够强大，那么提示词不用很清晰也能达到我们想要的效果，但如果 LLM 不够强大，那么提示词需要不断调整才能得到满意的结果。笔者在使用 LlamaIndex Router 的过程中发现，在选择 OpenAI <code>gpt-3.5-turbo</code> 模型的情况下，使用 <code>LLMSingleSelector</code> 选择器时偶尔会出现解析失败的情况，而使用 <code>PydanticSingleSelector</code> 选择器则比较稳定。</p><p>最后得到选择结果的序号后就可以通过该序号来选择工具组件了，下面是 <code>RouterQueryEngine</code> 的代码片段：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RouterQueryEngine</span>(<span class="title class_ inherited__">BaseQueryEngine</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_query</span>(<span class="params">self, query_bundle: QueryBundle</span>) -&gt; RESPONSE_TYPE:</span><br><span class="line">        ......</span><br><span class="line">        result = <span class="variable language_">self</span>._selector.select(<span class="variable language_">self</span>._metadatas, query_bundle)</span><br><span class="line">        selected_query_engine = <span class="variable language_">self</span>._query_engines[result.ind]</span><br><span class="line">        final_response = selected_query_engine.query(query_bundle)</span><br><span class="line">        ......</span><br></pre></td></tr></table></figure><ul><li><code>RouterQueryEngine</code> 的 <code>_query</code> 方法中首先通过选择器得到选择结果</li><li>然后根据选择结果的序号在 <code>_query_engines</code> 中选择相应的检索引擎</li><li>最后调用检索引擎的 <code>query</code> 方法生成最终结果</li></ul><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ul><li>优点：方法简单，易于实现</li><li>缺点：需要一个比较强大的 LLM 才能正确判断用户问题的意图，如果要将选择结果解析为对象还需要 LLM 具备 Function Calling 的能力</li></ul><h2 id="Embedding-Router"><a href="#Embedding-Router" class="headerlink" title="Embedding Router"></a>Embedding Router</h2><p>查询路由的另外一种实现方式是使用 Embedding 模型将用户问题进行向量化，然后通过向量相似性来将用户问题进行分类，得到分类结果后再选择相应的处理方式。</p><p><a href="https://github.com/aurelio-labs/semantic-router">Semantic Router</a> 是基于该原理实现的一个路由工具，它旨在提供超快的 AI 决策能力，通过语义向量进行快速决策，以提高 LLM 应用和 AI Agent 的效率。 Semantic Router 使用非常简单，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> semantic_router <span class="keyword">import</span> Route</span><br><span class="line"><span class="keyword">from</span> semantic_router.encoders <span class="keyword">import</span> CohereEncoder, OpenAIEncoder</span><br><span class="line"><span class="keyword">from</span> semantic_router.layer <span class="keyword">import</span> RouteLayer</span><br><span class="line"></span><br><span class="line"><span class="comment"># we could use this as a guide for our chatbot to avoid political conversations</span></span><br><span class="line">politics = Route(</span><br><span class="line">    name=<span class="string">&quot;politics&quot;</span>,</span><br><span class="line">    utterances=[</span><br><span class="line">        <span class="string">&quot;isn&#x27;t politics the best thing ever&quot;</span>,</span><br><span class="line">        <span class="string">&quot;why don&#x27;t you tell me about your political opinions&quot;</span>,</span><br><span class="line">        <span class="string">&quot;don&#x27;t you just love the president&quot;</span>,</span><br><span class="line">        <span class="string">&quot;they&#x27;re going to destroy this country!&quot;</span>,</span><br><span class="line">        <span class="string">&quot;they will save the country!&quot;</span>,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># this could be used as an indicator to our chatbot to switch to a more</span></span><br><span class="line"><span class="comment"># conversational prompt</span></span><br><span class="line">chitchat = Route(</span><br><span class="line">    name=<span class="string">&quot;chitchat&quot;</span>,</span><br><span class="line">    utterances=[</span><br><span class="line">        <span class="string">&quot;how&#x27;s the weather today?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;how are things going?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;lovely weather today&quot;</span>,</span><br><span class="line">        <span class="string">&quot;the weather is horrendous&quot;</span>,</span><br><span class="line">        <span class="string">&quot;let&#x27;s go to the chippy&quot;</span>,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># we place both of our decisions together into single list</span></span><br><span class="line">routes = [politics, chitchat]</span><br><span class="line"></span><br><span class="line"><span class="comment"># OpenAI Encoder</span></span><br><span class="line">os.environ[<span class="string">&quot;OPENAI_API_KEY&quot;</span>] = <span class="string">&quot;&lt;YOUR_API_KEY&gt;&quot;</span></span><br><span class="line">encoder = OpenAIEncoder()</span><br><span class="line"></span><br><span class="line">rl = RouteLayer(encoder=encoder, routes=routes)</span><br><span class="line"></span><br><span class="line">rl(<span class="string">&quot;don&#x27;t you love politics?&quot;</span>).name</span><br><span class="line"><span class="comment"># politics</span></span><br><span class="line">rl(<span class="string">&quot;how&#x27;s the weather today?&quot;</span>).name</span><br><span class="line"><span class="comment"># chitchat</span></span><br></pre></td></tr></table></figure><ul><li>首先定义 2 个 Route，分别是 <code>politics</code> 和 <code>chitchat</code>，每个 Route 包含多个示例语句</li><li>然后创建一个 Encoder，这里使用的是 OpenAI 的 Encoder，利用 OpenAI 的 Embedding 模型来生成向量</li><li>最后创建一个 RouteLayer，传入 Encoder 和 Route 列表</li><li>调用 RouteLayer 方法传入用户问题，得到分类结果，<strong>注意</strong>：并不是每一个用户问题都会得到一个预设的分类结果，如果用户问题不在预设的分类中，那么分类结果可能为空</li></ul><p>OpenAI Encoder 默认使用的是 <code>text-embedding-3-small</code> Embedding 模型，它比 OpenAI 之前的 <code>text-embedding-ada-002</code> Embedding 模型效果更好且价格更便宜。同时 Semantic Router 还支持其他 Encoder，比如 Huggingface Encoder，它默认使用的是 <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">sentence-transformers&#x2F;all-MiniLM-L6-v2</a> Embedding 模型，这是一个句子转换模型，它将句子和段落映射到一个 384 维度的向量空间，可用于分类或语义搜索等任务。</p><h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><ul><li>优点：只需要使用 Embedding 模型，相比 LLM Router 效率更高，消耗资源更少</li><li>缺点：需要提前录入一些示例语句，如果示例语句不够多或者不够全面，分类效果可能不太好</li></ul><h2 id="查询路由实践"><a href="#查询路由实践" class="headerlink" title="查询路由实践"></a>查询路由实践</h2><p>下面我们结合 LlamaIndex 和 Semantic Router 来实现一个查询路由，该路由会将用户的问题分发到不同的工具组件中，这些工具组件包括：使用 LLM 和用户进行闲聊，使用 RAG 流程检索文档并生成答案，以及使用 Bing 搜索引擎进行网络搜索。</p><img src="/images/post/2024/08/rag-router-practice.png" class="" width="1000" height="600"><p>首先我们定义一个与 LLM 闲聊的工具组件，这里我们使用 LlamaIndex 的 <a href="https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline/">Pipeline</a> 功能来构建一个查询流水线，更多的查询流水线功能可以参考我之前的<a href="https://zhaozhiming.github.io/2024/06/08/rag-module-pipeline/">这篇文章</a>，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.llms.openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> llama_index.core.query_pipeline <span class="keyword">import</span> QueryPipeline, InputComponent</span><br><span class="line"></span><br><span class="line">llm = OpenAI(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>, system_prompt=<span class="string">&quot;You are a helpful assistant.&quot;</span>)</span><br><span class="line">chitchat_p = QueryPipeline(verbose=<span class="literal">True</span>)</span><br><span class="line">chitchat_p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: InputComponent(),</span><br><span class="line">        <span class="string">&quot;llm&quot;</span>: llm,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line">chitchat_p.add_link(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;llm&quot;</span>)</span><br><span class="line">output = chitchat_p.run(<span class="built_in">input</span>=<span class="string">&quot;hello&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output: <span class="subst">&#123;output&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Output: assistant: Hello! How can I assist you today?</span><br></pre></td></tr></table></figure><ul><li>这里我们使用 OpenAI 的 <code>gpt-3.5-turbo</code> 模型来构建一个 LLM</li><li>然后使用 <code>QueryPipeline</code> 来构建一个查询流水线，添加 <code>input</code> 和 <code>llm</code> 两个模块，<code>input</code>模块是一个输入组件，默认输入参数键名称为 <code>input</code></li><li>接着添加两个模块的连接关系</li><li>最后调用 <code>run</code> 方法传入用户问题，得到回答</li></ul><p>然后我们再添加一个普通 RAG 的工具组件，同样是创建一个查询流水线，这里的测试文档还是用维基百科上的<a href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.core.response_synthesizers.tree_summarize <span class="keyword">import</span> TreeSummarize</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents)</span><br><span class="line">retriever = index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">rag_p = QueryPipeline(verbose=<span class="literal">True</span>)</span><br><span class="line">rag_p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: InputComponent(),</span><br><span class="line">        <span class="string">&quot;retriever&quot;</span>: retriever,</span><br><span class="line">        <span class="string">&quot;output&quot;</span>: TreeSummarize(),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rag_p.add_link(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;retriever&quot;</span>)</span><br><span class="line">rag_p.add_link(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;output&quot;</span>, dest_key=<span class="string">&quot;query_str&quot;</span>)</span><br><span class="line">rag_p.add_link(<span class="string">&quot;retriever&quot;</span>, <span class="string">&quot;output&quot;</span>, dest_key=<span class="string">&quot;nodes&quot;</span>)</span><br><span class="line">output = rag_p.run(<span class="built_in">input</span>=<span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output: <span class="subst">&#123;output&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Output: Tony Stark <span class="keyword">and</span> Bruce Banner.</span><br></pre></td></tr></table></figure><ul><li>前面部分是 LlamaIndex 常用的检索器构建流程，使用 <code>SimpleDirectoryReader</code> 来加载测试文档，然后使用 <code>VectorStoreIndex</code> 来构建一个检索器</li><li>创建一个查询流水线，添加 <code>input</code>、<code>retriever</code> 和 <code>output</code> 三个模块，<code>output</code> 模块是一个树形总结组件</li><li>添加三个模块的连接关系，<code>output</code>模块需要使用到 <code>input</code> 模块和 <code>retirever</code> 模块的输出结果</li><li>最后调用 <code>run</code> 方法传入用户问题，得到回答</li></ul><p>接下来我们再添加一个使用 Bing 搜索引擎的工具组件，同样我们使用查询流水线来进行创建，但这一次需要用到自定义模块，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">web_p = QueryPipeline(verbose=<span class="literal">True</span>)</span><br><span class="line">web_p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: InputComponent(),</span><br><span class="line">        <span class="string">&quot;web_search&quot;</span>: WebSearchComponent(),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line">web_p.add_link(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;web_search&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>网络搜索工具比较简单，只有 2 个模块，<code>input</code> 和 <code>web_search</code></li><li>其中的 <code>WebSearchComponent</code> 是一个自定义模块，下面我们会详细介绍这个模块的实现</li></ul><p>在实现这个自定义模块之前，我们需要先在 Azure 上创建一个 Bing 搜索服务，然后获取 API Key，具体操作可以参考微软的<a href="https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/overview">官方文档</a>。然后安装 LlamaIndex 的 Bing 查询工具库：<code>pip install llama-index-tools-bing-search</code>，然后就可以开始实现自定义模块了，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Any</span></span><br><span class="line"><span class="keyword">from</span> llama_index.core.query_pipeline <span class="keyword">import</span> CustomQueryComponent</span><br><span class="line"><span class="keyword">from</span> llama_index.tools.bing_search <span class="keyword">import</span> BingSearchToolSpec</span><br><span class="line"><span class="keyword">from</span> llama_index.agent.openai <span class="keyword">import</span> OpenAIAgent</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WebSearchComponent</span>(<span class="title class_ inherited__">CustomQueryComponent</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Web search component.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_validate_component_inputs</span>(<span class="params">self, <span class="built_in">input</span>: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Validate component inputs during run_component.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="string">&quot;input&quot;</span> <span class="keyword">in</span> <span class="built_in">input</span>, <span class="string">&quot;input is required&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_input_keys</span>(<span class="params">self</span>) -&gt; <span class="built_in">set</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Input keys dict.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_output_keys</span>(<span class="params">self</span>) -&gt; <span class="built_in">set</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;output&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_run_component</span>(<span class="params">self, **kwargs</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run the component.&quot;&quot;&quot;</span></span><br><span class="line">        tool_spec = BingSearchToolSpec(api_key=os.getenv(<span class="string">&quot;BING_SEARCH_API_KEY&quot;</span>))</span><br><span class="line">        agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())</span><br><span class="line">        question = kwargs[<span class="string">&quot;input&quot;</span>]</span><br><span class="line">        result = agent.chat(question)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;output&quot;</span>: result&#125;</span><br></pre></td></tr></table></figure><ul><li>我们直接看自定义组件中的核心方法 <code>_run_component</code></li><li>首先我们创建一个 <code>BingSearchToolSpec</code> 对象，传入 Bing 搜索引擎的 API Key，这里我们将 API Key 保存到环境变量 <code>BING_SEARCH_API_KEY</code> 中</li><li>这里我们使用了 LlamaIndex 的 Agent 功能，我们使用 <code>OpenAIAgent</code> 对象并传入 Bing 搜索工具</li><li>最后通过 <code>kwargs[&quot;input&quot;]</code> 获取用户问题并传递给 <code>agent.chat</code> 方法，得到搜索结果并返回</li><li>Bing 查询工具更多的用法可以参考<a href="https://llamahub.ai/l/tools/llama-index-tools-bing-search?from=">其文档</a></li></ul><p>3 个工具组件创建之后，我们需要创建一个路由模块，我们使用 Semantic Router 来实现这个路由模块，我们先定义 Semantic Router 的几个 Route，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">chitchat = Route(</span><br><span class="line">    name=<span class="string">&quot;chitchat&quot;</span>,</span><br><span class="line">    utterances=[</span><br><span class="line">        <span class="string">&quot;how&#x27;s the weather today?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;how are things going?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;lovely weather today&quot;</span>,</span><br><span class="line">        <span class="string">&quot;the weather is horrendous&quot;</span>,</span><br><span class="line">        <span class="string">&quot;let&#x27;s go to the chippy&quot;</span>,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rag = Route(</span><br><span class="line">    name=<span class="string">&quot;rag&quot;</span>,</span><br><span class="line">    utterances=[</span><br><span class="line">        <span class="string">&quot;What mysterious object did Loki use in his attempt to conquer Earth?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;How did Thanos achieve his plan of exterminating half of all life in the universe?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;What method did the Avengers use to reverse Thanos&#x27; actions?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Which member of the Avengers sacrificed themselves to defeat Thanos?&quot;</span>,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">web = Route(</span><br><span class="line">    name=<span class="string">&quot;web&quot;</span>,</span><br><span class="line">    utterances=[</span><br><span class="line">        <span class="string">&quot;Search online for the top three countries in the 2024 Paris Olympics medal table.&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Find the latest news about the U.S. presidential election.&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Look up the current updates on NVIDIA’s stock performance today.&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Search for what Musk said on X last month.&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Find the latest AI news.&quot;</span>,</span><br><span class="line">    ],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>这里我们定义了 3 个 Route，分别针对 3 种不同的问题类型</li><li><code>chitchat</code> Route 的示例语句是一些闲聊语句，对应 <code>chitchat</code> 工具组件</li><li><code>rag</code> Route 的示例语句是一些关于复仇者联盟电影剧情的问题，对应 <code>rag</code> 工具组件</li><li><code>web</code> Route 的示例语句是一些关于网络搜索的问题， 其中有不少 <code>Search</code>、<code>Find</code> 等关键词，对应 <code>web</code> 工具组件</li></ul><p>接下来我们创建一个自定义的路由模块，使用 Semantic Router 来实现查询路由，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.base.query_pipeline.query <span class="keyword">import</span> (</span><br><span class="line">    QueryComponent,</span><br><span class="line">    QUERY_COMPONENT_TYPE,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> llama_index.core.bridge.pydantic <span class="keyword">import</span> Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SemanticRouterComponent</span>(<span class="title class_ inherited__">CustomQueryComponent</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Semantic router component.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    components: <span class="type">Dict</span>[<span class="built_in">str</span>, QueryComponent] = Field(</span><br><span class="line">        ..., description=<span class="string">&quot;Components (must correspond to choices)&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, components: <span class="type">Dict</span>[<span class="built_in">str</span>, QUERY_COMPONENT_TYPE]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Init.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(components=components)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_validate_component_inputs</span>(<span class="params">self, <span class="built_in">input</span>: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Validate component inputs during run_component.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_input_keys</span>(<span class="params">self</span>) -&gt; <span class="built_in">set</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Input keys dict.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_output_keys</span>(<span class="params">self</span>) -&gt; <span class="built_in">set</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;output&quot;</span>, <span class="string">&quot;selection&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_run_component</span>(<span class="params">self, **kwargs</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run the component.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.components) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;No components&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> chitchat.name <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.components.keys():</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;No chitchat component&quot;</span>)</span><br><span class="line"></span><br><span class="line">        routes = [chitchat, rag, web]</span><br><span class="line">        encoder = OpenAIEncoder()</span><br><span class="line">        rl = RouteLayer(encoder=encoder, routes=routes)</span><br><span class="line">        question = kwargs[<span class="string">&quot;input&quot;</span>]</span><br><span class="line">        selection = rl(question).name</span><br><span class="line">        <span class="keyword">if</span> selection <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = <span class="variable language_">self</span>.components[selection].run_component(<span class="built_in">input</span>=question)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = <span class="variable language_">self</span>.components[<span class="string">&quot;chitchat&quot;</span>].run_component(<span class="built_in">input</span>=question)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;output&quot;</span>: output, <span class="string">&quot;selection&quot;</span>: selection&#125;</span><br></pre></td></tr></table></figure><ul><li>在自定义模块的构造器函数 <code>__init__</code> 中我们传入了一个字典，字典的键是 Route 的名称，值是对应的工具组件</li><li>在 <code>_output_keys</code> 方法中我们返回了 2 个输出键，一个是输出结果，一个是选择结果</li><li>在 <code>_run_component</code> 方法中我们首先对工具组件参数进行验证，确保有 <code>chitchat</code> 这个工具组件，因为我们需要将无法分类的问题分发到 <code>chitchat</code> 工具组件</li><li>然后我们使用 Semantic Router 来判断用户问题的意图，得到选择结果 <code>selection</code></li><li>再根据选择结果来选择相应的工具组件并执行</li><li>如果选择结果为空，则选择 <code>chitchat</code> 工具组件并执行</li><li>最后返回输出结果和选择结果</li></ul><p>最后我们将所有的工具组件和路由模块添加到一个单独的查询流水线中，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">p = QueryPipeline(verbose=<span class="literal">True</span>)</span><br><span class="line">p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;router&quot;</span>: SemanticRouterComponent(</span><br><span class="line">            components=&#123;</span><br><span class="line">                <span class="string">&quot;chitchat&quot;</span>: chitchat_p,</span><br><span class="line">                <span class="string">&quot;rag&quot;</span>: rag_p,</span><br><span class="line">                <span class="string">&quot;web&quot;</span>: web_p,</span><br><span class="line">            &#125;</span><br><span class="line">        ),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>新建的查询流水线只有一个模块 <code>router</code>，这个模块是我们自定义的路由模块 <code>SemanticRouterComponent</code></li><li>在路由模块中我们传入了 3 个之前定义的查询流水线，表示不同的用户意图执行不同的查询流水线</li><li>因为只有一个模块，所以无需添加连接关系</li></ul><p>下面我们来执行一下这个流水线，看看效果如何：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">output = p.run(<span class="built_in">input</span>=<span class="string">&quot;hello&quot;</span>)</span><br><span class="line"><span class="comment"># Selection: chitchat</span></span><br><span class="line"><span class="comment"># Output: assistant: Hello! How can I assist you today?</span></span><br><span class="line"></span><br><span class="line">output = p.run(<span class="built_in">input</span>=<span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span>)</span><br><span class="line"><span class="comment"># Selection: rag</span></span><br><span class="line"><span class="comment"># Output: Tony Stark and Bruce Banner.</span></span><br><span class="line"></span><br><span class="line">output = p.run(<span class="built_in">input</span>=<span class="string">&quot;Search online for the top three countries in the 2024 Paris Olympics medal table.&quot;</span>)</span><br><span class="line"><span class="comment"># Selection: web</span></span><br><span class="line"><span class="comment"># Output: The top three countries in the latest medal table for the 2024 Paris Olympics are as follows:</span></span><br><span class="line"><span class="comment"># 1. United States</span></span><br><span class="line"><span class="comment"># 2. China</span></span><br><span class="line"><span class="comment"># 3. Great Britain</span></span><br></pre></td></tr></table></figure><p>可以看到我们的查询路由工作的很好，根据用户问题的不同意图选择了不同的工具组件，并得到了相应的结果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天我们介绍了 RAG 检索策略中的查询路由，并介绍了 LLM Router 和 Embedding Router 两种查询路由的实现原理，最后通过一个实战项目了解了查询路由在实际项目中的使用。但目前的查询路由还有很多不确定性，因此我们无法保证查询路由总能做出完全准确的决策，需要经过精心测试才能得到更加可靠的 RAG 应用程序。</p><p>关注我，一起学习各种人工智能和 GenAI 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p><h2 id="参考引用"><a href="#参考引用" class="headerlink" title="参考引用"></a>参考引用</h2><ul><li><a href="https://medium.com/towards-data-science/routing-in-rag-driven-applications-a685460a7220">Routing in RAG-Driven Applications</a></li></ul>]]></content>
    
    
    <summary type="html">介绍 RAG 检索策略中的查询路由，以及如何在检索时使用</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="router" scheme="https://zhaozhiming.github.io/tags/router/"/>
    
    <category term="semantic-router" scheme="https://zhaozhiming.github.io/tags/semantic-router/"/>
    
  </entry>
  
  <entry>
    <title>都 2024 年了，你还在用 pip 吗？</title>
    <link href="https://zhaozhiming.github.io/2024/07/15/modern-python-project-tools-config/"/>
    <id>https://zhaozhiming.github.io/2024/07/15/modern-python-project-tools-config/</id>
    <published>2024-07-15T02:49:25.000Z</published>
    <updated>2024-10-03T08:49:29.889Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/07/next-generation-python-tools.jpg" class="" width="400" height="300"><p>编程语言 Python 随着 AI 的发展越来越受开发人员的喜爱，目前已经是最流行的编程语言，但由于 Python 是一门相对较老的语言，并且经过 Python2 到 Python3 这一漫长而复杂的迁移历程，使得一些 Python 开发人员可能还在使用一些过时的工具和库。今天我们就来带大家了解当前 Python 生态系统中最流行最实用的开发工具，让你彻底告别那些<code>老古董</code>。</p><span id="more"></span><h2 id="Python-环境管理工具"><a href="#Python-环境管理工具" class="headerlink" title="Python 环境管理工具"></a>Python 环境管理工具</h2><p>Python 项目中虚拟环境的管理非常重要，虚拟环境可以帮助我们在不同项目中使用不同的 Python 版本和依赖库，避免不同项目之间的依赖冲突。Python 最早使用的是 <code>virtualenv</code> 工具来管理虚拟环境，到了 Python3.3 后自带了内置的创建虚拟环境模块 <code>venv</code>，但它们都存在一个问题，就是只能使用同一个 Python 版本创建虚拟环境。假如你在 A 项目想使用 Python3.10，而在 B 项目想使用 Python3.9，那么你就需要安装两个不同版本的 Python，然后分别使用 <code>virtualenv</code> 或 <code>venv</code> 来创建虚拟环境。</p><p>在使用了一些 Python 环境管理工具后，我比较推荐 <a href="https://docs.anaconda.com/miniconda/">Miniconda</a> 这个工具，它是一个轻量级的 Conda 版本，可以帮助我们管理 Python 环境和依赖库，但我一般不使用它来管理依赖库，主要使用它来管理 Python 环境。</p><p>虽然 Miniconda 与其他 Python 环境管理工具（比如 <a href="https://github.com/pyenv/pyenv">pyenv</a>）相比会重量级一些（包含了一些数据科学相关的库），而且功能也不是很纯粹（即有环境管理也有依赖管理等功能）， 但是它的优势在于环境管理的功能非常出色，可以轻松地在不同操作系统上进行安装，同时适配 bash、fish、zsh 等不同的 shell 环境，而且跟 <code>virtualenv</code> 和 <code>venv</code> 一起使用也不会引起冲突。</p><h3 id="Miniconda-使用示例"><a href="#Miniconda-使用示例" class="headerlink" title="Miniconda 使用示例"></a>Miniconda 使用示例</h3><p>以下是 Miniconda 的一些常用命令：</p><ul><li>创建 Python 环境： <code>conda create -n myenv python=3.10</code></li><li>切换 Python 环境： <code>conda activate myenv</code></li><li>展示所有 Python 环境： <code>conda env list</code></li><li>初始化 shell 环境： <code>conda init &lt;bash/fish/zsh&gt;</code></li></ul><h2 id="Python-依赖管理工具"><a href="#Python-依赖管理工具" class="headerlink" title="Python 依赖管理工具"></a>Python 依赖管理工具</h2><p>Python 项目中最常使用的工具应该要属 Pip 了，Pip 是 Python 的依赖管理工具，用于安装和管理 Python 依赖，虽然 Pip 功能强大，但在管理项目依赖时存在一些问题，比如新增依赖时需要手动修改 <code>requirements.txt</code> 文件，而且没有版本锁定功能，导致在不同环境中安装的依赖版本可能不一致。</p><p>其他编程语言比如 JS 则能有效地处理这种情况，使用它的依赖管理工具 <code>npm</code> 会产生一个 <code>package.json</code> 文件来管理项目依赖，然后生成一个 <code>package-lock.json</code> 文件来锁定依赖版本，确保在不同环境中安装相同的依赖版本。</p><p>为了解决这一问题，在 Python 中出现了不少依赖管理工具，<a href="https://python-poetry.org/">Poetry</a> 是其中一个比较流行的工具。Poetry 使用一个 <code>pyproject.toml</code> 文件来管理项目的所有依赖项和元数据，使项目配置更加简洁明了，它会自动处理依赖项的版本冲突，并且能够生成锁文件 <code>poetry.lock</code>，确保在不同环境中安装相同的依赖版本。</p><h3 id="Poetry-安装"><a href="#Poetry-安装" class="headerlink" title="Poetry 安装"></a>Poetry 安装</h3><p>Poetry 有多种安装方式，最简单的是通过 Python 脚本进行安装，安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https://install.python-poetry.org | python3 -</span><br></pre></td></tr></table></figure><p>安装完成后，可以通过 Miniconda 创建一个特定版本的 Python 环境，然后在这个环境中使用 Poetry，具体操作如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 conda 创建一个 Python 环境</span></span><br><span class="line">conda create -n myenv python=3.10</span><br><span class="line"><span class="comment"># 切换到这个 Python 环境</span></span><br><span class="line">conda activate myenv</span><br><span class="line"><span class="comment"># 构建一个 Poetry 虚拟环境</span></span><br><span class="line">poetry <span class="built_in">env</span> use python</span><br><span class="line"><span class="comment"># 进入该环境</span></span><br><span class="line">poetry shell</span><br></pre></td></tr></table></figure><h3 id="Poetry-使用示例"><a href="#Poetry-使用示例" class="headerlink" title="Poetry 使用示例"></a>Poetry 使用示例</h3><p>在新项目中使用 Poetry 可以使用 <code>poetry new &lt;project_name&gt;</code> 来创建一个新的 Python 项目，如果是已有项目，可以在项目目录中使用 <code>poetry init</code> 命令来初始化项目，这两个命令都会生成一个 <code>pyproject.toml</code> 文件，用于管理项目的依赖项和元数据。</p><p>你可以使用 <code>poetry install</code> 来安装项目的所有依赖，如果是初次运行该命令，Poetry 会生成一个 <code>poetry.lock</code> 文件，用于锁定项目的依赖版本，确保在不同环境中安装相同的依赖版本。也可以使用 <code>poetry add &lt;package_name&gt;</code> 命令来单独添加一个依赖，这样 Poetry 会自动更新 <code>pyproject.toml</code> 和 <code>poetry.lock</code> 文件。</p><p>我们通过一个简单的例子来看下 Poetry 对依赖管理的方法，假设我们在项目中安装了一个新的依赖 <code>requests</code>，那么可以使用如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry add requests</span><br></pre></td></tr></table></figure><p>安装完成后，在 <code>pyproject.toml</code> 文件中只会添加 <code>requests</code> 这个依赖的信息：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[tool.poetry.dependencies]</span></span><br><span class="line"><span class="attr">requests</span> = <span class="string">&quot;^2.32.3&quot;</span></span><br></pre></td></tr></table></figure><p>但是在 <code>poetry.lock</code> 文件会中除了添加 <code>requests</code> 这个依赖外，还会添加 <code>requests</code> 这个依赖所需的其他依赖库的信息，可以使用以下命令查看依赖之间的关系：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ poetry show --tree</span><br><span class="line">requests 2.32.3 Python HTTP <span class="keyword">for</span> Humans.</span><br><span class="line">├── certifi &gt;=2017.4.17</span><br><span class="line">├── charset-normalizer &gt;=2,&lt;4</span><br><span class="line">├── idna &gt;=2.5,&lt;4</span><br><span class="line">└── urllib3 &gt;=1.21.1,&lt;3</span><br></pre></td></tr></table></figure><p>可以看到 <code>requests</code> 是项目根目录下的依赖，而其他几个依赖是 <code>requests</code> 所需的依赖。</p><p>如果你用 Pip 来安装依赖，那么 <code>pip install requests</code> 后再用 <code>pip freeze &gt; requirements.txt</code> 命令生成的 <code>requirements.txt</code> 文件会包含所有依赖的信息，使得你分不清哪些是项目的依赖，哪些是衍生的依赖。</p><p>Poetry 还允许你将 lock 文件导出成 <code>requirements.txt</code> 文件，这样你就可以使用 <code>pip install -r requirements.txt</code> 来安装项目的依赖，具体命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry <span class="built_in">export</span> --without-hashes --format=requirements.txt --output requirements.txt</span><br></pre></td></tr></table></figure><p>更多的 Poetry 使用方法可以参<a href="https://python-poetry.org/docs/">考官方文档</a>。</p><h3 id="其他依赖管理工具"><a href="#其他依赖管理工具" class="headerlink" title="其他依赖管理工具"></a>其他依赖管理工具</h3><p>除了 Poetry 外，还有一些其他的 Python 依赖管理工具，比如 <a href="https://pdm-project.org/en/latest/">Pdm</a> 就是不错的选择，它整体和 Poetry 类似，它还包含了 Python 环境管理的功能，这样就不需要和 Miniconda 配合也可直接使用，但是在流行度上比 Poetry 稍微逊色一些（截止时间 2024 年 7 月，Poetry 的 Github Star 数是 30K，而 Pdm 的 Github Star 数是 7.6K），可能是 Pdm 比 Poetry 发布时间晚的关系（Poetry 是 2018 年发布的，而 Pdm 是 2020 年发布的），后面如果 Pdm 发展的好的话，说不定会超过 Poetry。</p><p>另外一个值得推荐的 Python 依赖管理工具是 <a href="https://github.com/astral-sh/uv">Uv</a>，它是一款用 Rust 编写的极速 Python 包安装和解析工具，旨在作为 Pip 和 Pip-tools 的替代品，并逐步发展成为一个全面的 Python 项目和包管理器，下面是它和其他工具安装依赖的速度对比图：</p><img src="/images/post/2024/07/uv-compare.png" class="" width="1000" height="600"><p>得益于 Rust 的高性能，在速度上 Uv 完全碾压了其他工具，但可惜的是 Uv 目前还是使用 <code>requirements.txt</code> 来管理依赖，这样就无法保证可以在不同环境安装相同的依赖，而且 Uv 也不能和 Poetry 一起使用，因为 Uv 是按照兼容 Pip 的思路进行开发，而 Poetry 内部已经不用 Pip 做依赖管理了。</p><p>尽管 Uv 无法和 Poetry 一起使用，但是我们可以在 CI&#x2F;CD 中使用 Uv 来加速依赖安装，比如我们先用 Poetry 导出 <code>requirements.txt</code> 文件，然后在 CI&#x2F;CD 中使用 Uv 来安装依赖，这样就可以大大缩短依赖安装的时间。</p><h3 id="Pip-是否过时"><a href="#Pip-是否过时" class="headerlink" title="Pip 是否过时"></a>Pip 是否过时</h3><p>虽然这些依赖管理工具很强大，但一些小型项目可能更多开发人员还是会选择 Pip，因为 Pip 无需额外安装其他工具，一般有 Python 环境就可以直接使用。另外一点是编程语言的原生工具也在不断发展，以 JS 为例，npm 刚开始时也不支持 lock 文件，但后面参考了一些 JS 的流行工具，在社区的共同努力下慢慢完善了这个功能，所以 Pip 也有可能在未来的某个版本中加入类似 Poetry 的功能。</p><p>所以在 Pip 没有发展完善之前，我们可以使用 Poetry 这样的工具来解决依赖管理的问题，同时也能简化项目配置，提高开发效率。</p><h2 id="代码规范工具"><a href="#代码规范工具" class="headerlink" title="代码规范工具"></a>代码规范工具</h2><p>另外一种常用的工具是代码规范类工具，因为 Python 的语法比较灵活，所以在团队协作中可能会出现代码风格不一致的问题，为了解决这个问题，每种编程语言都会有一些代码规范工具，这类工具包括代码格式化工具、代码检查工具等。</p><p>在 Python 中代码格式化工具有 <a href="https://github.com/psf/black">Black</a>、<a href="https://github.com/google/yapf">YAPF</a>、<a href="https://github.com/hhatto/autopep8">autopep8</a> 等，而代码检查工具有 <a href="https://github.com/PyCQA/flake8">Flake8</a>、<a href="https://github.com/pylint-dev/pylint">Pylint</a>、<a href="https://github.com/python/mypy">mypy</a> 等，这些工具都可以通过 Pip 来安装，然后在项目中使用。</p><p>而最近比较流行的一个代码规范工具 <a href="https://github.com/astral-sh/ruff">Ruff</a>，它同时集成了代码格式化和代码检查功能，可以帮助我们更好地在项目中统一代码风格。Ruff 是 Uv 开发团队开发的另外一款工具，同样是使用 Rust 语言进行编写，从而使它的性能远远高于其他同类型的工具，下面是 Ruff 和其他工具的性能对比图：</p><img src="/images/post/2024/07/ruff-compare.png" class="" width="1000" height="600"><p>Ruff 内部集成了 Black 和 Flake8 等工具，下面我们就来介绍下 Ruff 如何安装及使用。</p><h3 id="Ruff-安装"><a href="#Ruff-安装" class="headerlink" title="Ruff 安装"></a>Ruff 安装</h3><p>Ruff 可以通过 Pip 或 Poetry 进行安装，在项目中使用的话，推荐使用 Poetry 安装到 dev 开发依赖中，表示这个工具只在开发环境中使用，具体命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry add --dev ruff</span><br></pre></td></tr></table></figure><p>如果想让 Ruff 在本地 IDE 中使用的话，建议是进行全局安装，这样就可以在任何项目中使用 Ruff，具体命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -LsSf https://astral.sh/ruff/install.sh | sh</span><br></pre></td></tr></table></figure><p>安装完成后，可以通过 IDE 的插件来使用 Ruff，比如在 VSCode 中安装 <a href="https://marketplace.visualstudio.com/items?itemName=charliermarsh.ruff">Ruff 插件</a>，然后在 VSCode 中使用快捷键 <code>Ctrl+Shift+P</code> 打开命令面板，输入 <code>user config</code> 打开用户配置文件，然后添加 Ruff 安装后的路径：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;ruff.path&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;/your/ruff/path&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Ruff-使用示例"><a href="#Ruff-使用示例" class="headerlink" title="Ruff 使用示例"></a>Ruff 使用示例</h3><p>Ruff 主要有 2 个命令，分别是 <code>ruff check</code> 和 <code>ruff format</code>，前者用于代码检查，后者用于代码格式化。</p><p>为了保证团队的代码风格一致，我们可以在 Git 的 hook 中添加 Ruff 检查命令，这样每个人在执行 <code>git commit</code> 命令时就会自动执行 Ruff 命令，如果检查失败则无法提交代码。</p><p>首先我们需要安装 <a href="https://pre-commit.com/">pre-commit</a> 工具，这个工具可以让我们在 Git hook 上轻松配置命令，同样将其安装到 dev 开发依赖即可：<code>poetry add --dev pre-commit</code>，然后在项目根目录下添加 <code>.pre-commit-config.yaml</code> 文件，内容如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">repos:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">repo:</span> <span class="string">https://github.com/astral-sh/ruff-pre-commit</span></span><br><span class="line">    <span class="comment"># Ruff 版本</span></span><br><span class="line">    <span class="attr">rev:</span> <span class="string">v0.5.1</span></span><br><span class="line">    <span class="attr">hooks:</span></span><br><span class="line">      <span class="comment"># 执行 ruff check</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">id:</span> <span class="string">ruff</span></span><br><span class="line">        <span class="comment"># 可选参数，自动修复代码</span></span><br><span class="line">        <span class="attr">args:</span> [<span class="string">--fix</span>]</span><br><span class="line">      <span class="comment"># 执行 ruff format</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">id:</span> <span class="string">ruff-format</span></span><br></pre></td></tr></table></figure><ul><li>在文件中我们添加 Ruff 工具来处理代码提交时的检查</li><li>我们添加了 2 个工具，第一个工具 id 是 <code>ruff</code> 表示执行 <code>ruff check</code> 命令，第二个工具 id 是 <code>ruff-format</code> 表示执行 <code>ruff format</code> 命令</li><li>第一个工具还有一个可选参数 <code>args: [--fix]</code>，表示会自动修复检查出有误的代码，但也不是所有代码都能自动修复，有些代码还是需要手动修复的</li></ul><p>最后我们在项目中执行 <code>pre-commit install</code> 命令，将文件内容添加到 Git hook 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ pre-commit install</span><br><span class="line">pre-commit installed at .git/hooks/pre-commit</span><br></pre></td></tr></table></figure><p>配置完成后，我们故意写一些代码错误，然后提交代码， 检查结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m <span class="string">&#x27;add some feature&#x27;</span></span><br><span class="line">ruff.....................................................................Failed</span><br><span class="line">- hook <span class="built_in">id</span>: ruff</span><br><span class="line">- <span class="built_in">exit</span> code: 1</span><br><span class="line"></span><br><span class="line">main.py:1:8: F401 [*] `requests` imported but unused</span><br><span class="line">  |</span><br><span class="line">1 | import requests</span><br><span class="line">  |        ^^^^^^^^ F401</span><br><span class="line">  |</span><br><span class="line">  = <span class="built_in">help</span>: Remove unused import: `requests`</span><br><span class="line"></span><br><span class="line">Found 1 error.</span><br><span class="line">[*] 1 fixable with the `--fix` option.</span><br><span class="line"></span><br><span class="line">ruff-format..............................................................Passed</span><br></pre></td></tr></table></figure><p>可以看到代码检查任务失败了，报了 <code>imported but unused</code> 的错误，错误编号 <code>F401</code>，第二个任务代码格式化检查通过。</p><p>如果想在代码中不检查某行代码，可以在代码行后面加上 <code># noqa: &#123;error_code&#125;</code>，比如：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests <span class="comment"># noqa: F401</span></span><br></pre></td></tr></table></figure><p>这样就可以在 Ruff 检查时忽略这个错误。更多的 Ruff 使用方法可以参<a href="https://docs.astral.sh/ruff/tutorial/">考官方文档</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天我们介绍了 Python 中新一代的项目工具，新工具带来的好处是开发效率的提升，因为每个新工具都是在解决旧工具的不足之处，是在旧工具的基础上进行了优化和改进。作为一个热衷提高生产力的开发人员，可以在适当时机尝试使用这些新工具，如果觉得新工具不合适，也可以退回去重新使用旧工具，关键在于尝试。</p><p>关注我，一起学习各种人工智能和 GenAI 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍新一代 Python 项目常用工具，以及如何在项目中使用</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="python" scheme="https://zhaozhiming.github.io/tags/python/"/>
    
    <category term="miniconda" scheme="https://zhaozhiming.github.io/tags/miniconda/"/>
    
    <category term="poetry" scheme="https://zhaozhiming.github.io/tags/poetry/"/>
    
    <category term="ruff" scheme="https://zhaozhiming.github.io/tags/ruff/"/>
    
    <category term="uv" scheme="https://zhaozhiming.github.io/tags/uv/"/>
    
  </entry>
  
  <entry>
    <title>高级 RAG 检索策略之知识图谱</title>
    <link href="https://zhaozhiming.github.io/2024/07/04/rag-knowledge-graphs/"/>
    <id>https://zhaozhiming.github.io/2024/07/04/rag-knowledge-graphs/</id>
    <published>2024-07-04T14:12:57.000Z</published>
    <updated>2024-10-03T08:49:29.888Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/07/rag-knowledge-graph.jpg" class="" width="400" height="300"><p>RAG（Retrieval Augmented Generation）技术中检索是一个非常重要的环节，检索的准确性直接影响到生成的质量，但普通 RAG 的向量检索技术并不能满足所有场景下的需求，比如在一些大型私有文档库中，传统的检索技术往往表现不好。目前已经有很多研究团队在 RAG 中引入知识图谱来提高检索的准确性，并且取得了很好的效果。今天我们就来了解一下知识图谱的原理，以及如何在 RAG 中进行使用。</p><span id="more"></span><h2 id="什么是知识图谱"><a href="#什么是知识图谱" class="headerlink" title="什么是知识图谱"></a>什么是知识图谱</h2><p>知识图谱是一种利用图结构来表示和建模现实世界中实体及其关系的技术方法。它将信息以节点（实体）和边（关系）的形式组织成一个有机的网络，从而实现对复杂知识的高效存储、查询和分析。知识图谱的核心在于通过三元组形式（实体-关系-实体）来描述事物之间的关联，这种结构化的数据表示方法不仅能够捕捉数据的语义含义，还能便于理解和分析。</p><h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><p>知识图谱在 RAG 中的使用流程图如下所示：</p><img src="/images/post/2024/07/graph-rag-flow.png" class="" width="1000" height="600"><p>在数据入库过程中，文档经过分块后，知识图谱 RAG 会将文档块进行实体和关系的提取，提取出实体和关系后，通常是将它们保存到图数据库中。</p><p>在检索过程中，知识图谱 RAG 会将问题进行实体提取，将提取出来的实体通过图数据库进行检索，获取相关的实体和关系，检索结果往往是一个庞大的实体关系网络，最后将检索到的实体和关系结合问题提交给 LLM（大语言模型）进行答案生成。</p><p>有些知识图谱 RAG 的实现也会结合图检索和向量检索两种方式，这样可以综合利用图检索和向量检索的优势，提高检索的准确性和效率。</p><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在 RAG 中使用知识图谱主要解决在大型文档库上问答和理解困难的问题，特别是那些普通 RAG 方法难以处理的全局性问题。普通 RAG 在回答针对整个文档库的全局性问题时表现不佳，例如问题：<code>请告诉我所有关于 XXX 的事情</code>，这个问题涉及到的上下文可能分布在整个大型文档库中，普通 RAG 的向量检索方法很难得到这种分散、细粒度的文档信息，向量检索经常使用 top-k 算法来获取最相近的上下文文档，这种方式很容易遗漏关联的文档块，从而导致信息检索不完整。</p><p>另外是 LLM 的上下文窗口限制问题，对于全局性问题往往涉及到非常多的上下文文档，如果要全部提交给 LLM 则很容易超出 LLM 的窗口限制，而知识图谱将文档提取成实体关系后再提交给 LLM，实际上大大压缩了文档块的大小，从而让所有相关文档提交给 LLM 成为可能。</p><h3 id="与普通-RAG-的区别"><a href="#与普通-RAG-的区别" class="headerlink" title="与普通 RAG 的区别"></a>与普通 RAG 的区别</h3><ul><li>知识图谱 RAG 使用图结构来表示和存储信息，捕捉实体间的复杂关系，而普通 RAG 通常使用向量化的文本数据</li><li>知识图谱 RAG 通过图遍历和子图检索来获取相关信息，普通 RAG 主要依赖向量相似度搜索</li><li>知识图谱 RAG 能更好地理解实体间的关系和层次结构，提供更丰富的上下文，普通 RAG 在处理复杂关系时能力有限</li></ul><h2 id="数据入库"><a href="#数据入库" class="headerlink" title="数据入库"></a>数据入库</h2><p>下面我们来看下知识图谱 RAG 具体的数据入库流程，普通 RAG 在文档分块后，通常是使用 Embedding 模型将文档块进行向量化，然后将向量和文档保存到向量数据库。与普通 RAG 不同，知识图谱 RAG 在入库过程中会将文档块进行实体和关系的提取，提取出实体和关系后再将它们保存到图数据库中。</p><img src="/images/post/2024/07/graph-rag-index-flow.png" class="" width="1000" height="600"><p>实体提取的传统方法是基于预定义的规则和词典、统计机器学习或者深度学习等技术，但进入到 LLM 时代后，实体提取更多的是使用 LLM 来进行，因为 LLM 能够更好地理解文本的语义，实现也更加简单。</p><p>比如在 <a href="https://www.llamaindex.ai/">LlamaIndex</a> 的 <code>KnowledgeGraphIndex</code> 类中的实体提取提示词是这样的：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_KG_TRIPLET_EXTRACT_TMPL = (</span><br><span class="line">    <span class="string">&quot;Some text is provided below. Given the text, extract up to &quot;</span></span><br><span class="line">    <span class="string">&quot;&#123;max_knowledge_triplets&#125; &quot;</span></span><br><span class="line">    <span class="string">&quot;knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\n&quot;</span></span><br><span class="line">    <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Example:&quot;</span></span><br><span class="line">    <span class="string">&quot;Text: Alice is Bob&#x27;s mother.&quot;</span></span><br><span class="line">    <span class="string">&quot;Triplets:\n(Alice, is mother of, Bob)\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Text: Philz is a coffee shop founded in Berkeley in 1982.\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Triplets:\n&quot;</span></span><br><span class="line">    <span class="string">&quot;(Philz, is, coffee shop)\n&quot;</span></span><br><span class="line">    <span class="string">&quot;(Philz, founded in, Berkeley)\n&quot;</span></span><br><span class="line">    <span class="string">&quot;(Philz, founded in, 1982)\n&quot;</span></span><br><span class="line">    <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Text: &#123;text&#125;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Triplets:\n&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>在提示词中要求 LLM 将文档块 <code>text</code> 提取成<code>实体-关系-实体</code>这样的三元组，实体一般是名词，表示文档块中的实体，关系是动词或者介词，表示实体之间的关系，并给出了几个 Few Shot，让 LLM 能更好地理解实体抽取的任务。</p><p>将实体提取出来后，通常是将实体和关系保存到图数据库中，但也有一些知识图谱 RAG 的实现会将这些数据保存到文件中，然后通过其特有的算法来进行检索，比如微软的 <a href="https://microsoft.github.io/graphrag/">GraphRAG</a>。</p><p>图数据库是一种专门用来存储图结构数据的数据库，它能够高效地存储和查询图数据，常见的图数据库有 Neo4j、ArangoDB 等。不同的图数据库有不同的查询语言，比如 Neo4j 的查询语言使用的是 Cypher，如果想要在 RAG 中使用 Neo4j 来存储知识图谱数据，那么掌握一些基础的 Cypher 语法是有必要的。</p><h2 id="检索生成"><a href="#检索生成" class="headerlink" title="检索生成"></a>检索生成</h2><p>了解了知识图谱 RAG 的数据入库流程之后，我们再来看下它的检索生成过程。普通 RAG 在检索过程中通常是将问题进行向量化，然后通过向量相似度搜索来获取最相近的几个文档块，然后将这些文档块提交给 LLM 进行答案生成。而知识图谱 RAG 在检索过程中会将问题进行实体提取，将提取出来的实体通过图数据库进行检索，这样可以获取到名称相同的实体，以及与实体相关的实体和关系，最后将检索到的所有实体和关系提交给 LLM 进行答案生成。</p><img src="/images/post/2024/07/graph-rag-retrieve-flow.png" class="" width="1000" height="600"><p>对问题进行实体提取与数据入库时的实体提取方法类似，也是通过 LLM 来进行，但只需要提取出问题中的实体即可，不需要提取三元组，可以看下 LlamaIndex 的 <code>KGTableRetriever</code> 类中提取问题关键字的提示词：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (</span><br><span class="line">    <span class="string">&quot;A question is provided below. Given the question, extract up to &#123;max_keywords&#125; &quot;</span></span><br><span class="line">    <span class="string">&quot;keywords from the text. Focus on extracting the keywords that we can use &quot;</span></span><br><span class="line">    <span class="string">&quot;to best lookup answers to the question. Avoid stopwords.\n&quot;</span></span><br><span class="line">    <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">    <span class="string">&quot;&#123;question&#125;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Provide keywords in the following comma-separated format: &#x27;KEYWORDS: &lt;keywords&gt;&#x27;\n&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>提示词要求 LLM 从问题中提取出多个关键字，并用逗号分隔，这些关键字通常是问题中的实体。将问题的实体提取出来后，再用实体名称去图数据库中进行检索， 检索的原理就是使用图数据库的查询语句对每个实体进行检索，获取对应的三元组。以 Neo4j 图数据库为例，下面是一个简单的 Cypher 查询语句：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MATCH (n &#123;name: &#x27;Alice&#x27;&#125;)-[r]-(m)</span><br><span class="line">RETURN n, r, m</span><br></pre></td></tr></table></figure><p>这个查询语句的意思是查找图数据库中所有与实体 Alice 相关的实体和关系，这样就可以获取到 Alice 相关的所有三元组。最后将得到的数据转换为文本，作为问题的上下文，提交给 LLM 进行答案生成。</p><h2 id="LlamaIndex-知识图谱-RAG-实现"><a href="#LlamaIndex-知识图谱-RAG-实现" class="headerlink" title="LlamaIndex 知识图谱 RAG 实现"></a>LlamaIndex 知识图谱 RAG 实现</h2><p>​了解完知识图谱 RAG 的原理后，接下来我们来看下如何在实际项目中使用知识图谱 RAG ，在 LlamaIndex 框架中已经实现了知识图谱的功能，使用 LlamaIndex 和 Neo4j 可以快速地实现知识图谱 RAG。</p><h3 id="Neo4j-安装"><a href="#Neo4j-安装" class="headerlink" title="Neo4j 安装"></a>Neo4j 安装</h3><p>Neo4j 是一个高性能的图形数据库，它将结构化数据存储在网络（从数学角度称为图）上而不是传统的表中，这种设计使得 Neo4j 在处理复杂的关系和连接时具有显著的优势。Neo4j 使用 Cypher 作为查询语言，Cypher 是一种声明式图数据库查询语言，类似于 SQL，但是专门用于图数据库。Cypher 语言的语法简单直观，易于学习和使用，可以快速编写复杂的图查询。Neo4j 除了支持图检索外，还支持其他多种检索方式，包括向量检索、全文检索等。</p><p>下面我们来看在如何安装 Neo4j 数据库，Neo4j 的安装非常简单，只需要通过 Docker 下载镜像并启动就可以了，安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run --name neo4j -d \</span><br><span class="line">    --publish=7474:7474 --publish=7687:7687 \</span><br><span class="line">    --volume=/your/host/path/neo4j-data/data:/data \</span><br><span class="line">    --<span class="built_in">env</span> NEO4J_PLUGINS=<span class="string">&#x27;[&quot;apoc&quot;]&#x27;</span> \</span><br><span class="line">    neo4j:5.21.0</span><br></pre></td></tr></table></figure><ul><li>我们使用 Neo4j 的 Docker 镜像进行安装，版本是 5.21.0</li><li>Neo4j 镜像会开放 2 个端口，端口 7474 的服务是 Web 管理服务，端口 7687 的服务是数据库服务</li><li>我们将 Neo4j 的数据目录映射到宿主机的 <code>/your/host/path/neo4j-data/data</code> 目录</li><li>我们通过环境变量给 Neo4j 安装一个插件 Apoc，保证 Python 程序可以通过账号密码连接数据库</li></ul><p>服务成功启动后，我们打开浏览器访问 <code>http://localhost:7474</code>，可以看到 Neo4j 的 Web 管理界面，如下图所示：</p><img src="/images/post/2024/07/neo4j-web.png" class="" width="1000" height="600"><p>输入初始账号密码：<code>neo4j/neo4j</code>，然后设置新密码，就可以进入到 Neo4j 的管理界面了。</p><h3 id="在-LlamaIndex-中使用-Neo4j"><a href="#在-LlamaIndex-中使用-Neo4j" class="headerlink" title="在 LlamaIndex 中使用 Neo4j"></a>在 LlamaIndex 中使用 Neo4j</h3><p>安装完 Neo4j 数据库后，我们就可以在 LlamaIndex 中使用 Neo4j 了，首先使用 <code>Neo4jGraphStore</code> 类来连接 Neo4j 数据库：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.graph_stores.neo4j <span class="keyword">import</span> Neo4jGraphStore</span><br><span class="line"></span><br><span class="line">username = <span class="string">&quot;neo4j&quot;</span></span><br><span class="line">password = <span class="string">&quot;neo4j&quot;</span></span><br><span class="line">url = <span class="string">&quot;bolt://localhost:7687&quot;</span></span><br><span class="line">database = <span class="string">&quot;neo4j&quot;</span></span><br><span class="line">graph_store = Neo4jGraphStore(</span><br><span class="line">    username=username,</span><br><span class="line">    password=password,</span><br><span class="line">    url=url,</span><br><span class="line">    database=database,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>使用 <code>Neo4jGraphStore</code> 创建连接 Neo4j 数据库的存储对象，传入用户名、密码、连接地址、数据库名称等参数</li><li><code>bolt</code> 是 Neo4j 数据库使用的一种高效的二进制协议，用于在客户端和服务器之间传输数据</li><li>Neo4j 数据库的社区版只能使用一个数据库，这里的数据库名称是固定的 <code>neo4j</code></li></ul><p>然后将文档保存到 Neo4j 数据库中，这里的测试文档还是用维基百科上的<a href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> StorageContext, SimpleDirectoryReader KnowledgeGraphIndex</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">storage_context = StorageContext.from_defaults(graph_store=graph_store)</span><br><span class="line">index = KnowledgeGraphIndex.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    storage_context=storage_context,</span><br><span class="line">    max_triplets_per_chunk=<span class="number">2</span>,</span><br><span class="line">    include_embeddings=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>使用 <code>SimpleDirectoryReader</code> 加载文档数据</li><li>使用 <code>StorageContext</code> 创建存储上下文对象，传入图数据库存储对象</li><li>使用 <code>KnowledgeGraphIndex</code> 从文档中创建知识图谱索引对象</li><li><code>max_triplets_per_chunk=2</code> 参数表示每个文档块将被最多提取成 2 个三元组</li><li><code>include_embeddings=True</code> 参数表示将提取后的三元组转成 Embedding 向量并保存</li><li><code>KnowledgeGraphIndex</code> 默认使用 OpenAI 的 LLM 模型和 Embedding 模型 来进行实体提取和 Embedding，因此需要在环境变量中设置 OpenAI 的 API Key</li></ul><p>文档经过分块、实体提取、Embedding 等操作后，最后将实体和关系保存到 Neo4j 数据库中。数据入库完成后，我们可以在 Neo4j 数据库中查看所有的实体和关系，如下图所示：</p><img src="/images/post/2024/07/neo4j-dataset.png" class="" width="1000" height="600"><p>最后我们构建查询引擎，并对问题进行检索生成：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">query_engine = index.as_query_engine(</span><br><span class="line">    include_text=<span class="literal">True</span>,</span><br><span class="line">    response_mode=<span class="string">&quot;tree_summarize&quot;</span>,</span><br><span class="line">    embedding_mode=<span class="string">&quot;hybrid&quot;</span>,</span><br><span class="line">    similarity_top_k=<span class="number">5</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">response = query_engine.query(<span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Response: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>使用 <code>index.as_query_engine</code> 创建查询引擎对象</li><li><code>response_mode=&quot;tree_summarize&quot;</code> 参数表示最终结果使用树形总结的方式来生成</li><li><code>embedding_mode=&quot;hybrid&quot;</code> 参数表示使用图检索和向量检索的混合模式</li><li><code>similarity_top_k=5</code> 参数表示最多返回 5 个相似的文档块，<code>verbose=True</code> 参数表示检索过程中打印详细信息</li><li>使用查询引擎进行问题的检索和生成，最后打印出生成答案</li></ul><p>我们再来看下程序运行后的结果，因为我们开启了调试模式，所以在检索过程中会打印出详细的检索信息，如下所示：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Extracted keywords: [<span class="string">&#x27;Avengers&#x27;</span>, <span class="string">&#x27;Ultron&#x27;</span>, <span class="string">&#x27;created&#x27;</span>, <span class="string">&#x27;members&#x27;</span>]</span><br><span class="line">KG context:</span><br><span class="line">The following are knowledge sequence <span class="keyword">in</span> max depth 2 <span class="keyword">in</span> the form of directed graph like:</span><br><span class="line">`subject -[predicate]-&gt;, object, &lt;-[predicate_next_hop]-, object_next_hop ...`</span><br><span class="line">[<span class="string">&#x27;CAPTURES&#x27;</span>, <span class="string">&#x27;Romanoff&#x27;</span>, <span class="string">&#x27;USES&#x27;</span>, <span class="string">&quot;Loki&#x27;s scepter to close&quot;</span>]</span><br><span class="line">[<span class="string">&#x27;BATTLE&#x27;</span>, <span class="string">&#x27;Chitauri&#x27;</span>, <span class="string">&#x27;KNOWN_AS&#x27;</span>, <span class="string">&#x27;Extraterrestrial race&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;CAPTURES&#x27;</span>, <span class="string">&#x27;Romanoff&#x27;</span>, <span class="string">&#x27;MAKES_HER_WAY_TO&#x27;</span>, <span class="string">&#x27;Generator&#x27;</span>]</span><br><span class="line">......</span><br><span class="line">Response: Tony Stark and Bruce Banner.</span><br></pre></td></tr></table></figure><ul><li>首先提取问题中的关键词，这里提取出了<code>Avengers</code>, <code>Ultron</code>, <code>created</code>, <code>members</code>这几个关键词</li><li>然后打印出根据关键词检索到的实体和关系三元组</li><li>最后根据问题和这些上下文生成了答案</li></ul><h2 id="GraphRAG"><a href="#GraphRAG" class="headerlink" title="GraphRAG"></a>GraphRAG</h2><p>看完 LlamaIndex 的知识图谱 RAG 实现后，我们再来看下另外一个知识图谱 RAG 的实现。最近微软开源了一个知识图谱 RAG 的实现叫 <a href="https://microsoft.github.io/graphrag/">GraphRAG</a>，它是一个基于知识图谱的 RAG 应用，可以用于问答、文本生成等任务。GraphRAG 是在微软之前发布的<a href="https://arxiv.org/pdf/2404.16130">论文</a> 理论基础上进行开发，与普通知识图谱 RAG 不同的地方是，它并没有用到图数据库，而是直接将知识图谱保存到文件中，然后通过其特有的图检索算法进行检索。另外 GraphRAG 还利用知识图谱的模块化特性，将知识图谱划分为多个语义相关的社区，并为每个社区生成概括性的摘要。 在回答用户查询时，GraphRAG 会根据查询内容检索相关的社区摘要，并利用这些摘要生成最终答案。</p><p>下面我们介绍一下 GraphRAG 的安装和使用方法，让大家可以快速地了解 GraphRAG。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>关于 GraphRAG 的安装方法，这里推荐使用源码安装的方式来安装 GraphRAG，因为这样可以让我们在使用过程中通过修改源码的方式来调试 GraphRAG，从而更好地理解 GraphRAG 的原理。</p><p>首先我们需要下载 GraphRAG 的源码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/microsoft/graphrag.git</span><br><span class="line"><span class="built_in">cd</span> graphrag</span><br></pre></td></tr></table></figure><p>然后我们需要使用 <a href="https://python-poetry.org/">Poetry</a> 来安装 GraphRAG 的依赖，Poetry 的安装可以参考其官网的<a href="https://python-poetry.org/docs/#installation">安装手册</a>，GraphRAG 安装依赖的命令如下：</p><blockquote><p>Poetry 是一个用于 Python 项目依赖管理和打包的工具，Poetry 使用一个 <code>pyproject.toml</code> 文件来管理项目的所有依赖项和元数据，使项目配置更加简洁明了，它会自动处理依赖项的版本冲突，并且能够生成锁文件 <code>poetry.lock</code>，确保在不同环境中安装相同的依赖版本。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 conda 创建一个 Python 环境</span></span><br><span class="line">conda create -n graphrag python=3.10</span><br><span class="line"><span class="comment"># 切换到这个 Python 环境</span></span><br><span class="line">conda activate graphrag</span><br><span class="line"><span class="comment"># 构建一个 Poetry 虚拟环境</span></span><br><span class="line">poetry <span class="built_in">env</span> use python</span><br><span class="line"><span class="comment"># 进入该环境</span></span><br><span class="line">poetry shell</span><br><span class="line"><span class="comment"># 安装依赖，会根据 GraphRAG 的 poetry.lock 文件安装依赖</span></span><br><span class="line">poetry install</span><br></pre></td></tr></table></figure><h3 id="初始化配置"><a href="#初始化配置" class="headerlink" title="初始化配置"></a>初始化配置</h3><p>GraphRAG 安装完成后，我们再来准备测试文档，创建一个测试文件夹，用来存放我们的测试文档：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p ./ragtest/input</span><br><span class="line"><span class="comment"># GraphRAG 官方文档是下载这个文件作为测试文档，我们也可以放其他的 txt 文档</span></span><br><span class="line">curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt &gt; ./ragtest/input/book.txt</span><br></pre></td></tr></table></figure><p>然后创建配置文件，我们使用 GraphRAG 的初始化命令来生成配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry run poe index --init --root ./ragtest</span><br></pre></td></tr></table></figure><p>初始化完成后，我们可以看到在 <code>./ragtest</code> 目录下生成了 <code>settings.yaml</code> 和 <code>.env</code> 2 个文件，在 <code>.env</code> 文件中通过 <code>GRAPHRAG_API_KEY</code> 键来设置 OpenAI 的 API Key，而<code>settings.yaml</code> 文件用来保存 GraphRAG 的流水线配置信息。</p><h3 id="入库流程"><a href="#入库流程" class="headerlink" title="入库流程"></a>入库流程</h3><p>初始化配置完成后，我们使用 GraphRAG 执行数据入库流水线，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry run poe index --root ./ragtest</span><br></pre></td></tr></table></figure><p>这个执行过程比较耗时，因为 GraphRAG 会将文档进行一系列的操作，包括文档分块、实体提取、文本 Embedding、生成社区报告等，以下是 GraphRAG 的索引入库流程图：</p><img src="/images/post/2024/07/graphrag-index-flow.png" class="" width="1000" height="600"><p>执行完成后，我们可以在 <code>./ragtest/output/&#123;timestamp&#125;/artifacts</code> 目录下可以看到生成的索引文件，默认是 <code>parquet</code> 格式，后面的检索流程会从这里读取数据。</p><p>想了解更多关于 GraphRAG 数据入库的信息，可以参考其<a href="https://microsoft.github.io/graphrag/posts/index/1-default_dataflow/">官方文档</a>。</p><h3 id="检索流程"><a href="#检索流程" class="headerlink" title="检索流程"></a>检索流程</h3><p>文档入库完成后，我们就可以使用 GraphRAG 进行检索生成了，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poetry run poe query --root ./ragtest --method <span class="built_in">local</span> <span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span></span><br></pre></td></tr></table></figure><p>GraphRAG 的检索模式有 2 种，分别是本地模式和全局模式，上面的命令使用 <code>--method</code> 参数来指定哪种模式。</p><p>本地模式类似传统的知识图谱 RAG，通过结合来自知识图谱的相关数据和原始文档的文本块生成答案，而全局模式是通过在所有社区报告上进行搜索，以类似 map-reduce 的方式生成答案。以下是 GraphRAG 本地检索的流程图：</p><img src="/images/post/2024/07/graphrag-query-local-flow.png" class="" width="1000" height="600"><p>经过测试，GraphRAG 在检索质量上要比普通 RAG 更好，文档数据越多优势越明显，但是 GraphRAG 的检索速度要比普通 RAG 慢，因为 GraphRAG 需要对文档进行多种方式检索。</p><p>关于 GraphRAG 的检索流程的更多信息，可以参考其<a href="https://microsoft.github.io/graphrag/posts/query/overview/">官方文档</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>知识图谱 RAG 是一种基于知识图谱的 RAG 技术，它通过图结构来表示和存储信息，提取其中的实体和关系，然后通过图检索的方式来进行检索生成。对于一些大型的私有文档库和复杂的全局性问题，知识图谱 RAG 无疑是一个很好的选择，但在使用过程中也要综合考虑，知识图谱的增强往往也会导致检索生成的速度变慢以及耗费的资源增加，因此在实际应用中需要根据具体的场景来选择合适的 RAG 技术。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍如何使用知识图谱来改进 RAG 的检索策略</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="knownledge-graph" scheme="https://zhaozhiming.github.io/tags/knownledge-graph/"/>
    
    <category term="graphrag" scheme="https://zhaozhiming.github.io/tags/graphrag/"/>
    
    <category term="neo4j" scheme="https://zhaozhiming.github.io/tags/neo4j/"/>
    
  </entry>
  
  <entry>
    <title>高级 RAG 检索策略之 Self-RAG</title>
    <link href="https://zhaozhiming.github.io/2024/06/30/rag-selfrag/"/>
    <id>https://zhaozhiming.github.io/2024/06/30/rag-selfrag/</id>
    <published>2024-06-30T08:22:07.000Z</published>
    <updated>2024-10-03T08:49:29.888Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/07/rag-selfrag.jpg" class="" width="400" height="300"><p>Self-RAG 是另外一种形式的 RAG（Retrieval Augmented Generation），它与其他 RAG 检索策略不同，它并不是在 RAG 流程上对某个模块进行增强， 而是在 RAG 不同的模块上进行优化改进，从而达到改进整体 RAG 流程的目的。如果你对 Self-RAG 比较陌生或者只是听说它的名字，那么今天请跟我一起来了解 Self-RAG 的实现原理，以及通过学习 Self-RAG 的代码实现来更好地理解其中的细节。</p><span id="more"></span><h2 id="Self-RAG-整体概览"><a href="#Self-RAG-整体概览" class="headerlink" title="Self-RAG 整体概览"></a>Self-RAG 整体概览</h2><p><a href="https://selfrag.github.io/">Self-RAG</a> 是一种新的 RAG 方式，通过训练后的 LLM（大语言模型）进行检索、生成和评估等任务，以提高生成结果的准确性和质量。研究团队经过实验证明，Self-RAG 在开放领域问答、推理和事实验证任务中表现优异，效果要比商业模型 ChatGPT 好，同时也比开源模型 Llama2 使用普通 RAG 的效果更好，Self-RAG 的论文地址可以看<a href="https://arxiv.org/abs/2310.11511">这里</a>。</p><p>Self-RAG 旨在解决普通 RAG 的几个问题：</p><ul><li>过度检索：普通 RAG 对输入问题始终进行相关知识检索，可能引入无用甚至偏离主题的内容，从而影响输出结果</li><li>输出不准确：因为不能保证 LLM 始终是基于检索知识来回答问题，所以输出结果可能与检索到的知识不一致</li></ul><p>在后面的介绍中我们会慢慢了解为什么 Self-RAG 能解决这些问题。</p><h3 id="Self-RAG-流程"><a href="#Self-RAG-流程" class="headerlink" title="Self-RAG 流程"></a>Self-RAG 流程</h3><p>为了更好地理解 Self-RAG，我们来看下它与普通 RAG 的区别，首先我们来看下普通 RAG 的流程图：</p><img src="/images/post/2024/07/base-rag-flow.png" class="" width="1000" height="600"><ul><li>这个流程图省略了文档入库的部分，主要展示检索和生成的过程</li><li>每一次查询，普通 RAG 都会去检索相关文档，然后将<strong>所有的文档</strong>和问题一起输入到 LLM 中，然后生成结果</li><li>生成结果的提示词需要添加所有的检索文档作为问题的上下文，一般会要求 LLM 优先根据上下文知识进行回答</li></ul><p>我们再来看下 Self-RAG 的流程图：</p><img src="/images/post/2024/07/selfrag-flow.png" class="" width="1000" height="600"><ul><li>Self-RAG 首先会使用 LLM 对问题进行首次结果生成（这里的 LLM 是经过特殊训练后的 LLM，后面会详细介绍），然后根据生成的结果可以直接判断是否需要检索，如果不需要检索，则直接返回结果</li><li>如果需要检索，Self-RAG 会检索出相关文档，然后将<strong>每个文档</strong>和问题一起输入到 LLM 中，获取每个文档的生成结果，生成结果的提示词只用到了单个文档来作为问题的上下文</li><li>然后对每个文档的生成结果进行评估，评选出得分最高的结果，最终返回这个结果</li></ul><h3 id="Self-RAG-与普通-RAG-的区别"><a href="#Self-RAG-与普通-RAG-的区别" class="headerlink" title="Self-RAG 与普通 RAG 的区别"></a>Self-RAG 与普通 RAG 的区别</h3><p>从以上 2 个流程图可以看出，Self-RAG 与普通 RAG 的区别主要有以下几点：</p><ul><li>普通 RAG 每次查询都需要检索，而 Self-RAG 可以根据生成的结果判断是否需要检索</li><li>普通 RAG 将检索到的所有文档作为上下文，而 Self-RAG 只用到了单个文档作为上下文，但需要对每个文档进行结果生成</li><li>Self-RAG 对生成结果有一个评估和挑选的过程，而普通 RAG 没有这个过程，因为普通 RAG 的生成结果只有一个</li><li>普通 RAG 使用的是通用 LLM，而 Self-RAG 使用的是经过特殊训练后的 LLM</li></ul><h2 id="按需检索"><a href="#按需检索" class="headerlink" title="按需检索"></a>按需检索</h2><p>了解了 Self-RAG 的整体流程后，我们再来了解一下 Self-RAG 每个阶段的实现原理，首先是首次查询阶段。</p><p>与普通的 RAG 不同，Self-RAG 使用的是经过训练的 LLM，这种 LLM 在文本生成的过程中，会输出一些特殊的 Token，这些 Token 叫 <strong>Reflection Token</strong>。在 RAG 流程中，Self-RAG 会使用 Reflection Token 来进行不同的操作。</p><p>Self-RAG 开始使用 LLM 对问题进行生成时，会输出 <code>Retrieve</code> 类型的 Reflection Token，这种类型的 Token 的值有 3 种，分别是：</p><ul><li>Retrieval：表示需要检索</li><li>No Retrieval：表示不需要检索</li><li>Continue to Use Evidence: 表示模型可以继续使用先前检索到的证据</li></ul><p>我们来通过几个例子来看一下 Self-RAG 生成的 Reflection Token 是怎样的，首先我们提问一个不需要检索的问题，问题和输出结果示例如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Question: Write a essay of your best summer vacation.</span><br><span class="line">Answer: Sure![No Retrieval]As an AI, I don<span class="string">&#x27;t have personal experiences or memories, but I can write about the importance and significance of summer vacations for individuals and families.[No Retrieval]......</span></span><br></pre></td></tr></table></figure><p>在返回结果中我们可以看到包含了<code>[No Retrieval]</code>关键字，表示这个问题不需要检索，可以直接返回结果。</p><p>我们再问一个需要检索的问题，问题和输出结果示例如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Question: How did US statues get their names?</span><br><span class="line">Answer: A number of them.[Retrieval]&lt;paragraph&gt;[Irrelevant]Some were named <span class="keyword">for</span> the people <span class="built_in">who</span> originally sponsored them, some were named <span class="keyword">for</span> events or issues that they represented, and some were named <span class="keyword">for</span> mythological figures.[Utility:4]</span><br></pre></td></tr></table></figure><p>在返回结果中我们可以看到包含了<code>[Retrieval]&lt;paragraph&gt;</code> 关键字，表示这个问题需要补充外部知识，即需要检索。</p><p>当然，普通 RAG 也可以通过其他方式来实现按需检索，比如首先通过 LLM 确定查询问题是否需要检索，返回一个<code>yes/no</code>的结果，如果不需要检索，则再用 LLM 生成最终结果。这意味着，如果查询问题不需要检索，普通 RAG 需要调用 2 次 LLM，而 Self-RAG 只需要调用 1 次 LLM。</p><img src="/images/post/2024/07/need-require-different.png" class="" width="1000" height="600"><p>可以看到在不需要检索这个分支上，Self-RAG 比普通 RAG 的效率更高。</p><h2 id="检索与生成"><a href="#检索与生成" class="headerlink" title="检索与生成"></a>检索与生成</h2><p>当 Self-RAG 经过首次查询后，发现需要检索，那么就会使用检索器根据问题检索文档，在检索方面，Self-RAG 和普通 RAG 并没有什么不同，都是通过向量相似性来检索文档。当检索完文档后，会将每个文档和问题一起输入到 LLM 中，获取每个文档的生成结果。</p><p>在第二次的生成结果中，Self-RAG 的 LLM 会生成 3 种类型的 Reflection Token，分别是：</p><ul><li>IsREL：检查检索到的文档是否为问题提供了有用的信息，它的值有<code>[Relevant]</code>和<code>[Irrelevant]</code>，表示文档的相关性</li><li>IsSUP：检查检索到的文档是否都为生成的答案提供了支持，它的值有<code>[Fully supported]</code>，<code>[Partially supported]</code>，<code>[No support / Contradictory]</code>，表示支持的程度</li><li>IsUSE：表示生成的答案是否对问题有帮助，它的值有<code>[Utility:5]</code>、<code>[Utility:4]</code>、<code>[Utility:3]</code>、<code>[Utility:2]</code>、<code>[Utility:1]</code>，表示答案的质量，数字越大表示质量越高</li></ul><img src="/images/post/2024/07/selfrag-second-generate.png" class="" width="400" height="600"><p>我们来看下 Self-RAG 在这个阶段的生成结果，问题和输出结果示例如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Question: What mysterious object did Loki use <span class="keyword">in</span> his attempt to conquer Earth?</span><br><span class="line">Answer1: [Relevant]The mysterious object that Loki used <span class="keyword">in</span> his attempt to conquer Earth was the Tesseract, <span class="built_in">which</span> is a powerful energy <span class="built_in">source</span> of unknown potential.[Fully supported][Utility:5]</span><br><span class="line">Answer2: [Relevant]Thanos used the Time Stone to dodge Loki<span class="string">&#x27;s attack.[No support / Contradictory][Utility:5]</span></span><br></pre></td></tr></table></figure><p>可以看到，在每个生成结果中，基本上都包含了以上 3 种 Reflection Token，这些 Token 会在后面的评估阶段进行使用。</p><h3 id="优化生成效率"><a href="#优化生成效率" class="headerlink" title="优化生成效率"></a>优化生成效率</h3><p>在这个阶段因为需要使用 LLM 对每个文档生成结果，可能会有人觉得这个阶段的效率会比普通 RAG 低，但实际上可以使用并发的方式来进行结果生成，从而提高效率。</p><img src="/images/post/2024/07/selfrag-concurrent-generate.png" class="" width="500" height="600"><p>上图中 Self-RAG 虽然需要调用 3 次 LLM，但通过并发的方式，执行时间和调用 1 次 LLM 的时间是一样的，所以在这个阶段 Self-RAG 的效率和普通 RAG 基本一致，虽然 Self-RAG 耗费的 token 会多一些。</p><h2 id="评估与选择"><a href="#评估与选择" class="headerlink" title="评估与选择"></a>评估与选择</h2><p>当生成了每个文档的结果后，Self-RAG 会对每个文档的结果进行评估，评估的方式是通过上个阶段生成的 Reflection Token 来计算每个文档的得分，然后选择得分最高的文档作为最终结果。</p><h3 id="评估参数-logprobs"><a href="#评估参数-logprobs" class="headerlink" title="评估参数 logprobs"></a>评估参数 logprobs</h3><p>在了解评估的方式之前，我们先来了解一下 Self-RAG 评估的一个重要参数<code>logprobs</code>，这个参数指的是对每个生成的 token（即单词或子词）的概率对数，这个参数是在生成结果时 LLM 输出的，通过这个参数可以计算每个 token 的得分。</p><img src="/images/post/2024/07/general-workflow-for-prompt.png" class="" width="1000" height="600"><p>我们来看下 OpenAI API 返回的一个结果示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cmpl-6yE4TGqItUpYJ6xYcIzY6&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text_completion&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;created&quot;</span><span class="punctuation">:</span> <span class="number">1623073722</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;davinci&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;choices&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot; I&#x27;m good, thanks!&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;logprobs&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;tokens&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot; I&#x27;m&quot;</span><span class="punctuation">,</span> <span class="string">&quot; good&quot;</span><span class="punctuation">,</span> <span class="string">&quot;,&quot;</span><span class="punctuation">,</span> <span class="string">&quot; thanks&quot;</span><span class="punctuation">,</span> <span class="string">&quot;!&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;token_logprobs&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">-0.1</span><span class="punctuation">,</span> <span class="number">-0.05</span><span class="punctuation">,</span> <span class="number">-0.2</span><span class="punctuation">,</span> <span class="number">-0.3</span><span class="punctuation">,</span> <span class="number">-0.15</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;top_logprobs&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot; I&#x27;m&quot;</span><span class="punctuation">:</span> <span class="number">-0.1</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot; I am&quot;</span><span class="punctuation">:</span> <span class="number">-2.3</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot; I&quot;</span><span class="punctuation">:</span> <span class="number">-3.1</span></span><br><span class="line">          <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot; good&quot;</span><span class="punctuation">:</span> <span class="number">-0.05</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot; fine&quot;</span><span class="punctuation">:</span> <span class="number">-1.5</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot; great&quot;</span><span class="punctuation">:</span> <span class="number">-2.0</span></span><br><span class="line">          <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;,&quot;</span><span class="punctuation">:</span> <span class="number">-0.2</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;.&quot;</span><span class="punctuation">:</span> <span class="number">-2.5</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;!&quot;</span><span class="punctuation">:</span> <span class="number">-3.0</span></span><br><span class="line">          <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot; thanks&quot;</span><span class="punctuation">:</span> <span class="number">-0.3</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot; thank you&quot;</span><span class="punctuation">:</span> <span class="number">-1.8</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot; thank&quot;</span><span class="punctuation">:</span> <span class="number">-2.6</span></span><br><span class="line">          <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;!&quot;</span><span class="punctuation">:</span> <span class="number">-0.15</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;.&quot;</span><span class="punctuation">:</span> <span class="number">-1.9</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;?&quot;</span><span class="punctuation">:</span> <span class="number">-2.7</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;text_offset&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">5</span><span class="punctuation">,</span> <span class="number">9</span><span class="punctuation">,</span> <span class="number">14</span><span class="punctuation">,</span> <span class="number">15</span><span class="punctuation">,</span> <span class="number">21</span><span class="punctuation">]</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;finish_reason&quot;</span><span class="punctuation">:</span> <span class="string">&quot;length&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;usage&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;prompt_tokens&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;completion_tokens&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;total_tokens&quot;</span><span class="punctuation">:</span> <span class="number">10</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>在这个示例中，logprobs 参数的输出如下：</p><ul><li>tokens：生成的 token 列表 [“ I’m”, “ good”, “,”, “ thanks”, “!”]</li><li>token_logprobs：每个生成的 token 的对数概率值 [-0.1, -0.05, -0.2, -0.3, -0.15]</li><li>top_logprobs：每个生成的 token 的前几名候选 token 的对数概率值及其对应的 token，例如第一个 token<code> I’m</code>的前几名候选 token 及其对数概率值为<code>&#123;&quot; I’m&quot;: -0.1, &quot; I am&quot;: -2.3, &quot; I&quot;: -3.1&#125;</code></li><li>text_offset：每个 token 在生成文本中的偏移量 [5, 9, 14, 15, 21]</li></ul><p>在 Self-RAG 中，评估功能使用<code>logprobs</code>参数来计算<code>IsREL</code>、<code>IsSUP</code>和<code>IsUSE</code>这 3 种 Reflection Token 的得分。比如输出中出现了<code>[Fully supported]</code>这个 token，那么说明 LLM 推理的时候计算出了<code>[Fully supported]</code>、<code>[Partially supported]</code>等可能的 token 输出的概率，但最后选择了<code>[Fully supported]</code>。因此，在评估这次输出的 <code>IsSUP</code> 的分数时，就可以基于 logprobs 中这些 tokens 的概率来计算。</p><h3 id="评估公式"><a href="#评估公式" class="headerlink" title="评估公式"></a>评估公式</h3><p>了解了<code>logprobs</code>参数后，我们再来看下 Self-RAG 3 个评估指标的计算公式，首先是<code>IsREL</code>的计算公式：</p><blockquote><p>s(ISREL) &#x3D; p(ISREL &#x3D; RELEVANT) &#x2F; (p(ISREL &#x3D; RELEVANT) + p(ISREL &#x3D; IRRELEVANT))</p></blockquote><ul><li>p(ISREL &#x3D; RELEVANT) 代表模型预测 ISREL 为 Relevant 的概率</li><li>p(ISREL &#x3D; IRRELEVANT) 代表模型预测 ISREL 为 Irrelevant 的概率</li></ul><p>然后是<code>IsSUP</code>的计算公式：</p><blockquote><p>s(ISSUP) &#x3D; p(ISSUP &#x3D; FULLY) &#x2F; S + 0.5 * p(ISSUP &#x3D; PARTIALLY) &#x2F; S</p></blockquote><ul><li>p(ISSUP &#x3D; FULLY) 代表模型预测 ISSUP 为 Fully Supported 的概率</li><li>p(ISSUP &#x3D; PARTIALLY) 代表模型预测 ISSUP 为 Partially Supported 的概率</li><li>S 代表三种可能值的概率之和： S &#x3D; ∑t∈{FULLY,PARTIALLY,NO} p(ISSUP &#x3D; t)</li></ul><p>最后是<code>IsUSE</code>的计算公式：</p><blockquote><p>s(ISUSE) &#x3D; (∑i wi * p(ISUSE &#x3D; i)) &#x2F; S</p></blockquote><ul><li>wi 代表每个等级的权重，分别为：{-1, -0.5, 0, 0.5, 1}，对应 ISUSE&#x3D;{1, 2, 3, 4, 5}</li><li>p(ISUSE &#x3D; i) 代表模型预测 ISUSE 为等级 i 的概率</li><li>S 代表五种等级的概率之和：S &#x3D; ∑t∈{1,2,3,4,5} p(ISUSE &#x3D; t)</li></ul><p>可以看到这些计算公式都是基于 logprobs 参数来计算的，这样可以更好地评估每个文档的生成结果，从而选择得分最高的文档作为最终结果。</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>Self-RAG 需要训练的模型有 2 种，一种是评估模型（Critic），另一种是生成模型（Generator）。评估模型使用 GPT4 生成的数据作为训练语料，生成模型则是使用检索数据和评估模型的生成数据作为训练语料，两者都可以基于基础大模型进行训练。</p><p>Self-RAG 研究团队分别基于 Llama2-7b 和 Llama2-13b 这 2 个模型进行训练，训练好的模型可以在 <a href="https://huggingface.co/selfrag">Huggingface</a> 上进行下载，但官方的模型并没有明确区分是评估模型还是生成模型，也就是说这些模型可以同时用于评估和生成。</p><p>我们来看下模型训练的数据，首先是评估模型的训练数据，下面是训练数据的截取片段：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;task&quot;</span><span class="punctuation">:</span> <span class="string">&quot;retrieval&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;When provided with instruction, please evaluate whether seeking additional information from external sources such as the web (e.g., Wikipedia) aids in producing a more comprehensive response. Respond with either [Retrieval] or [No Retrieval].&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Task instruction: Appraise the following website design. https://example.com\n&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[Retrieval]&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>评估模型的每项训练数据都是一项任务，任务类型分别有<code>retrieval</code>、<code>groundness</code>、<code>utility</code> 和 <code>multi_retrieval</code>这 4 种任务</li><li><code>retrieval</code>任务用来训练生成<code>IsREL</code>评估指标</li><li><code>groudness</code> 任务用来训练生成<code>IsSUP</code>评估指标</li><li><code>utility</code>任务用来训练生成<code>IsUSE</code>评估指标</li><li><code>multi_retrieval</code>任务用来训练生成<code>Retrieve</code>类型的 Reflection Token，即是否需要检索</li></ul><p>通过训练数据可以看出，评估模型主要训练如何评估这 4 类任务，根据指令和输入数据，输出正确的评估结果。</p><p>再来看生成模型的训练数据，下面是训练数据的截取片段：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Question: Write a text based on \&quot;rangers show some iron to tie the series\&quot;\nText: along with being talented , self-assured and highly paid , these experienced rangers are also considerate .\n\nQuestion: Write a text based on \&quot;union wo n&#x27;t dismantle blockage of gm canada headquarters\&quot;\nText: canadian auto workers officials friday refused to end a blockade of general motors canada headquarters despite an offer to potentially bring new car production to a complex where a truck plant is slated for closure .\n\nQuestion: Write a text based on \&quot;six azerbaijan opposition parliamentary candidates declare hunger strike\&quot;\nText:&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[No Retrieval]six opposition candidates in the upcoming parliamentary elections in azerbaijan declared a hunger strike friday to pressure the government into ensuring fair polls .[Utility:5]&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;flan_v2_18667&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;dataset_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;flan_v2&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>通过训练数据可以看出，生成模型主要训练如何生成带有 Reflection Token 的综合性结果。</p><h2 id="LlamaIndex-Self-RAG-Pack"><a href="#LlamaIndex-Self-RAG-Pack" class="headerlink" title="LlamaIndex Self-RAG Pack"></a>LlamaIndex Self-RAG Pack</h2><p>目前已经有了一些开源的 Self-RAG 实现，比如在 <a href="https://www.llamaindex.ai/">LlamaIndex</a> 的 <a href="https://docs.llamaindex.ai/en/stable/community/llama_packs/">Llama Packs</a> 上就有人提供了 <a href="https://llamahub.ai/l/llama-packs/llama-index-packs-self-rag?from=">Self-RAG 的实现</a>，下面我们来看下如何使用 LlamaIndex 的 Self-RAG Pack。</p><blockquote><p>Llama Packs 是一个社区驱动的预包装模块或模板集合，用于快速开始构建基于 LLM 的应用程序，如果把 LlamaIndex 比作 VsCode 的话，那么 Llama Packs 就是 VsCode 的插件。</p></blockquote><h3 id="使用介绍"><a href="#使用介绍" class="headerlink" title="使用介绍"></a>使用介绍</h3><p>首先我们需要下载 Self-RAG 模型， 这里我们下载量化版的 Self-RAG 模型，这样我们可以在没有 GPU 的机器上运行模型，下载命令如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -q huggingface-hub</span><br><span class="line">huggingface-cli download m4r1/selfrag_llama2_7b-GGUF selfrag_llama2_7b.q4_k_m.gguf --local-dir <span class="string">&quot;&lt;DIR_PATH&gt;&quot;</span> --local-dir-use-symlinks False</span><br></pre></td></tr></table></figure><p>然后下载 LlamaIndex 的 Self-RAG Pack，下载命令如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llamaindex-cli download-llamapack SelfRAGPack --download-dir ./self_rag_pack</span><br></pre></td></tr></table></figure><p>下载完成后，我们可以看到下载的文件夹中包含了 Self-RAG Pack 的源码，我们可以通过以下代码来调用 Self-RAG Pack：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> self_rag_pack.llama_index.packs.self_rag.base <span class="keyword">import</span> SelfRAGQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents)</span><br><span class="line">retriever = index.as_retriever(similarity_top_k=<span class="number">10</span>)</span><br><span class="line">model_path = <span class="string">&quot;/your/selfrag-model-path/selfrag_llama2_7b.q4_k_m.gguf&quot;</span></span><br><span class="line">query_engine = SelfRAGQueryEngine(<span class="built_in">str</span>(model_path), retriever, verbose=<span class="literal">True</span>)</span><br><span class="line">response = query_engine.query(<span class="string">&quot;Who won best Director in the 1972 Academy Awards?&quot;</span>)</span><br></pre></td></tr></table></figure><p><code>SelfRAGQueryEngine</code> 初始化需要模型路径和检索器，检索器可以使用普通 RAG 的检索器 <code>VectorStoreIndex</code>。</p><h3 id="核心代码解读"><a href="#核心代码解读" class="headerlink" title="核心代码解读"></a>核心代码解读</h3><p>Self-RAG Pack 的核心代码在 <code>SelfRAGQueryEngine</code> 类中，这个类包含了 Self-RAG 的整个流程，首先我们看下 <code>custom_query</code> 方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">custom_query</span>(<span class="params">self, query_str: <span class="built_in">str</span></span>) -&gt; Response:</span><br><span class="line">    response = <span class="variable language_">self</span>.llm(prompt=_format_prompt(query_str), **_GENERATE_KWARGS)</span><br><span class="line">    answer = response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;text&quot;</span>]</span><br><span class="line">    source_nodes = []</span><br><span class="line"></span><br><span class="line">    ...... <span class="comment"># 省略检索部分代码</span></span><br><span class="line"></span><br><span class="line">    answer = _postprocess_answer(answer)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.verbose:</span><br><span class="line">        print_text(<span class="string">f&quot;Final answer: <span class="subst">&#123;answer&#125;</span>\n&quot;</span>, color=<span class="string">&quot;green&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> Response(response=<span class="built_in">str</span>(answer), source_nodes=source_nodes)</span><br></pre></td></tr></table></figure><ul><li>通过 LLM 进行首次查询，获取生成结果</li><li>对生成结果进行判断，如果需要检索，则调用检索器进行检索，这部分代码后面介绍</li><li>如果无需检索，则对结果进行后处理，然后返回结果，这里的后处理主要是去掉 Reflection Token</li></ul><p>再来看<code>custom_query</code>方法中的检索部分代码：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">custom_query</span>(<span class="params">self, query_str: <span class="built_in">str</span></span>) -&gt; Response:</span><br><span class="line">    ...... <span class="comment"># 省略已展示过的代码</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;[Retrieval]&quot;</span> <span class="keyword">in</span> answer:</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.verbose:</span><br><span class="line">            print_text(<span class="string">&quot;Retrieval required\n&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">        documents = <span class="variable language_">self</span>.retriever.retrieve(query_str)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.verbose:</span><br><span class="line">            print_text(<span class="string">f&quot;Received: <span class="subst">&#123;<span class="built_in">len</span>(documents)&#125;</span> documents\n&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">        paragraphs = [</span><br><span class="line">            _format_prompt(query_str, document.node.text) <span class="keyword">for</span> document <span class="keyword">in</span> documents</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.verbose:</span><br><span class="line">            print_text(<span class="string">&quot;Start evaluation\n&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line"></span><br><span class="line">        critic_output = <span class="variable language_">self</span>._run_critic(paragraphs)</span><br><span class="line"></span><br><span class="line">        paragraphs_final_score = critic_output.paragraphs_final_score</span><br><span class="line">        llm_response_per_paragraph = critic_output.llm_response_per_paragraph</span><br><span class="line">        source_nodes = critic_output.source_nodes</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.verbose:</span><br><span class="line">            print_text(<span class="string">&quot;End evaluation\n&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line"></span><br><span class="line">        best_paragraph_id = <span class="built_in">max</span>(</span><br><span class="line">            paragraphs_final_score, key=paragraphs_final_score.get</span><br><span class="line">        )</span><br><span class="line">        answer = llm_response_per_paragraph[best_paragraph_id]</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.verbose:</span><br><span class="line">            print_text(<span class="string">f&quot;Selected the best answer: <span class="subst">&#123;answer&#125;</span>\n&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">    ...... <span class="comment"># 省略已展示过的代码</span></span><br></pre></td></tr></table></figure><ul><li>通过判断生成结果中是否包含<code>[Retrieval]</code>来判断是否需要检索</li><li>如果需要检索，则调用检索器进行检索，获取检索到的文档， 并将检索到的文档内容和问题一起构建成提示词列表<code>paragraphs</code></li><li>执行<code>_run_critic</code>方法进行结果生成并评估，这个方法的代码后面介绍</li><li>从<code>_run_critic</code>方法中得到每个文档的分数列表<code>paragraphs_final_score</code>，每个文档的生成结果列表<code>llm_response_per_paragraph</code>和文档列表<code>source_nodes</code></li><li>在<code>paragraphs_final_score</code>中选择得分最高的文档作为最终结果，然后返回该结果</li></ul><p>接下来我们看下 <code>_run_critic</code> 方法的实现：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_run_critic</span>(<span class="params">self, paragraphs: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; CriticOutput:</span><br><span class="line">    paragraphs_final_score = &#123;&#125;</span><br><span class="line">    llm_response_text = &#123;&#125;</span><br><span class="line">    source_nodes = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p_idx, paragraph <span class="keyword">in</span> <span class="built_in">enumerate</span>(paragraphs):</span><br><span class="line">        pred = <span class="variable language_">self</span>.llm(paragraph, **<span class="variable language_">self</span>.generate_kwargs)</span><br><span class="line">        <span class="comment"># Cache llm answer</span></span><br><span class="line">        llm_response_text[p_idx] = pred[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;text&quot;</span>]</span><br><span class="line">        logprobs = pred[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;logprobs&quot;</span>]</span><br><span class="line">        pred_log_probs = logprobs[<span class="string">&quot;top_logprobs&quot;</span>]</span><br><span class="line">        <span class="comment"># Compute isRel score, on the first predicted token</span></span><br><span class="line">        isRel_score = _relevance_score(pred_log_probs[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute isSup score</span></span><br><span class="line">        isSup_score = _is_supported_score(logprobs[<span class="string">&quot;tokens&quot;</span>], pred_log_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute isUse score</span></span><br><span class="line">        isUse_score = _is_useful_score(logprobs[<span class="string">&quot;tokens&quot;</span>], pred_log_probs)</span><br><span class="line"></span><br><span class="line">        paragraphs_final_score[p_idx] = (</span><br><span class="line">            isRel_score + isSup_score + <span class="number">0.5</span> * isUse_score</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># Add the paragraph as source node with its relevance score</span></span><br><span class="line">        source_nodes.append(</span><br><span class="line">            NodeWithScore(</span><br><span class="line">                node=TextNode(text=paragraph, id_=p_idx),</span><br><span class="line">                score=isRel_score,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        ...... <span class="comment"># 省略打印语句</span></span><br><span class="line">    <span class="keyword">return</span> CriticOutput(llm_response_text, paragraphs_final_score, source_nodes)</span><br></pre></td></tr></table></figure><ul><li>初始化几个变量：每个文档的分数列表<code>paragraphs_final_score</code>，每个文档的生成结果列表<code>llm_response_per_paragraph</code>和文档列表<code>source_nodes</code></li><li>遍历提示词列表<code>paragraphs</code>，每个提示词是问题加检索文档，将提示词发送给 LLM，获取生成结果，这里是串行地调用 LLM，如果换成并发的方式效率更高</li><li>从 LLM 的生成结果中获取生成的文本和<code>logprobs</code>参数，将文本内容保存到<code>llm_response_text</code>中，将<code>logprobs</code>参数用来计算<code>IsREL</code>、<code>IsSUP</code>和<code>IsUSE</code>的得分</li><li>分别计算<code>IsREL</code>、<code>IsSUP</code>和<code>IsUSE</code>的得分，然后通过这个计算公式计算出每个文档的最终得分：<code>final_score = IsREL + IsSUP + 0.5 * IsUSE</code>，并将结果保存到<code>paragraphs_final_score</code>中</li><li>将每个文档转换为 <code>NodeWithScore</code> 对象保存到<code>source_nodes</code>中</li><li>最后返回<code>CriticOutput</code>对象，包含了每个文档的生成结果、分数和文档列表</li></ul><p>关于更多 Self-RAG Pack 的代码实现，可以查看<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-self-rag/llama_index/packs/self_rag/base.py">这里</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过今天的介绍，我们了解了 Self-RAG 的实现原理，以及通过 LlamaIndex 的 Self-RAG Pack 代码来学习 Self-RAG 的具体实现，让我们可以更好地理解 Self-RAG 的细节和流程。Self-RAG 主要通过自训练的 LLM 来为生成结果提供 Reflection Token，从而可以轻松实现按需检索和评估等功能，无需再次调用 LLM 或者使用其他第三方库，这样可以提高效率和准确性。希望通过今天的介绍大家可以更好地理解 Self-RAG，将来可以更好地应用 Self-RAG 到实际项目中。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍 Self-RAG 的实现原理以及具体实践</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="selfrag" scheme="https://zhaozhiming.github.io/tags/selfrag/"/>
    
  </entry>
  
  <entry>
    <title>评估 RAG？只要 LlamaIndex 就足够了</title>
    <link href="https://zhaozhiming.github.io/2024/06/16/llamaindex-buildin-evaluation/"/>
    <id>https://zhaozhiming.github.io/2024/06/16/llamaindex-buildin-evaluation/</id>
    <published>2024-06-15T23:06:29.000Z</published>
    <updated>2024-10-03T08:49:29.888Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/06/llamaindex-evaluation.jpg" class="" width="400" height="300"><p>我们之前介绍过一些 RAG （Retrieval Augmented Generation）的评估工具，比如 Turlens、Ragas 等，它们的评估指标丰富、使用方便，但它们始终是独立的第三方工具，需要和 LLM（大语言模型）开发框架（LangChain、LlamaIndex）进行集成才能使用，功能一旦更新不及时就会导致不可用的问题。如果你正在使用的是 LlamaIndex 开发框架，那么恭喜你，LlamaIndex 内置了评估工具，可以帮助你快速评估 RAG 应用，无需集成第三方的评估工具。今天我们就来详细了解一下 LlamaIndex 内置评估工具的原理以及它们的使用方法。</p><span id="more"></span><h2 id="LlamaIndex-评估工具"><a href="#LlamaIndex-评估工具" class="headerlink" title="LlamaIndex 评估工具"></a>LlamaIndex 评估工具</h2><p><a href="https://www.llamaindex.ai/">LlamaIndex</a> 不但可以与很多外部优秀的第三方评估工具进行集成，而且在内部也自带了一套评估工具，如果你想快速地体验 RAG 的评估功能，那么使用 LlamaIndex 内置的评估工具就足够了。LlamaIndex 有以下评估指标：</p><ul><li>Answer Relevcancy</li><li>Context Relevancy</li><li>Relevancy</li><li>Faithfulness</li><li>Correctness</li></ul><p>这些评估指标我们后面会详细介绍，另外还有 LlamaIndex 特有的对比评估 Pairwise，可以帮助你评估两个检索引擎哪个生成的答案更好。</p><p>LlamaIndex 还提供了测试数据的生成功能，可以帮助我们轻松地生成评估所需的测试数据，包括评估的问题、参考答案等，这样我们就可以快速地进行评估工作，而不需要花费大量的时间去准备测试数据。</p><p>如果你想提升评估工作的效率，LlamaIndex 也提供了批量运行评估任务的工具，可以快速评估多种评估指标以及大量测试数据，批量任务的执行时间和单次任务的执行时间基本无异，这样就可以帮助我们快速地执行大量评估任务。</p><h2 id="测试数据生成"><a href="#测试数据生成" class="headerlink" title="测试数据生成"></a>测试数据生成</h2><p>评估 RAG 应用需要用到几个评估实体，分别是：</p><ul><li>Question: 指用户输入的问题，RAG 应用通过问题检索到相关的文档上下文</li><li>Context: 指检索到的文档上下文，RAG 应用检索到相关文档后会将这些上下文结合用户问题一起提交给 LLM，最后生成答案</li><li>Answer: 指生成的答案，RAG 应用将问题和上下文提交给 LLM 后，LLM 会根据这些信息来生成答案</li><li>Grouth Truth: 指人工标注的正确答案，利用这个实体可以对生成的答案进行分析，从而得到评估结果，在 LlamaIndex 中，这个实体叫做 Reference Answer</li></ul><p>其中 Question 和 Ground Truth 通过用户提供，Context 通过检索得到，Answer 是由 LLM 生成，后面我们在讲解的时候会沿用这些实体名称。在 LlamaIndex 中提供了生成测试数据集的功能，可以帮助我们快速生成测试数据集，无需人工干预。</p><p>首先我们来看下如何生成评估所需的 Question，这里的测试文档使用维基百科上的<a href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> SimpleDirectoryReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core.llama_dataset.generator <span class="keyword">import</span> RagDatasetGenerator</span><br><span class="line"><span class="keyword">from</span> llama_index.llms.openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">llm = OpenAI(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">dataset_generator = RagDatasetGenerator.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    llm=llm,</span><br><span class="line">    num_questions_per_chunk=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line">dataset = dataset_generator.generate_questions_from_nodes()</span><br><span class="line">examples = dataset.examples</span><br><span class="line"><span class="keyword">for</span> i, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples):</span><br><span class="line">    contexts = [n[:<span class="number">100</span>] <span class="keyword">for</span> n <span class="keyword">in</span> example.reference_contexts]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>. <span class="subst">&#123;example.query&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line"><span class="number">1.</span> Question: How did Ultron initially come into existence <span class="keyword">and</span> what was his ultimate goal?</span><br><span class="line"><span class="number">2.</span> Question: What event prompts the Avengers to devise a plan involving time travel to undo Thanos<span class="string">&#x27;s actions in &quot;Avengers: Endgame&quot;?</span></span><br><span class="line"><span class="string">3. Question: How does Thanos acquire the Power Stone and what events transpire after he obtains it?</span></span><br><span class="line"><span class="string">4. Question: How does Thanos ultimately achieve his goal of completing the Gauntlet and causing half of all life across the universe to disintegrate in &quot;Avengers: Infinity War&quot;?</span></span><br><span class="line"><span class="string">5. Question: How does Loki initially gain access to Earth and what is his ultimate goal upon arriving?</span></span><br></pre></td></tr></table></figure><ul><li>使用<code>SimpleDirectoryReader</code>读取文档</li><li>LlamaIndex 在新版本中推荐使用<code>RagDatasetGenerator</code>来生成测试数据，参数<code>documents</code>表示读取的文档列表，<code>llm</code>表示使用的大语言模型， 这里我们使用 OpenAI 的<code>gpt3.5</code>模型，<code>num_questions_per_chunk</code>表示每个文档生成的问题数量，这里我们设置为 1</li><li>然后调用数据生成器的<code>generate_questions_from_nodes</code>方法生成问题集，其原理是用 LLM 来根据文档生成问题，生成后的数据保存在<code>examples</code>属性中</li><li>最后遍历<code>examples</code> 对象，生成的 Question 在<code>example.query</code> 属性中</li><li>从显示结果中可以看到生成了 5 个 Question</li></ul><p>除了生成 Question 外，数据生成器还可以生成 Ground Truth，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">dataset = dataset_generator.generate_dataset_from_nodes()</span><br><span class="line">examples = dataset.examples</span><br><span class="line"><span class="keyword">for</span> i, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples):</span><br><span class="line">    contexts = [n[:<span class="number">100</span>] <span class="keyword">for</span> n <span class="keyword">in</span> example.reference_contexts]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>. <span class="subst">&#123;example.query&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Ground Truth: <span class="subst">&#123;example.reference_answer[:<span class="number">100</span>]&#125;</span>...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line"><span class="number">1.</span> Question: How did Ultron initially come into existence <span class="keyword">and</span> what was his ultimate goal?</span><br><span class="line">Ground Truth: Ultron initially came into existence when Tony Stark <span class="keyword">and</span> Bruce Banner discovered an artificial intel...</span><br><span class="line"><span class="number">2.</span> Question: What event prompts the Avengers to devise a plan involving time travel to undo Thanos<span class="string">&#x27;s actions in &quot;Avengers: Endgame&quot;?</span></span><br><span class="line"><span class="string">Ground Truth: The event that prompts the Avengers to devise a plan involving time travel to undo Thanos&#x27;</span>s actions ...</span><br><span class="line"><span class="number">3.</span> Question: How does Thanos acquire the Power Stone <span class="keyword">and</span> what events transpire after he obtains it?</span><br><span class="line">Ground Truth: Thanos acquires the Power Stone <span class="keyword">from</span> the planet Xandar. After obtaining the Power Stone, Thanos <span class="keyword">and</span> ...</span><br><span class="line"><span class="number">4.</span> Question: How does Thanos ultimately achieve his goal of completing the Gauntlet <span class="keyword">and</span> causing half of <span class="built_in">all</span> life across the universe to disintegrate <span class="keyword">in</span> <span class="string">&quot;Avengers: Infinity War&quot;</span>?</span><br><span class="line">Ground Truth: Thanos ultimately achieves his goal of completing the Gauntlet <span class="keyword">and</span> causing half of <span class="built_in">all</span> life across t...</span><br><span class="line"><span class="number">5.</span> Question: How does Loki initially gain access to Earth <span class="keyword">and</span> what <span class="keyword">is</span> his ultimate goal upon arriving?</span><br><span class="line">Ground Truth: Loki initially gains access to Earth by using the Tesseract to <span class="built_in">open</span> a wormhole. His ultimate goal up...</span><br></pre></td></tr></table></figure><p>这次使用数据生成器的<code>generate_dataset_from_nodes</code>方法来生成测试数据，生成的数据不仅包含 Question，还包含 Ground Truth，也是就代码中的<code>example.reference_answer</code>属性的值。其实除了 Question 和 Ground Truth 外，在生成的数据中还包含<code>reference_contexts</code>，这是数据生成器使用其内部检索器检索到的上下文，这个数据暂时对我们没有用处，我们只需要关注 Question 和 Ground Truth 即可。</p><h3 id="将数据集保存到-json-文件"><a href="#将数据集保存到-json-文件" class="headerlink" title="将数据集保存到 json 文件"></a>将数据集保存到 json 文件</h3><p>每次运行程序都重新生成一遍测试数据比较耗费资源，我们可以将生成的数据集保存到 json 文件中，下次直接读取 json 文件即可，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> llama_index.core.llama_dataset.rag <span class="keyword">import</span> LabelledRagDataset</span><br><span class="line"></span><br><span class="line">dataset_json = <span class="string">&quot;./output/test-dataset.json&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dataset_json):</span><br><span class="line">    dataset = dataset_generator.generate_dataset_from_nodes()</span><br><span class="line">    examples = dataset.examples</span><br><span class="line">    dataset.save_json(dataset_json)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dataset = LabelledRagDataset.from_json(dataset_json)</span><br><span class="line">    examples = dataset.examples</span><br></pre></td></tr></table></figure><ul><li>保存数据时使用<code>dataset</code>对象的<code>save_json</code>方法</li><li>读取数据时使用<code>LabelledRagDataset</code>的<code>from_json</code>方法</li></ul><h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><p>下面我们来详细介绍 LlamaIndex 的评估指标，并通过代码示例来了解如何使用这些评估指标。</p><h3 id="Answer-Relevcancy"><a href="#Answer-Relevcancy" class="headerlink" title="Answer Relevcancy"></a>Answer Relevcancy</h3><p>Answer Revelancy 是评估 Answer 和 Question 的相关性，这个指标可以帮助我们评估生成的答案是否和问题相关，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.evaluation <span class="keyword">import</span> AnswerRelevancyEvaluator</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, Settings</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"></span><br><span class="line">question = examples[<span class="number">0</span>].query</span><br><span class="line"></span><br><span class="line">node_parser = SentenceSplitter()</span><br><span class="line">nodes = node_parser.get_nodes_from_documents(documents)</span><br><span class="line">Settings.llm = llm</span><br><span class="line">vector_index = VectorStoreIndex(nodes)</span><br><span class="line">engine = vector_index.as_query_engine()</span><br><span class="line">response = engine.query(question)</span><br><span class="line">answer = <span class="built_in">str</span>(response)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;question&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Answer: <span class="subst">&#123;answer&#125;</span>&quot;</span>)</span><br><span class="line">evaluator = AnswerRelevancyEvaluator(llm)</span><br><span class="line">result = evaluator.evaluate(query=question, response=answer)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;result.score&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;feedback: <span class="subst">&#123;result.feedback&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Question: How did Ultron initially come into existence <span class="keyword">and</span> what was his ultimate goal?</span><br><span class="line">Answer: Ultron initially came into existence when Tony Stark <span class="keyword">and</span> Bruce Banner discovered an artificial intelligence within Loki<span class="string">&#x27;s scepter and decided to use it to complete Stark&#x27;</span>s <span class="string">&quot;Ultron&quot;</span> <span class="keyword">global</span> defense program. Ultron<span class="string">&#x27;s ultimate goal was to eradicate humanity in order to save Earth.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">score: 1.0</span></span><br><span class="line"><span class="string">feedback: 1. The provided response matches the subject matter of the user&#x27;</span>s query by explaining how Ultron initially came into existence <span class="keyword">and</span> what his ultimate goal was.</span><br><span class="line"><span class="number">2.</span> The response directly addresses the focus <span class="keyword">and</span> perspective of the use<span class="string">r&#x27;s query by detailing the specific events that led to Ultron&#x27;</span>s creation <span class="keyword">and</span> his ultimate goal of eradicating humanity.</span><br><span class="line"></span><br><span class="line">[RESULT] <span class="number">2</span></span><br></pre></td></tr></table></figure><ul><li>我们使用测试数据集的第一条数据的问题作为评估问题</li><li>然后构建一个普通的 RAG 查询引擎，并通过查询评估问题来得到答案</li><li>将问题和答案传递给<code>AnswerRelevancyEvaluator</code>评估器，通过<code>evaluate</code>方法来评估问题和答案的相关性</li><li>评估结果的<code>score</code>范围是 0~1，得分越高表示答案和问题的相关性越高，得分为 1 表示完全相关</li><li>评估结果中还有<code>feedback</code>属性，用来解释评估结果，这个属性可以帮助我们了解评估结果的产生原因</li></ul><p>LlamaIndex 中每种评估器的初始化参数都基本一致，以<code>AnswerRelevancyEvaluator</code> 为例， 有以下主要参数：</p><ul><li>llm: 评估使用的大语言模型</li><li>eval_template: 评估时所用的提示词模板</li><li>score_threshold: 这个参数在不同的评估器中有不同的含义，在<code>AnswerRelevancyEvaluator</code> 中这个参数用来将反馈中的分数转换到 0~1 范围，在<code>CorrectnessEvaluator</code> 中这个参数用来评判答案是否正确</li></ul><p>在上面的反馈结果中我们可以看到<code>[RESULT] 2</code>，这个值就是反馈中的分数，LLM 在评估过程中评估了 2 个问题，每个问题回答正确则得 1 分，从得分结果来看，2 个问题都回答正确，所以得分为 2，然后除以阀值 2.0，得到最终分数为 1.0。</p><h4 id="评估提示词模板修改"><a href="#评估提示词模板修改" class="headerlink" title="评估提示词模板修改"></a>评估提示词模板修改</h4><p><code>eval_template</code>参数用来设置评估提示词模板，我们可以来看下<code>AnswerRelevancyEvaluator</code>默认的评估提示词：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">DEFAULT_EVAL_TEMPLATE = PromptTemplate(</span><br><span class="line">    <span class="string">&quot;Your task is to evaluate if the response is relevant to the query.\n&quot;</span></span><br><span class="line">    <span class="string">&quot;The evaluation should be performed in a step-by-step manner by answering the following questions:\n&quot;</span></span><br><span class="line">    <span class="string">&quot;1. Does the provided response match the subject matter of the user&#x27;s query?\n&quot;</span></span><br><span class="line">    <span class="string">&quot;2. Does the provided response attempt to address the focus or perspective &quot;</span></span><br><span class="line">    <span class="string">&quot;on the subject matter taken on by the user&#x27;s query?\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Each question above is worth 1 point. Provide detailed feedback on response according to the criteria questions above  &quot;</span></span><br><span class="line">    <span class="string">&quot;After your feedback provide a final result by strictly following this format: &#x27;[RESULT] followed by the integer number representing the total score assigned to the response&#x27;\n\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Query: \n &#123;query&#125;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Response: \n &#123;response&#125;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Feedback:&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>评估提示词模板是一个<code>PromptTemplate</code>对象，这个对象有一个<code>template</code>属性，这个属性就是评估提示词模板的字符串内容，如果我们想要修改评估提示词，一种方法是重新写一套评估提示词指令，另外一种方法是在这个模板的前面或后面添加提示词来对评估指令进行微调，比如我想让 LLM 将评估结果用中文回复，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.evaluation.answer_relevancy <span class="keyword">import</span> DEFAULT_EVAL_TEMPLATE</span><br><span class="line"></span><br><span class="line">translate_prompt = <span class="string">&quot;\n\nPlease reply in Chinese.&quot;</span></span><br><span class="line">eval_template = DEFAULT_EVAL_TEMPLATE</span><br><span class="line">eval_template.template += translate_prompt</span><br><span class="line">evaluator = AnswerRelevancyEvaluator(</span><br><span class="line">    llm=llm, eval_template=eval_template</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这里我们在<code>AnswerRelevancyEvaluator</code>的默认提示词模板上添加了返回中文回复的提示词，然后通过<code>eval_template</code>参数传递给评估器，这样评估器在评估任务完成后就会将评估结果用中文返回。</p><h3 id="Context-Relevancy"><a href="#Context-Relevancy" class="headerlink" title="Context Relevancy"></a>Context Relevancy</h3><p>Context Relevancy 是评估 Context 和 Question 的相关性，这个指标可以帮助我们评估检索到的文档上下文和问题的相关性，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.evaluation <span class="keyword">import</span> ContextRelevancyEvaluator</span><br><span class="line"></span><br><span class="line">contexts = [n.get_content() <span class="keyword">for</span> n <span class="keyword">in</span> response.source_nodes]</span><br><span class="line">evaluator = ContextRelevancyEvaluator(llm)</span><br><span class="line">result = evaluator.evaluate(query=question, contexts=contexts)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;result.score&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;feedback: <span class="subst">&#123;result.feedback&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">score: <span class="number">1.0</span></span><br><span class="line">feedback: <span class="number">1.</span> The retrieved context matches the subject matter of the use<span class="string">r&#x27;s query. It provides a detailed explanation of how Ultron initially came into existence and what his ultimate goal was.</span></span><br><span class="line"><span class="string">2. The retrieved context can be used exclusively to provide a full answer to the user&#x27;</span>s query. It covers <span class="built_in">all</span> the necessary information about Ultron<span class="string">&#x27;s creation and his goal to eradicate humanity.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[RESULT] 4.0</span></span><br></pre></td></tr></table></figure><ul><li>我们通过查询引擎返回的结果<code>response</code>中的<code>source_nodes</code>属性获取到 Context，并将其转化为字符串列表，评估时需要这种格式的数据</li><li>构建<code>ContextRelevancyEvaluator</code>评估器</li><li>将 Question 和 Context 传递给评估器的<code>evaluate</code>方法进行评估</li><li>最后输出评估结果</li></ul><p>从评估结果中可以看到，评估器评估了 2 个问题，每个问题得分 2，最终得分为 4，这个得分是通过评估器内部的评估模板计算出来的，分数经过转换后得到 score 为 1.0。</p><p>在评估结果中除了<code>score</code>和<code>feedback</code>属性外，还有其他一些属性：</p><ul><li>query: 评估的问题，也就是 Question</li><li>contexts: 评估的上下文，也就是 Context</li><li>response: 评估的回答，也就是 Answer</li><li>passing: 是否通过，如果评估结果通过则为 True，否则为 False，在一些评估器中这个属性和评估器的<code>score_threshold</code>属性有关</li><li>pairwise_source: 对比评估源，这是对比评估才有的属性，后面会详细介绍</li></ul><h3 id="Relevancy"><a href="#Relevancy" class="headerlink" title="Relevancy"></a>Relevancy</h3><p>Relevancy 是评估 Answer、Context 与 Question 是否相关，这个指标可以帮助我们评估问题是否真正得到了回答，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.evaluation <span class="keyword">import</span> AnswerRelevancyEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = RelevancyEvaluator(llm)</span><br><span class="line">result = evaluator.evaluate(query=question, response=answer, contexts=contexts)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;result.score&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;feedback: <span class="subst">&#123;result.feedback&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;passing: <span class="subst">&#123;result.passing&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">score: <span class="number">1.0</span></span><br><span class="line">feedback: YES</span><br><span class="line">passing: <span class="literal">True</span></span><br></pre></td></tr></table></figure><ul><li>构建<code>RelevancyEvaluator</code>评估器</li><li>这个评估器需要传递 Question、Answer 和 Context 三个参数进行评估</li><li>最后输出评估结果</li></ul><p>因为这个评估是检查 Answer 和 Context 是否与 Question 相关， 因此评估结果是一个布尔值， 当<code>feedback</code>为<code>YES</code>表示 Answer、Context 与 Question 相关，同时<code>passing</code>为<code>True</code>，<code>score</code>为 1.0。</p><h3 id="Faithfulness"><a href="#Faithfulness" class="headerlink" title="Faithfulness"></a>Faithfulness</h3><p>Faithfulness 是评估 Answer 和 Context 是否匹配，这个指标可以帮助我们评估生成的答案是否符合上下文，检查答案是否有<strong>幻觉</strong>，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.evaluation <span class="keyword">import</span> FaithfulnessEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = FaithfulnessEvaluator(llm)</span><br><span class="line">result = evaluator.evaluate(response=answer, contexts=contexts)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;result.score&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;feedback: <span class="subst">&#123;result.feedback&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;passing: <span class="subst">&#123;result.passing&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">score: <span class="number">1.0</span></span><br><span class="line">feedback: YES</span><br><span class="line">passing: <span class="literal">True</span></span><br></pre></td></tr></table></figure><ul><li>构建<code>FaithfulnessEvaluator</code>评估器</li><li>这个评估器需要传递 Answer 和 Context 两个参数进行评估</li><li>最后输出评估结果，评估结果也是一个布尔值，当<code>feedback</code>为<code>YES</code>表示两者相关， 同时<code>passing</code>为<code>True</code>，<code>score</code>为 1.0</li></ul><p>LlamaIndex 的评估工具不仅可以对检索引擎进行评估，还可以对 Pipeline 进行评估，只要将 Pipeline 的输出结果作为评估的参数即可：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_pipeline <span class="keyword">import</span> QueryPipeline, InputComponent</span><br><span class="line"><span class="keyword">from</span> llama_index.core.response_synthesizers.simple_summarize <span class="keyword">import</span> SimpleSummarize</span><br><span class="line"></span><br><span class="line">p = QueryPipeline(verbose=<span class="literal">True</span>)</span><br><span class="line">p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: InputComponent(),</span><br><span class="line">        <span class="string">&quot;retriever&quot;</span>: retriever,</span><br><span class="line">        <span class="string">&quot;output&quot;</span>: SimpleSummarize(),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">p.add_link(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;retriever&quot;</span>)</span><br><span class="line">p.add_link(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;output&quot;</span>, dest_key=<span class="string">&quot;query_str&quot;</span>)</span><br><span class="line">p.add_link(<span class="string">&quot;retriever&quot;</span>, <span class="string">&quot;output&quot;</span>, dest_key=<span class="string">&quot;nodes&quot;</span>)</span><br><span class="line">output = p.run(<span class="built_in">input</span>=question)</span><br><span class="line">answer = <span class="built_in">str</span>(output)</span><br><span class="line">contexts = [n.get_content() <span class="keyword">for</span> n <span class="keyword">in</span> output.source_nodes]</span><br></pre></td></tr></table></figure><p>我们创建一个基本的 RAG Pipeline， 然后使用 Pipeline 来代替检索引擎进行问题检索和回答生成，最后将得到的 Answer 和 Context 传递给评估器进行评估即可。关于<code>Pipeline</code>的更多介绍可以参考我之前的<a href="https://zhaozhiming.github.io/2024/06/08/rag-module-pipeline/">这篇文章</a>。</p><h3 id="Correctness"><a href="#Correctness" class="headerlink" title="Correctness"></a>Correctness</h3><p>Correctness 是评估 Answer 和 Ground Truth 的相关性和正确性，这个指标可以帮助我们评估生成的答案是否正确，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.evaluation <span class="keyword">import</span> CorrectnessEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = CorrectnessEvaluator(llm)</span><br><span class="line">ground_truth = dataset_examples[<span class="number">1</span>].reference_answer</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;question&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Answer: <span class="subst">&#123;answer&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ground Truth: <span class="subst">&#123;ground_truth&#125;</span>&quot;</span>)</span><br><span class="line">result = evaluator.evaluate(query=question, response=answer, reference=ground_truth)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;result.score&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;feedback: <span class="subst">&#123;result.feedback&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;passing: <span class="subst">&#123;result.passing&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Question: What event prompts the Avengers to devise a plan involving time travel to undo Thanos<span class="string">&#x27;s actions in &quot;Avengers: Endgame&quot;?</span></span><br><span class="line"><span class="string">Answer: The event that prompts the Avengers to devise a plan involving time travel to undo Thanos&#x27;</span>s actions <span class="keyword">in</span> <span class="string">&quot;Avengers: Endgame&quot;</span> occurs when Scott Lang escapes <span class="keyword">from</span> the Quantum Realm <span class="keyword">and</span> reaches the Avengers Compound. He explains that he experienced only five hours <span class="keyword">while</span> trapped, despite being there <span class="keyword">for</span> five years. This leads to the realization that the Quantum Realm allows <span class="keyword">for</span> time travel, prompting the Avengers to ask Tony Stark to <span class="built_in">help</span> them retrieve the Infinity Stones <span class="keyword">from</span> the past to reverse Thanos<span class="string">&#x27;s actions in the present.</span></span><br><span class="line"><span class="string">Ground Truth: The event that prompts the Avengers to devise a plan involving time travel to undo Thanos&#x27;</span>s actions <span class="keyword">in</span> <span class="string">&quot;Avengers: Endgame&quot;</span> <span class="keyword">is</span> the discovery that Thanos has already destroyed the Infinity Stones, preventing <span class="built_in">any</span> further use to reverse his actions.</span><br><span class="line"></span><br><span class="line">score: <span class="number">4.0</span></span><br><span class="line">feedback: The generated answer <span class="keyword">is</span> relevant <span class="keyword">and</span> mostly correct <span class="keyword">in</span> detailing the events leading to the Avengers<span class="string">&#x27; decision to use time travel in &quot;Avengers: Endgame.&quot; It accurately describes Scott Lang&#x27;</span>s escape <span class="keyword">from</span> the Quantum Realm <span class="keyword">and</span> his crucial role <span class="keyword">in</span> introducing the concept of time manipulation via the Quantum Realm. However, it slightly deviates <span class="keyword">from</span> the reference answer, which emphasizes the destruction of the Infinity Stones by Thanos <span class="keyword">as</span> the critical event. The generated answer instead focuses on the discovery of time travel <span class="keyword">as</span> a viable option, which <span class="keyword">is</span> also a correct perspective but <span class="keyword">not</span> the only one. Thus, the score reflects high relevance <span class="keyword">and</span> correctness <span class="keyword">with</span> a minor deviation <span class="keyword">in</span> focus.</span><br><span class="line">passing: <span class="literal">True</span></span><br></pre></td></tr></table></figure><ul><li>构建<code>CorrectnessEvaluator</code>评估器</li><li>使用我们之前创建的测试数据集中某条数据的<code>reference_answer</code>作为 Ground Truth</li><li>将 Question、Answer 和 Ground Truth 传递给评估器的<code>evaluate</code>方法进行评估</li></ul><p><code>CorrectnessEvaluator</code>评估器的得分范围是 1 ～ 5，当分数大于等于 4 时表示答案正确，<code>passing</code>为<code>True</code>，评估器根据 Qustion、Answer 和 Ground Truth 进行评估，最后输出评估结果。</p><h3 id="Pairwise"><a href="#Pairwise" class="headerlink" title="Pairwise"></a>Pairwise</h3><p>Pairwise 是对比评估，可以帮助我们评估两个检索引擎生成的 Answer 哪个更好，在执行对比评估之前，我们需要再构建一个检索引擎，这个检索引擎我们使用不同的文档分块策略，这样才可以与之前的检索引擎进行区分，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">node_parser = SentenceSplitter(chunk_size=<span class="number">128</span>, chunk_overlap=<span class="number">25</span>)</span><br><span class="line">nodes = node_parser.get_nodes_from_documents(documents)</span><br><span class="line">Settings.llm = llm</span><br><span class="line">vector_index = VectorStoreIndex(nodes)</span><br><span class="line">second_engine = vector_index.as_query_engine()</span><br><span class="line">second_response = engine.query(question)</span><br><span class="line">second_answer = <span class="built_in">str</span>(second_response)</span><br></pre></td></tr></table></figure><ul><li>原来的检索引擎<code>engine</code>使用的是<code>SentenceSplitter</code>文档分割器默认的分块策略，<code>chunk_size</code>为 1024，<code>chunk_overlap</code>为 200</li><li>我们新建了另外一个检索引擎<code>second_engine</code>，并将文档分割器的<code>chunk_size</code>设置为 128，<code>chunk_overlap</code>设置为 25</li><li>然后使用<code>second_engine</code>来查询问题，得到另一个答案<code>second_answer</code></li></ul><p>然后我们使用<code>PairwiseEvaluator</code>评估器来对比两个答案，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.evaluation <span class="keyword">import</span> PairwiseComparisonEvaluator</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;question&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Answer: <span class="subst">&#123;answer&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Second Answer: <span class="subst">&#123;second_answer&#125;</span>&quot;</span>)</span><br><span class="line">evaluator = PairwiseComparisonEvaluator(llm)</span><br><span class="line">result = evaluator.evaluate(</span><br><span class="line">    query=question, response=answer, second_response=second_answer</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;result.score&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;feedback: <span class="subst">&#123;result.feedback&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;pairwise source: <span class="subst">&#123;<span class="built_in">str</span>(result.pairwise_source)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Question: What event prompts the Avengers to devise a plan involving time travel to undo Thanos<span class="string">&#x27;s actions in &quot;Avengers: Endgame&quot;?</span></span><br><span class="line"><span class="string">Answer: The event that prompts the Avengers to devise a plan involving time travel to undo Thanos&#x27;</span>s actions <span class="keyword">in</span> <span class="string">&quot;Avengers: Endgame&quot;</span> <span class="keyword">is</span> the discovery that Thanos has already destroyed the Infinity Stones, preventing <span class="built_in">any</span> further use to reverse his actions.</span><br><span class="line">Second Answer: The destruction of the Infinity Stones by Thanos prompts the Avengers to devise a plan involving time travel to undo Thanos<span class="string">&#x27;s actions in &quot;Avengers: Endgame&quot;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">score: 1.0</span></span><br><span class="line"><span class="string">feedback: Assistant A provides a more detailed and informative response by explaining that the Avengers discover that Thanos has already destroyed the Infinity Stones, which is the event that prompts them to devise a plan involving time travel to undo his actions in &quot;Avengers: Endgame.&quot; This additional context enhances the understanding of the situation and the motivation behind the Avengers&#x27;</span> plan. Assistant B, on the other hand, simply states that the destruction of the Infinity Stones by Thanos <span class="keyword">is</span> the event that leads to the Avengers<span class="string">&#x27; plan without providing any further elaboration.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Therefore, based on the level of detail and clarity provided in the responses, [[A]] Assistant A is better.</span></span><br><span class="line"><span class="string">pairwise source: EvaluationSource.ORIGINAL</span></span><br></pre></td></tr></table></figure><ul><li>构建<code>PairwiseComparisonEvaluator</code>评估器</li><li>将 Question、Answer 和 Second Answer 传递给评估器的<code>evaluate</code>方法进行评估</li></ul><p>在显示结果中，我们打印了 Question、Answer 和 Second Answer，以及评估结果的几个属性，从评估结果中可以看到，第一个 Answer 比第二个 Answer 更好。在评估结果中还有一个<code>pairwise_source</code>属性，值是<code>EvaluationSource.ORIGINAL</code>，表示评估顺序是原始顺序。</p><p>在 <code>PairwiseComparisonEvaluator</code>评估器中，有一个初始化参数<code>enforce_consensus</code>，默认值是 True。在评估器进行对比评估时，首先会将 Answer 和 Second Answer 进行对比， 即<code>evaluate(response=answer, second_response=second_answer)</code>，如果<code>enforce_consensus</code>为 True，<strong>则会将 Answer 和 Second Answer 反过来再进行对比</strong>， 即<code>evaluate(response=second_answer, second_response=answer)</code>， 最后根据两次结果来产生最终的评估结果。如果最终结果使用的是反转后的结果，那么<code>pairwise source</code>的值就是<code>EvaluationSource.FLIPPED</code>。</p><p>可以看下另外一种对比结果，在下面的评估结果中，2 个 Answer 的得分一样，评估结果是平局：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">score: 0.5</span><br><span class="line">feedback: Both Assistant A and Assistant B provided the same answer to the user<span class="string">&#x27;s question, stating that Tony Stark and Bruce Banner are the two members of the Avengers who created Ultron. Since both responses are identical in terms of accuracy and relevance to the user&#x27;</span>s question, there is no significant difference between the two answers. Therefore, <span class="keyword">in</span> this <span class="keyword">case</span>, it is a tie between Assistant A and Assistant B.</span><br><span class="line"></span><br><span class="line">Therefore, the final verdict is <span class="string">&#x27;[[C]]&#x27;</span> <span class="keyword">for</span> a tie.</span><br><span class="line">pairwise_source: EvaluationSource.ORIGINAL</span><br></pre></td></tr></table></figure><h2 id="批量评估"><a href="#批量评估" class="headerlink" title="批量评估"></a>批量评估</h2><p>介绍完了 LlamaIndex 的评估指标后，有人可能会担心如果一次性运行这么多评估指标，那么运行时间会不会很长，其实不用担心，LlamaIndex 很贴心地提供了一个批量评估的工具，可以帮助我们快速地运行多个评估指标，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.evaluation <span class="keyword">import</span> BatchEvalRunner</span><br><span class="line"></span><br><span class="line">answer_relevancy_evaluator = AnswerRelevancyEvaluator(llm)</span><br><span class="line">context_relevancy_evaluator = ContextRelevancyEvaluator(llm)</span><br><span class="line">relevant_evaluator = RelevancyEvaluator(llm)</span><br><span class="line">correctness_evaluator = CorrectnessEvaluator(llm)</span><br><span class="line">faithfulness_evaluator = FaithfulnessEvaluator(llm)</span><br><span class="line"></span><br><span class="line">runner = BatchEvalRunner(</span><br><span class="line">    evaluators=&#123;</span><br><span class="line">        <span class="string">&quot;answer_relevancy&quot;</span>: answer_relevancy_evaluator,</span><br><span class="line">        <span class="string">&quot;context_relevancy&quot;</span>: context_relevancy_evaluator,</span><br><span class="line">        <span class="string">&quot;relevancy&quot;</span>: relevant_evaluator,</span><br><span class="line">        <span class="string">&quot;correctness&quot;</span>: correctness_evaluator,</span><br><span class="line">        <span class="string">&quot;faithfulness&quot;</span>: faithfulness_evaluator,</span><br><span class="line">    &#125;,</span><br><span class="line">    workers=<span class="number">8</span>,</span><br><span class="line">)</span><br><span class="line">questions = [example.query <span class="keyword">for</span> example <span class="keyword">in</span> examples]</span><br><span class="line">ground_truths = [example.reference_answer <span class="keyword">for</span> example <span class="keyword">in</span> examples]</span><br><span class="line">metrics_results = runner.evaluate_queries(</span><br><span class="line">    engine, queries=questions, reference=ground_truths</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> metrics <span class="keyword">in</span> metrics_results.keys():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;metrics: <span class="subst">&#123;metrics&#125;</span>&quot;</span>)</span><br><span class="line">    eval_results = metrics_results[metrics]</span><br><span class="line">    <span class="keyword">for</span> eval_result <span class="keyword">in</span> eval_results:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;eval_result.score&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;feedback: <span class="subst">&#123;eval_result.feedback&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> eval_result.passing <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;passing: <span class="subst">&#123;eval_result.passing&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">metrics: answer_relevancy</span><br><span class="line">score: <span class="number">1.0</span></span><br><span class="line">feedback: <span class="number">1.</span> The provided response matches the subject matter of the use<span class="string">r&#x27;s query by explaining how Ultron initially came into existence and what his ultimate goal was.</span></span><br><span class="line"><span class="string">2. The response directly addresses the focus and perspective of the user&#x27;</span>s query by detailing the specific events that led to Ultron<span class="string">&#x27;s creation and his ultimate goal of eradicating humanity.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[RESULT] 2</span></span><br><span class="line"><span class="string">......</span></span><br></pre></td></tr></table></figure><ul><li>我们首先创建了 5 个评估器，分别是<code>AnswerRelevancyEvaluator</code>、<code>ContextRelevancyEvaluator</code>、<code>RelevancyEvaluator</code>、<code>CorrectnessEvaluator</code>、<code>FaithfulnessEvaluator</code></li><li>然后通过测试数据集提取了 Question 列表<code>questions</code>和 Ground Truth 列表<code>ground_truths</code>，每个列表分别有 5 个元素</li><li>使用<code>BatchEvalRunner</code>构建一个批量评估运行器，初始化参数<code>evaluators</code>为 5 个评估器，<code>workers</code>参数表示并行运行的工作线程数，<code>workers</code>的数量可以根据运行机器上的 CPU 核数来决定</li><li>调用<code>aevaluate_queries</code>方法来运行评估，传递的参数是查询引擎、Question 列表和 Ground Truth 列表</li><li>评估结果最后会根据评估器名称保存在<code>metrics_results</code>字典中，我们遍历这个字典，输出评估结果</li></ul><p>5 个评估器加上 5 个问题，相当于我们执行了 25 次评估，但执行时间和运行单次评估的时间基本相同，但需要注意的是，<code>BatchEvalRunner</code>只能在检索引擎下使用，不能通过 Pipeline 使用。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>LlamaIndex 内置的评估工具有以下优缺点。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>不需要额外安装第三方库，可以快速使用</li><li>评估指标可以满足大部分评估需求</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>评估方法基本上是通过 LLM 加提示词的方式来评估，评估使用的 LLM 不同，可能评估效果差别也会比较大，其他 RAG 评估工具会使用一些计算公式来结合提示词进行评估，从而减小 LLM 的影响</li><li>是 LlamaIndex 内置的功能，这是优点也是缺点，毕竟评估功能与其他 RAG 功能相比重要性较低，以后随着 LlamaIndex 更多新功能的加入，评估功能的开发优先级可能会降低</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体而言，LlamaIndex 的评估功能可以帮助我们快速地评估 RAG 的性能，满足我们基本的 RAG 评估需求，无需借助其他第三方库。如果你正在使用 LlamaIndex 开发 RAG 应用，建议使用 LlamaIndex 内置的评估工具，使用后如果发现满足不了需求再考虑使用其他第三方评估工具。希望这篇文章可以帮助大家更好地了解 LlamaIndex 的评估功能。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍如何使用 LlamaIndex 内置评估工具进行 RAG 应用评估</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="evaluation" scheme="https://zhaozhiming.github.io/tags/evaluation/"/>
    
  </entry>
  
  <entry>
    <title>高级 RAG 检索策略之流程与模块化</title>
    <link href="https://zhaozhiming.github.io/2024/06/08/rag-module-pipeline/"/>
    <id>https://zhaozhiming.github.io/2024/06/08/rag-module-pipeline/</id>
    <published>2024-06-08T12:54:26.000Z</published>
    <updated>2024-10-03T08:49:29.887Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/06/rag-module-flow.jpg" class="" width="400" height="300"><p>我们介绍了很多关于高级 RAG（Retrieval Augmented Generation）的检索策略，每一种策略就像是机器中的零部件，我们可以通过对这些零部件进行不同的组合，来实现不同的 RAG 功能，从而满足不同的需求。今天我们就来介绍高级 RAG 检索中一些常见的 RAG 模块，以及如何通过流程的方式来组合这些模块，实现高级 RAG 检索功能。</p><span id="more"></span><h2 id="RAG-模块化"><a href="#RAG-模块化" class="headerlink" title="RAG 模块化"></a>RAG 模块化</h2><p>模块化 RAG 提出了一种高度可扩展的范例，将 RAG 系统分为模块类型、模块和操作符的三层结构。每个模块类型代表 RAG 系统中的一个核心流程，包含多个功能模块。每个功能模块又包含多个特定的操作符。整个 RAG 系统变成了多个模块和相应操作符的排列组合，形成了我们所说的 RAG 流程。在流程中，每种模块类型可以选择不同的功能模块，并且在每个功能模块中可以选择一个或多个操作符。</p><img src="/images/post/2024/06/rag-module-intro.jpg" class="" width="1000" height="600"><h2 id="RAG-流程"><a href="#RAG-流程" class="headerlink" title="RAG 流程"></a>RAG 流程</h2><p>RAG 流程是指在 RAG 系统中，从输入查询到输出生成文本的整个工作流程。这个流程通常涉及多个模块和操作符的协同工作，包括但不限于检索器、生成器以及可能的预处理和后处理模块。RAG 流程的设计旨在使得 LLM（大语言模型）能够在生成文本时利用外部知识库或文档集，从而提高回答的准确性和相关性。</p><p>RAG 推理阶段的流程一般分为以下几种模式：</p><ul><li>Sequential: 线性流程，包括高级和简单的 RAG 范式</li><li>Conditional: 基于查询的关键词或语义选择不同的 RAG 路径</li><li>Branching: 包括多个并行分支，分为预检索和后检索的分支结构</li><li>Loop: 包括迭代、递归和自适应检索等多种循环结构</li></ul><p>下图是 Loop 模式的 RAG 流程图：</p><img src="/images/post/2024/06/loop-rag-flow.jpeg" class="" width="1000" height="600"><p>后面我们主要以 Sequential 模式为例，介绍如何通过模块化和流水线的方式来实现高级 RAG 检索功能。</p><h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><p><a href="https://www.llamaindex.ai/">LlamaIndex</a>的查询流水线（Query Pipeline）功能提供了一种模块化的方式来组合 RAG 检索策略。我们可以通过定义不同的模块，然后将这些模块按照一定的顺序组合起来，形成一个完整的查询流水线。下面我们通过一个从简单到复杂的示例来演示如何使用 LlamaIndex 的查询流水线功能实现高级 RAG 检索。</p><h3 id="普通-RAG"><a href="#普通-RAG" class="headerlink" title="普通 RAG"></a>普通 RAG</h3><p>首先我们定义一个普通 RAG 的流水线，这个流水线包含了 3 个模块，分别是：输入、检索和输出。其中输入模块用于接收用户输入的查询，检索模块用于从知识库中检索相关文档，输出模块用于根据检索结果生成回答。</p><img src="/images/post/2024/06/rag-flow-base.png" class="" width="1000" height="600"><p>在定义查询流水线之前，我们先将我们的测试文档索引入库，这里的测试文档还是用维基百科上的<a href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> llama_index.llms.openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> llama_index.embeddings.openai <span class="keyword">import</span> OpenAIEmbedding</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> (</span><br><span class="line">    Settings,</span><br><span class="line">    SimpleDirectoryReader,</span><br><span class="line">    StorageContext,</span><br><span class="line">    VectorStoreIndex,</span><br><span class="line">    load_index_from_storage,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">node_parser = SentenceSplitter()</span><br><span class="line">llm = OpenAI(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">embed_model = OpenAIEmbedding(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)</span><br><span class="line">Settings.llm = llm</span><br><span class="line">Settings.embed_model = embed_model</span><br><span class="line">Settings.node_parser = node_parser</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;storage&quot;</span>):</span><br><span class="line">    index = VectorStoreIndex.from_documents(documents)</span><br><span class="line">    index.set_index_id(<span class="string">&quot;avengers&quot;</span>)</span><br><span class="line">    index.storage_context.persist(<span class="string">&quot;./storage&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    store_context = StorageContext.from_defaults(persist_dir=<span class="string">&quot;./storage&quot;</span>)</span><br><span class="line">    index = load_index_from_storage(</span><br><span class="line">        storage_context=store_context, index_id=<span class="string">&quot;avengers&quot;</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><ul><li>首先我们通过<code>SimpleDirectoryReader</code>读取<code>./data</code>目录下的文档</li><li>然后我们定义了一个<code>SentenceSplitter</code>用于将文档进行分割</li><li>接着我们使用<code>OpenAI</code>的 LLM 和 Embedding 模型来生成文本和向量，并将他们添加到<code>Settings</code>中</li><li>最后我们将文档索引入库，并将索引保存到<code>./storage</code>目录下，以便后续使用</li></ul><p>接下来我们定义一个普通的 RAG 流水线，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_pipeline <span class="keyword">import</span> QueryPipeline, InputComponent</span><br><span class="line"><span class="keyword">from</span> llama_index.core.response_synthesizers.simple_summarize <span class="keyword">import</span> SimpleSummarize</span><br><span class="line"></span><br><span class="line">retriever =  index.as_retriever()</span><br><span class="line">p = QueryPipeline(verbose=<span class="literal">True</span>)</span><br><span class="line">p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: InputComponent(),</span><br><span class="line">        <span class="string">&quot;retriever&quot;</span>: retriever,</span><br><span class="line">        <span class="string">&quot;output&quot;</span>: SimpleSummarize(),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">p.add_link(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;retriever&quot;</span>)</span><br><span class="line">p.add_link(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;output&quot;</span>, dest_key=<span class="string">&quot;query_str&quot;</span>)</span><br><span class="line">p.add_link(<span class="string">&quot;retriever&quot;</span>, <span class="string">&quot;output&quot;</span>, dest_key=<span class="string">&quot;nodes&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>我们创建了一个普通检索器<code>retriever</code>，用于从知识库中检索相关文档</li><li>然后创建了一个<code>QueryPipeline</code>对象，这是查询流水线的主体，设置 verbose 参数为 True 用于输出详细信息</li><li>通过<code>QueryPipeline</code>的<code>add_modules</code>方法添加了 3 个模块：<code>input</code>、<code>retriever</code>和<code>output</code></li><li><code>input</code>模块的实现类是<code>InputComponent</code>，这是查询流水线常用的输入组件，<code>retriever</code>模块是我们定义的检索器，<code>output</code>模块的实现类是<code>SimpleSummarize</code>，这是可以将问题和检索结果进行简单总结的输出组件</li><li>接着我们添加模块间的连接关系，<code>add_link</code>方法用于连接模块之间的关系，第一个参数是源模块，第二个参数是目标模块</li><li><code>dest_key</code>参数用于指定目标模块的输入参数，因为<code>output</code>模块有 2 个参数，分别是问题和检索结果，所以我们需要指定<code>dest_key</code>参数，当目标模块只有一个参数时则不需要指定</li><li>在<code>add_link</code>方法中，与<code>dest_key</code>参数对应的是<code>src_key</code>参数，当源模块有多个参数时，我们需要指定<code>src_key</code>参数，反之则不需要。</li></ul><p>查询流水线添加模块和连接关系的方式除了<code>add_modules</code>和<code>add_link</code>方法外，还可以通过<code>add_chain</code>方法添加，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = QueryPipeline(verbose=<span class="literal">True</span>)</span><br><span class="line">p.add_chain([InputComponent(), retriever])</span><br></pre></td></tr></table></figure><p>这种方式可以一次性添加模块与连接关系，但这种方式只能添加单参数的模块，如果模块有多个参数则需要使用<code>add_modules</code>和<code>add_link</code>方法。</p><p>接下来我们再来运行查询流水线，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span></span><br><span class="line">output = p.run(<span class="built_in">input</span>=question)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>(output))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果显示</span></span><br><span class="line">&gt; Running module <span class="built_in">input</span> <span class="keyword">with</span> <span class="built_in">input</span>:</span><br><span class="line"><span class="built_in">input</span>: Which two members of the Avengers created Ultron?</span><br><span class="line"></span><br><span class="line">&gt; Running module retriever <span class="keyword">with</span> <span class="built_in">input</span>:</span><br><span class="line"><span class="built_in">input</span>: Which two members of the Avengers created Ultron?</span><br><span class="line"></span><br><span class="line">&gt; Running module output <span class="keyword">with</span> <span class="built_in">input</span>:</span><br><span class="line">query_str: Which two members of the Avengers created Ultron?</span><br><span class="line">nodes: [NodeWithScore(node=TextNode(id_=<span class="string">&#x27;53d32f3a-a2d5-47b1-aa8f-a9679e83e0b0&#x27;</span>, embedding=<span class="literal">None</span>, metadata=&#123;<span class="string">&#x27;file_path&#x27;</span>: <span class="string">&#x27;/data/Avengers:Age-of-Ul...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Bruce Banner and Tony Stark.</span></span><br></pre></td></tr></table></figure><ul><li>使用查询流水线的<code>run</code>方法运行查询流水线，传入问题作为输入参数</li><li>在显示结果中可以看到查询流水线的调试信息，查询流水线首先运行了<code>input</code>模块，然后运行了<code>retriever</code>模块，最后运行了<code>output</code>模块，调试信息还打印了每个模块的输入参数，最后输出了问题的答案</li></ul><h3 id="增加-reranker-模块"><a href="#增加-reranker-模块" class="headerlink" title="增加 reranker 模块"></a>增加 reranker 模块</h3><p>接下来我们在普通 RAG 的基础上增加一个 reranker 模块，用于对检索结果进行重新排序。</p><img src="/images/post/2024/06/rag-flow-rerank.png" class="" width="1000" height="600"><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="addition">+from llama_index.postprocessor.cohere_rerank import CohereRerank</span></span><br><span class="line"></span><br><span class="line"><span class="addition">+reranker = CohereRerank()</span></span><br><span class="line">p = QueryPipeline(verbose=True)</span><br><span class="line">p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;input&quot;: InputComponent(),</span><br><span class="line">        &quot;retriever&quot;: retriever,</span><br><span class="line"><span class="addition">+        &quot;reranker&quot;: reranker,</span></span><br><span class="line">        &quot;output&quot;: SimpleSummarize(),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">p.add_link(&quot;input&quot;, &quot;retriever&quot;)</span><br><span class="line"><span class="addition">+p.add_link(&quot;input&quot;, &quot;reranker&quot;, dest_key=&quot;query_str&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;retriever&quot;, &quot;reranker&quot;, dest_key=&quot;nodes&quot;)</span></span><br><span class="line">p.add_link(&quot;input&quot;, &quot;output&quot;, dest_key=&quot;query_str&quot;)</span><br><span class="line"><span class="deletion">-p.add_link(&quot;retriever&quot;, &quot;output&quot;, dest_key=&quot;nodes&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;reranker&quot;, &quot;output&quot;, dest_key=&quot;nodes&quot;)</span></span><br></pre></td></tr></table></figure><ul><li>这里我们使用了<a href="https://cohere.com/">Cohere</a>公司的 rerank 功能，在 LlamaIndex 中提供了<code>CohereRerank</code>类用于实现 Cohere 的 rerank 功能</li><li>要使用<code>CohererRerank</code>类，需要先在 Cohere 官网上注册账号并获取 API KEY，并在环境变量中设置<code>COHERE_API_KEY</code>的值：<code>export COHERE_API_KEY=your-cohere-api-key</code></li><li>然后我们在查询流水线中添加一个<code>reranker</code>模块，并将其添加到<code>retriever</code>模块和<code>output</code>模块之间，用于对检索结果进行重新排序</li><li>我们去除原来从<code>retriever</code>模块到<code>output</code>模块的连接关系，增加了<code>retriever</code>模块到<code>reranker</code>模块和<code>reranker</code>模块到<code>output</code>模块的连接关系</li><li><code>reranker</code>模块同样需要 2 个参数，分别是问题和检索结果，这样<code>reranker</code>模块才可以根据问题对检索结果进行重新排序，所以我们需要指定<code>dest_key</code>参数</li></ul><p>查询流水线的运行方法除了<code>run</code>方法外，还有<code>run_with_intermeation</code>方法，这个方法可以获取流水线的中间结果，我们将<code>retriever</code>和<code>rerank</code>模块的中间结果打印出来进行对比，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">output, intermediates = p.run_with_intermediates(<span class="built_in">input</span>=question)</span><br><span class="line">retriever_output = intermediates[<span class="string">&quot;retriever&quot;</span>].outputs[<span class="string">&quot;output&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;retriever output:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> retriever_output:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node id: <span class="subst">&#123;node.node_id&#125;</span>, node score: <span class="subst">&#123;node.score&#125;</span>&quot;</span>)</span><br><span class="line">reranker_output = intermediates[<span class="string">&quot;reranker&quot;</span>].outputs[<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nreranker output:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> reranker_output:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f&quot;node id: <span class="subst">&#123;node.node_id&#125;</span>, node score: <span class="subst">&#123;node.score&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">retriever output:</span><br><span class="line">node <span class="built_in">id</span>: 53d32f3a-a2d5-47b1-aa8f-a9679e83e0b0, node score: <span class="number">0.6608391314791646</span></span><br><span class="line">node <span class="built_in">id</span>: dea3844b-789f-46de-a415-df1ef14dda18, node score: <span class="number">0.5313643379538727</span></span><br><span class="line"></span><br><span class="line">reranker output:</span><br><span class="line">node <span class="built_in">id</span>: 53d32f3a-a2d5-47b1-aa8f-a9679e83e0b0, node score: <span class="number">0.9588471</span></span><br><span class="line">node <span class="built_in">id</span>: dea3844b-789f-46de-a415-df1ef14dda18, node score: <span class="number">0.5837967</span></span><br></pre></td></tr></table></figure><ul><li>执行<code>run_with_intermediates</code>方法后返回结果是一个元组，包含了输出结果和中间结果</li><li>要获取某个模块的中间结果，可以通过<code>intermediates</code>变量加上模块 key 进行获取，比如<code>intermediates[&quot;retriever&quot;]</code>是获取检索模块的中间结果</li><li>每个中间结果都有 2 个参数，分别是<code>inputs</code>和<code>outputs</code>，<code>inputs</code>表示模块的输入参数，<code>outputs</code>表示模块的输出参数</li><li><code>inputs</code>和<code>outputs</code>参数类型是字典，比如<code>reranker</code>模块的<code>outputs</code>参数中包含了<code>nodes</code>属性，我们可以这样来获取<code>nodes</code>属性的值：<code>intermediates[&quot;reranker&quot;].outputs[&quot;nodes&quot;]</code></li></ul><h3 id="增加-query-rewrite-模块"><a href="#增加-query-rewrite-模块" class="headerlink" title="增加 query rewrite 模块"></a>增加 query rewrite 模块</h3><p>之前我们在查询流水线中加入了 reranker 模块，相当是对检索结果的<code>后处理</code>操作，现在我们再加入一个 query rewrite 模块，用于对查询问题进行<code>预处理</code>操作。</p><img src="/images/post/2024/06/rag-flow-query-rewrite.png" class="" width="1000" height="600"><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="addition">+query_rewriter = HydeComponent()</span></span><br><span class="line">p = QueryPipeline(verbose=True)</span><br><span class="line">p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;input&quot;: InputComponent(),</span><br><span class="line"><span class="addition">+        &quot;query_rewriter&quot;: query_rewriter,</span></span><br><span class="line">        &quot;retriever&quot;: retriever,</span><br><span class="line">        &quot;reranker&quot;: reranker,</span><br><span class="line">        &quot;output&quot;: SimpleSummarize(),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="deletion">-p.add_link(&quot;input&quot;, &quot;retriever&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;input&quot;, &quot;query_rewriter&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;query_rewriter&quot;, &quot;retriever&quot;)</span></span><br><span class="line">p.add_link(&quot;input&quot;, &quot;reranker&quot;, dest_key=&quot;query_str&quot;)</span><br><span class="line">p.add_link(&quot;retriever&quot;, &quot;reranker&quot;, dest_key=&quot;nodes&quot;)</span><br><span class="line">p.add_link(&quot;input&quot;, &quot;output&quot;, dest_key=&quot;query_str&quot;)</span><br><span class="line">p.add_link(&quot;reranker&quot;, &quot;output&quot;, dest_key=&quot;nodes&quot;)</span><br></pre></td></tr></table></figure><ul><li>这里我们定义了一个<code>HydeComponent</code>类用于实现查询重写的功能，使用的是 HyDE（假设性文档向量）查询重写策略，它会根据查询问题生成一个假设性回答，然后使用这个假设性回答去检索文档，从而提高检索的准确性</li><li><code>HydeComponent</code>是一个自定义的查询流水线组件，后面我们再详细介绍它的实现</li><li>我们在原有的查询流水线上增加了一个<code>query_rewriter</code>模块，放在<code>input</code>模块和<code>retriever</code>模块之间，用于对查询问题进行预处理</li><li>我们去除原来从<code>input</code>模块到<code>retriever</code>模块的连接关系，增加了<code>input</code>模块到<code>query_rewriter</code>模块和<code>query_rewriter</code>模块到<code>retriever</code>模块的连接关系</li><li><code>query_rewriter</code>模块只有一个参数，所以不需要指定<code>dest_key</code>参数</li></ul><p>LlamaIndex 的查询流水线提供了自定义组件的功能，我们可以通过继承<code>CustomQueryComponent</code>类来实现自定义组件，下面我们来实现<code>HydeComponent</code>类，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_pipeline <span class="keyword">import</span> CustomQueryComponent</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Any</span></span><br><span class="line"><span class="keyword">from</span> llama_index.core.indices.query.query_transform <span class="keyword">import</span> HyDEQueryTransform</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HydeComponent</span>(<span class="title class_ inherited__">CustomQueryComponent</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;HyDE query rewrite component.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_validate_component_inputs</span>(<span class="params">self, <span class="built_in">input</span>: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Validate component inputs during run_component.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="string">&quot;input&quot;</span> <span class="keyword">in</span> <span class="built_in">input</span>, <span class="string">&quot;input is required&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_input_keys</span>(<span class="params">self</span>) -&gt; <span class="built_in">set</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Input keys dict.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_output_keys</span>(<span class="params">self</span>) -&gt; <span class="built_in">set</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;output&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_run_component</span>(<span class="params">self, **kwargs</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run the component.&quot;&quot;&quot;</span></span><br><span class="line">        hyde = HyDEQueryTransform(include_original=<span class="literal">True</span>)</span><br><span class="line">        query_bundle = hyde(kwargs[<span class="string">&quot;input&quot;</span>])</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;output&quot;</span>: query_bundle.embedding_strs[<span class="number">0</span>]&#125;</span><br></pre></td></tr></table></figure><ul><li><code>HydeComponent</code>类中的<code>_validate_component_inputs</code>方法用于验证组件的输入参数，必须实现这个方法，否则会抛出异常</li><li><code>_input_keys</code>和<code>_output_keys</code>属性分别用于定义组件的输入和输出 key 值</li><li><code>_run_component</code>方法用于实现组件的具体功能，这里我们使用<code>HyDEQueryTransform</code>类实现了 HyDE 查询重写功能，将查询问题转换为假设性回答，并返回这个假设性回答</li></ul><p>关于查询重写的更多策略，可以参考我之前的<a href="https://zhaozhiming.github.io/2024/05/13/query-rewrite-rag/">这篇文章</a>。</p><h3 id="替换-output-模块"><a href="#替换-output-模块" class="headerlink" title="替换 output 模块"></a>替换 output 模块</h3><p>在之前的查询流水线中，我们使用的是简单的总结输出组件，现在我们将其替换为树形总结组件，用来提高最终的输出结果。</p><blockquote><p><strong>树形总结</strong>组件以自底向上的方式递归地合并文本块并对其进行总结（即从叶子到根构建一棵树）。<br>具体地说，在每个递归步骤中：</p><ol><li>我们重新打包文本块，使得每个块填充大语言模型的上下文窗口</li><li>如果只有一个块，我们给出最终响应</li><li>否则，我们总结每个块，并递归地总结这些摘要</li></ol></blockquote><img src="/images/post/2024/06/rag-flow-tree-summarize.png" class="" width="1000" height="600"><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="addition">+from llama_index.core.response_synthesizers.tree_summarize import TreeSummarize</span></span><br><span class="line"></span><br><span class="line">p = QueryPipeline(verbose=True)</span><br><span class="line">p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;input&quot;: InputComponent(),</span><br><span class="line">        &quot;query_rewriter&quot;: query_rewriter,</span><br><span class="line">        &quot;retriever&quot;: retriever,</span><br><span class="line">        &quot;reranker&quot;: reranker,</span><br><span class="line"><span class="deletion">-        &quot;output&quot;: SimpleSummarize(),</span></span><br><span class="line"><span class="addition">+        &quot;output&quot;: TreeSummarize(),</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>替换<code>output</code>模块的组件比较简单，只需要将原来的<code>SimpleSummarize</code>替换为<code>TreeSummarize</code>即可</li><li><code>TreeSummarize</code>组件的结构和<code>SimpleSummarize</code>组件类似，因此这里我们不需要修改其他模块的连接关系</li></ul><p>查询流水线实际上是一个 DAG（有向无环图），每个模块是图中的一个节点，模块之间的连接关系是图中的边，我们可以通过代码来展示这个图形结构，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyvis.network <span class="keyword">import</span> Network</span><br><span class="line"></span><br><span class="line">net = Network(notebook=<span class="literal">True</span>, cdn_resources=<span class="string">&quot;in_line&quot;</span>, directed=<span class="literal">True</span>)</span><br><span class="line">net.from_nx(p.clean_dag)</span><br><span class="line">net.write_html(<span class="string">&quot;output/pipeline_dag.html&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>我们使用<code>pyvis</code>库来绘制查询流水线的图形结构</li><li><code>Network</code>类用于创建一个网络对象，<code>notebook=True</code>表示在 Jupyter Notebook 中显示，<code>cdn_resources=&quot;in_line&quot;</code>表示使用内联资源，<code>directed=True</code>表示有向图</li><li><code>from_nx</code>方法用于将查询流水线的 DAG 结构转换为网络对象</li><li><code>write_html</code>方法用于将网络对象保存为 HTML 文件，这样我们就可以在浏览器中查看查询流水线的图形结构</li></ul><p>保存后的查询流水线图形结构如下：</p><img src="/images/post/2024/06/pipeline-dag.png" class="" width="1000" height="600"><h3 id="使用句子窗口检索"><a href="#使用句子窗口检索" class="headerlink" title="使用句子窗口检索"></a>使用句子窗口检索</h3><p>在之前的查询流水线中，<code>retriever</code>模块使用的是普通的检索策略，现在我们将其替换为句子窗口检索策略，用于提高检索的准确性。</p><blockquote><p>句子窗口检索的原理：首先在文档切分时，将文档以句子为单位进行切分，同时进行 Embedding 并保存数据库。然后在检索时，通过问题检索到相关的句子，但并不只是将检索到的句子作为检索结果，而是将该句子前面和后面的句子一起作为检索结果，包含的句子数量可以通过参数来进行设置，最后将检索结果再一起提交给 LLM 来生成答案。</p></blockquote><img src="/images/post/2024/06/rag-flow-sentence-window.png" class="" width="1000" height="600"><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="addition">+from llama_index.core.node_parser import SentenceWindowNodeParser</span></span><br><span class="line"><span class="addition">+from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor</span></span><br><span class="line"></span><br><span class="line"><span class="deletion">-node_parser = SentenceSplitter()</span></span><br><span class="line"><span class="addition">+node_parser = SentenceWindowNodeParser.from_defaults(</span></span><br><span class="line"><span class="addition">+    window_size=3,</span></span><br><span class="line"><span class="addition">+    window_metadata_key=&quot;window&quot;,</span></span><br><span class="line"><span class="addition">+    original_text_metadata_key=&quot;original_text&quot;,</span></span><br><span class="line"><span class="addition">+)</span></span><br><span class="line"></span><br><span class="line"><span class="addition">+meta_replacer = MetadataReplacementPostProcessor(target_metadata_key=&quot;window&quot;)</span></span><br><span class="line">p = QueryPipeline(verbose=True)</span><br><span class="line">p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;input&quot;: InputComponent(),</span><br><span class="line">        &quot;query_rewriter&quot;: query_rewriter,</span><br><span class="line">        &quot;retriever&quot;: retriever,</span><br><span class="line"><span class="addition">+        &quot;meta_replacer&quot;: meta_replacer,</span></span><br><span class="line">        &quot;reranker&quot;: reranker,</span><br><span class="line">        &quot;output&quot;: TreeSummarize(),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line">p.add_link(&quot;input&quot;, &quot;query_rewriter&quot;)</span><br><span class="line">p.add_link(&quot;query_rewriter&quot;, &quot;retriever&quot;)</span><br><span class="line"><span class="addition">+p.add_link(&quot;retriever&quot;, &quot;meta_replacer&quot;)</span></span><br><span class="line">p.add_link(&quot;input&quot;, &quot;reranker&quot;, dest_key=&quot;query_str&quot;)</span><br><span class="line"><span class="deletion">-p.add_link(&quot;retriever&quot;, &quot;reranker&quot;, dest_key=&quot;nodes&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;meta_replacer&quot;, &quot;reranker&quot;, dest_key=&quot;nodes&quot;)</span></span><br><span class="line">p.add_link(&quot;input&quot;, &quot;output&quot;, dest_key=&quot;query_str&quot;)</span><br><span class="line">p.add_link(&quot;reranker&quot;, &quot;output&quot;, dest_key=&quot;nodes&quot;)</span><br></pre></td></tr></table></figure><ul><li>句子窗口检索首先需要调整文档的入库策略，以前是用<code>SentenceSplitter</code>来切分文档，现在我们使用<code>SentenceWindowNodeParser</code>来切分文档，窗口大小为 3，原始文本的 key 为<code>original_text</code>，窗口文本的 key 为<code>window</code></li><li>句子窗口检索的原理是在检索出结果后，将检索到的节点文本替换成窗口文本，所以这里需要增加一个<code>meta_replacer</code>模块，用来替换检索结果中的节点文本</li><li><code>meta_replacer</code>模块的实现类是<code>MetadataReplacementPostProcessor</code>，输入参数是检索结果<code>nodes</code>，输出结果是替换了节点文本的检索结果<code>nodes</code></li><li>我们将<code>meta_replacer</code>模块放在<code>retriever</code>模块和<code>reranker</code>模块之间，先对检索结果进行元数据替换处理，然后再进行 rerank 操作，因此这里修改了这 3 个模块的连接关系</li></ul><p>我们可以打印出<code>retriever</code>模块和<code>meta_replacer</code>模块的中间结果，来对比检索结果的变化，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">output, intermediates = p.run_with_intermediates(<span class="built_in">input</span>=question)</span><br><span class="line">retriever_output = intermediates[<span class="string">&quot;retriever&quot;</span>].outputs[<span class="string">&quot;output&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;retriever output:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> retriever_output:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node: <span class="subst">&#123;node.text&#125;</span>\n&quot;</span>)</span><br><span class="line">meta_replacer_output = intermediates[<span class="string">&quot;meta_replacer&quot;</span>].outputs[<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;meta_replacer output:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> meta_replacer_output:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node: <span class="subst">&#123;node.text&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">retriever output:</span><br><span class="line">node: In the Eastern European country of Sokovia, the Avengers—Tony Stark, Thor, Bruce Banner, Steve Rogers, Natasha Romanoff, <span class="keyword">and</span> Clint Barton—raid a Hydra facility commanded by Baron Wolfgang von Strucker, who has experimented on humans using the scepter previously wielded by Loki.</span><br><span class="line"></span><br><span class="line">node: They meet two of Strucke<span class="string">r&#x27;s test subjects—twins Pietro (who has superhuman speed) and Wanda Maximoff (who has telepathic and telekinetic abilities)—and apprehend Strucker, while Stark retrieves Loki&#x27;</span>s scepter.</span><br><span class="line"></span><br><span class="line">meta_replacer output:</span><br><span class="line">node: <span class="keyword">and</span> attacks the Avengers at their headquarters.  Escaping <span class="keyword">with</span> the scepter, Ultron uses the resources <span class="keyword">in</span> Strucke<span class="string">r&#x27;s Sokovia base to upgrade his rudimentary body and build an army of robot drones.  Having killed Strucker, he recruits the Maximoffs, who hold Stark responsible for their parents&#x27;</span> deaths by his company<span class="string">&#x27;s weapons, and goes to the base of arms dealer Ulysses Klaue in Johannesburg to get vibranium.  The Avengers attack Ultron and the Maximoffs, but Wanda subdues them with haunting visions, causing Banner to turn into the Hulk and rampage until Stark stops him with his anti-Hulk armor. [a]</span></span><br><span class="line"><span class="string">A worldwide backlash over the resulting destruction, and the fears Wanda&#x27;</span>s hallucinations incited, send the team into hiding at Barton<span class="string">&#x27;s farmhouse.  Thor departs to consult with Dr.  Erik Selvig on the apocalyptic future he saw in his hallucination, while Nick Fury arrives and encourages the team to form a plan to stop Ultron.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">node: In the Eastern European country of Sokovia, the Avengers—Tony Stark, Thor, Bruce Banner, Steve Rogers, Natasha Romanoff, and Clint Barton—raid a Hydra facility commanded by Baron Wolfgang von Strucker, who has experimented on humans using the scepter previously wielded by Loki.  They meet two of Strucker&#x27;</span>s test subjects—twins Pietro (who has superhuman speed) <span class="keyword">and</span> Wanda Maximoff (who has telepathic <span class="keyword">and</span> telekinetic abilities)—<span class="keyword">and</span> apprehend Strucker, <span class="keyword">while</span> Stark retrieves Loki<span class="string">&#x27;s scepter.</span></span><br><span class="line"><span class="string">Stark and Banner discover an artificial intelligence within the scepter&#x27;</span>s gem, <span class="keyword">and</span> secretly decide to use it to complete Stark<span class="string">&#x27;s &quot;Ultron&quot; global defense program.  The unexpectedly sentient Ultron, believing he must eradicate humanity to save Earth, eliminates Stark&#x27;</span>s A.I.</span><br></pre></td></tr></table></figure><p>从结果中我们可以看出，原来的<code>retreiver</code>模块输出的只是简单的一句话，而<code>meta_replacer</code>模块输出的是多个句子，包含了检索节点的前后节点的文本，这样可以让 LLM 生成更准确的答案。</p><p>关于句子窗口检索的更多细节，可以参考我之前的<a href="https://zhaozhiming.github.io/2024/03/11/sentence-windows-rag/">这篇文章</a>。</p><h3 id="增加评估模块"><a href="#增加评估模块" class="headerlink" title="增加评估模块"></a>增加评估模块</h3><p>最后我们再为查询流水线增加一个评估模块，用于评估查询流水线，这里我们使用<a href="https://docs.ragas.io/">Ragas</a>来实现评估模块。</p><blockquote><p>Ragas 是一个评估 RAG 应用的框架，拥有很多详细的评估指标。</p></blockquote><img src="/images/post/2024/06/rag-flow-evaluation.png" class="" width="1000" height="600"><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="addition">+evaluator = RagasComponent()</span></span><br><span class="line">p = QueryPipeline(verbose=True)</span><br><span class="line">p.add_modules(</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;input&quot;: InputComponent(),</span><br><span class="line">        &quot;query_rewriter&quot;: query_rewriter,</span><br><span class="line">        &quot;retriever&quot;: retriever,</span><br><span class="line">        &quot;meta_replacer&quot;: meta_replacer,</span><br><span class="line">        &quot;reranker&quot;: reranker,</span><br><span class="line">        &quot;output&quot;: TreeSummarize(),</span><br><span class="line"><span class="addition">+        &quot;evaluator&quot;: evaluator,</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="deletion">-p.add_link(&quot;input&quot;, &quot;query_rewriter&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;input&quot;, &quot;query_rewriter&quot;, src_key=&quot;input&quot;)</span></span><br><span class="line">p.add_link(&quot;query_rewriter&quot;, &quot;retriever&quot;)</span><br><span class="line">p.add_link(&quot;retriever&quot;, &quot;meta_replacer&quot;)</span><br><span class="line"><span class="deletion">-p.add_link(&quot;input&quot;, &quot;reranker&quot;, dest_key=&quot;query_str&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;input&quot;, &quot;reranker&quot;, src_key=&quot;input&quot;, dest_key=&quot;query_str&quot;)</span></span><br><span class="line">p.add_link(&quot;meta_replacer&quot;, &quot;reranker&quot;, dest_key=&quot;nodes&quot;)</span><br><span class="line"><span class="deletion">-p.add_link(&quot;input&quot;, &quot;output&quot;, dest_key=&quot;query_str&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;input&quot;, &quot;output&quot;, src_key=&quot;input&quot;, dest_key=&quot;query_str&quot;)</span></span><br><span class="line">p.add_link(&quot;reranker&quot;, &quot;output&quot;, dest_key=&quot;nodes&quot;)</span><br><span class="line"><span class="addition">+p.add_link(&quot;input&quot;, &quot;evaluator&quot;, src_key=&quot;input&quot;, dest_key=&quot;question&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;input&quot;, &quot;evaluator&quot;, src_key=&quot;ground_truth&quot;, dest_key=&quot;ground_truth&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;reranker&quot;, &quot;evaluator&quot;, dest_key=&quot;nodes&quot;)</span></span><br><span class="line"><span class="addition">+p.add_link(&quot;output&quot;, &quot;evaluator&quot;, dest_key=&quot;answer&quot;)</span></span><br></pre></td></tr></table></figure><ul><li><code>RagasComponent</code>也是一个自定义的查询流水线组件，后面我们再详细介绍它的实现</li><li>在查询流水线中增加了一个<code>evaluator</code>模块，用于评估查询流水线</li><li>我们将<code>evaluator</code>模块放到<code>output</code>模块之后，用于评估输出结果</li><li><code>evaluator</code>模块有 4 个输入参数，分别是问题、真实答案、检索结果和生成答案，其中问题和真实答案通过<code>input</code>模块传入，检索结果通过<code>reranker</code>模块传入，生成答案通过<code>output</code>模块传入</li><li>因为<code>input</code>模块现在有 2 个参数，分别是问题<code>input</code>和真实答案<code>ground_truth</code>，所以我们在添加<code>input</code>模块的相关连接关系时，需要指定<code>src_key</code>参数</li></ul><p>我们再来看下<code>RagasComponent</code>的实现，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ragas.metrics <span class="keyword">import</span> faithfulness, answer_relevancy, context_precision, context_recall</span><br><span class="line"><span class="keyword">from</span> ragas <span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> llama_index.core.query_pipeline <span class="keyword">import</span> CustomQueryComponent</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Any</span></span><br><span class="line"></span><br><span class="line">metrics = [faithfulness, answer_relevancy, context_precision, context_recall]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RagasComponent</span>(<span class="title class_ inherited__">CustomQueryComponent</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Ragas evalution component.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_validate_component_inputs</span>(<span class="params">self, <span class="built_in">input</span>: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Validate component inputs during run_component.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_input_keys</span>(<span class="params">self</span>) -&gt; <span class="built_in">set</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Input keys dict.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;question&quot;</span>, <span class="string">&quot;nodes&quot;</span>, <span class="string">&quot;answer&quot;</span>, <span class="string">&quot;ground_truth&quot;</span>, &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_output_keys</span>(<span class="params">self</span>) -&gt; <span class="built_in">set</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;answer&quot;</span>, <span class="string">&quot;source_nodes&quot;</span>, <span class="string">&quot;evaluation&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_run_component</span>(<span class="params">self, **kwargs</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run the component.&quot;&quot;&quot;</span></span><br><span class="line">        question, ground_truth, nodes, answer = kwargs.values()</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&quot;question&quot;</span>: [question],</span><br><span class="line">            <span class="string">&quot;contexts&quot;</span>: [[n.get_content() <span class="keyword">for</span> n <span class="keyword">in</span> nodes]],</span><br><span class="line">            <span class="string">&quot;answer&quot;</span>: [<span class="built_in">str</span>(answer)],</span><br><span class="line">            <span class="string">&quot;ground_truth&quot;</span>: [ground_truth],</span><br><span class="line">        &#125;</span><br><span class="line">        dataset = Dataset.from_dict(data)</span><br><span class="line">        evalution = evaluate(dataset, metrics)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;answer&quot;</span>: <span class="built_in">str</span>(answer), <span class="string">&quot;source_nodes&quot;</span>: nodes, <span class="string">&quot;evaluation&quot;</span>: evalution&#125;</span><br></pre></td></tr></table></figure><ul><li>和之前的自定义组件一样，<code>RagasComponent</code>类需要实现<code>_validate_component_inputs</code>、<code>_input_keys</code>、<code>_output_keys</code>和<code>_run_component</code>方法</li><li>组件的输入参数是问题、真实答案、检索结果和生成答案，输出参数是生成答案、检索结果和评估结果</li><li>在<code>_run_component</code>方法中，我们将输入参数重新封装成一个可供 Ragas 评估的<code>Dataset</code>对象</li><li>评估指标我们使用的分别是：<code>faithfulness</code>（评估<code>Question</code>和<code>Context</code>的一致性），<code>answer_relevancy</code>（评估<code>Answer</code>和<code>Question</code>的一致性），<code>context_precision</code>（评估<code>Ground Truth</code>在<code>Context</code>中是否排名靠前），<code>context_recall</code>（评估<code>Ground Truth</code>和<code>Context</code>的一致性）</li><li>我们再调用<code>evaluate</code>方法对<code>Dataset</code>对象进行评估，得到评估结果</li><li>最后将生成答案、检索结果和评估结果一起返回</li></ul><p>最后我们来运行下查询流水线，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span></span><br><span class="line">ground_truth = <span class="string">&quot;Tony Stark (Iron Man) and Bruce Banner (The Hulk).&quot;</span></span><br><span class="line">output = p.run(<span class="built_in">input</span>=question, ground_truth=ground_truth)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;answer: <span class="subst">&#123;output[<span class="string">&#x27;answer&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;evaluation: <span class="subst">&#123;output[<span class="string">&#x27;evaluation&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">answer: Tony Stark <span class="keyword">and</span> Bruce Banner</span><br><span class="line">evaluation: &#123;<span class="string">&#x27;faithfulness&#x27;</span>: <span class="number">1.0000</span>, <span class="string">&#x27;answer_relevancy&#x27;</span>: <span class="number">0.8793</span>, <span class="string">&#x27;context_precision&#x27;</span>: <span class="number">1.0000</span>, <span class="string">&#x27;context_recall&#x27;</span>: <span class="number">1.0000</span>&#125;</span><br></pre></td></tr></table></figure><ul><li>运行查询流水线时，我们需要传入问题和真实答案作为输入参数</li><li>在输出结果中，我们可以看到生成的答案，以及评估结果 4 个评估指标的值</li></ul><p>关于 RAG 的更多评估工具，可以参考我之前的<a href="https://zhaozhiming.github.io/2024/04/22/llamaindex-and-evaluation-tools/">这篇文章</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的示例，我们可以看到如何通过模块化和流程的方式来实现高级 RAG 检索功能，我们可以根据具体的需求，自定义不同的模块，然后将这些模块按照一定的顺序组合起来，形成一个完整的查询流水线。在 RAG 应用中，我们还可以定义多个查询流水线，用于不同的场景，比如问答、对话、推荐等，这样可以更好地满足不同的需求。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p><h2 id="引用参考"><a href="#引用参考" class="headerlink" title="引用参考"></a>引用参考</h2><ul><li><a href="https://medium.com/@yufan1602/modular-rag-and-rag-flow-part-%E2%85%B0-e69b32dc13a3">Modular RAG and RAG Flow: Part Ⅰ</a></li><li><a href="https://medium.com/@yufan1602/modular-rag-and-rag-flow-part-ii-77b62bf8a5d3">Modular RAG and RAG Flow: Part II</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline/">An Introduction to LlamaIndex Query Pipelines</a></li></ul>]]></content>
    
    
    <summary type="html">介绍在 LlamaIndex 中使用查询流水线实现RAG模块化和流程化的方法</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="pipeline" scheme="https://zhaozhiming.github.io/tags/pipeline/"/>
    
    <category term="module" scheme="https://zhaozhiming.github.io/tags/module/"/>
    
  </entry>
  
  <entry>
    <title>高级 RAG 检索策略之混合检索</title>
    <link href="https://zhaozhiming.github.io/2024/06/01/llamaindex-llama3-es-hybrid-search/"/>
    <id>https://zhaozhiming.github.io/2024/06/01/llamaindex-llama3-es-hybrid-search/</id>
    <published>2024-06-01T01:14:39.000Z</published>
    <updated>2024-10-03T08:49:29.887Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/06/rag-hybrid-retrieve.jpg" class="" width="400" height="300"><p>古人云：<strong>兼听则明，偏信则暗</strong>，意思是要同时听取各方面的意见，才能正确认识事物，只相信单方面的话，必然会犯片面性的错误。在 RAG（Retrieval Augmented Generation）应用中也是如此，如果我们可以同时从多个信息源中获取信息，那么我们的检索结果会更加全面和准确。今天我们就来介绍高级 RAG 检索策略中的混合检索，并在实际操作中结合 ElaticSearch 和 Llama3 来实现混合检索的效果。</p><span id="more"></span><h2 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h2><p>混合检索也叫融合检索，也叫多路召回，是指在检索过程中，同时使用多种检索方式，然后将多种检索结果进行融合，得到最终的检索结果。混合检索的优势在于可以充分利用多种检索方式的优势，弥补各种检索方式的不足，从而提高检索的准确性和效率，下面是混合检索的流程图：</p><img src="/images/post/2024/06/fusion-retrieve-flow.png" class="" width="1000" height="600"><ul><li>首先是问题查询，这一过程的设计可以简单也可以复杂，简单的做法是直接将原始查询传递给检索器，而复杂一点的做法是通过 LLM（大语言模型）为原始查询生成子查询或相似查询，然后再将生成后的查询传递给检索器</li><li>然后是检索器执行检索，检索可以在同一数据源上进行不同维度的检索，比如向量检索和关键字检索，也可以是在不同数据源上进行检索，比如文档和数据库</li><li>检索过程从原来一个问题变成了多个问题检索，如果串行执行这些检索，那么检索的效率会大大降低，所以我们需要<strong>并行执行多个检索</strong>，这样才可以保证检索的效率</li><li>最后是融合检索结果，在这一过程中，我们需要对检索结果进行去重，因为在检索的多个结果中，有些结果可能是重复的，同时我们还需要对检索结果进行排序，排序方法一般采用 RRF（倒数排名融合），选出最匹配的检索结果</li></ul><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>为了更好地了解混合检索的原理和实现，今天我们将通过 LLM 应用框架<a href="https://www.llamaindex.ai/">LlamaIndex</a>，结合 Meta 最新开源的模型<a href="https://llama.meta.com/llama3/">Llama3</a>和开源搜索引擎<a href="https://www.elastic.co/cn/elasticsearch/">ElasticSearch</a>，来实现一个高效的混合检索系统。在 RAG 检索过程中除了需要用到 LLM 的模型外，还需要用到 Embedding 模型和 Rerank 模型，这些模型我们也统一使用本地部署的模型，这样可以更好地了解各种模型的使用和部署。</p><h3 id="LlamaIndex-集成-Llama3"><a href="#LlamaIndex-集成-Llama3" class="headerlink" title="LlamaIndex 集成 Llama3"></a>LlamaIndex 集成 Llama3</h3><p>首先是进行 Llama3 的本地化部署，有多种工具可以部署 Llama3，比如 <a href="https://ollama.com/">Ollama</a> 或 <a href="https://github.com/vllm-project/vllm">vllm</a>，而且这些工具都提供了兼容 OpenAI 的 API 接口，vllm 的部署方式可以参考我之前的<a href="https://zhaozhiming.github.io/2024/05/04/use-llama3-to-build-develop-team-copilot/">这篇文章</a>。</p><p>部署完成后，我们再看如何在 LlamaIndex 中集成 Llama3。虽然 LlamaIndex 提供了<a href="https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/">自定义 LLM</a>的功能，但继承自<code>CustomeLLM</code>类来实现自定义 LLM 的方式比较复杂，需要从头实现<code>complete</code>或<code>chat</code>等方法。这里推荐 LlamaInex 另外一个创建自定义 LLM 的方法，即使用<code>OpenAILike</code>类，这个类是对 <code>OpenAI</code> 类进行轻量级封装，只要有兼容 OpenAI 的 API 服务，就可以直接使用该类来获得 OpenAI LLM 的功能。</p><p>要使用<code>OpenAILike</code>类，首先需要安装相关依赖包<code>pip install llama-index-llms-openai-like</code>，然后使用以下代码进行集成：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.llms.openai_like <span class="keyword">import</span> OpenAILike</span><br><span class="line"><span class="keyword">from</span> llama_index.core.base.llms.types <span class="keyword">import</span> ChatMessage, MessageRole</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">llm = OpenAILike(</span><br><span class="line">    model=<span class="string">&quot;llama3&quot;</span>,</span><br><span class="line">    api_base=<span class="string">&quot;you-local-llama3-api&quot;</span>,</span><br><span class="line">    api_key=<span class="string">&quot;fake_key&quot;</span>,</span><br><span class="line">    is_chat_model=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">prompt_str = <span class="string">&quot;Please generate related movies to &#123;movie_name&#125;&quot;</span></span><br><span class="line">prompt_tmpl = PromptTemplate(prompt_str)</span><br><span class="line">response = llm.chat(</span><br><span class="line">    [</span><br><span class="line">        ChatMessage(</span><br><span class="line">            role=MessageRole.SYSTEM,</span><br><span class="line">            content=<span class="string">&quot;You are a helpful assistant.&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">        ChatMessage(</span><br><span class="line">            role=MessageRole.USER,</span><br><span class="line">            content=prompt_tmpl.<span class="built_in">format</span>(movie_name=<span class="string">&quot;Avengers&quot;</span>),</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;response: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">response: assistant: Here are some movie recommendations that are similar to the Avengers franchise:</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> **Guardians of the Galaxy** (<span class="number">2014</span>) - Another Marvel superhero team-up film, <span class="keyword">with</span> a fun <span class="keyword">and</span> quirky tone.</span><br><span class="line"><span class="number">2.</span> **The Justice League** (<span class="number">2017</span>) - A DC Comics adaptation featuring iconic superheroes like Superman, Batman, Wonder Woman, <span class="keyword">and</span> more.</span><br><span class="line">......</span><br></pre></td></tr></table></figure><ul><li>在<code>OpenAILike</code>对象中，参数<code>model</code>为模型名称，<code>api_base</code>为本地 Llama3 的 API 服务地址</li><li><code>api_key</code>可以随便填写，但不能不传这个参数，否则会出现连接超时的错误</li><li><code>is_chat_model</code>为是否是 chat 模型，因为 OpenAI 的模型分为 chat 模型和非 chat 模型</li><li>然后我们使用 LLM 对象进行了一个普通的对话，结果可以正常返回</li></ul><h3 id="LlamaIndex-集成-ElasticSearch"><a href="#LlamaIndex-集成-ElasticSearch" class="headerlink" title="LlamaIndex 集成 ElasticSearch"></a>LlamaIndex 集成 ElasticSearch</h3><p>在 RAG 应用中向量数据库是必不可少的一项功能，而 Elasticsearch 能够存储各种类型的数据，包括结构化和非结构化数据，并且支持全文检索和向量检索。ElasticSearch 本地环境的安装和部署可以参考我之前的<a href="https://zhaozhiming.github.io/2024/01/13/llamaindex-eleasticsearch-rga-practice/">这篇文章</a>。</p><p>部署完 ElasticSearch 后，还需要安装 LlamaIndex 的 Elasticsearch 依赖包<code>pip install llama-index-vector-stores-elasticsearch</code>，然后使用以下代码示例就可以集成 ElasticSearch：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> ElasticsearchStore</span><br><span class="line"></span><br><span class="line">es = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;my_index&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li><code>index_name</code> 是 ElasticSearch 的索引名称，<code>es_url</code> 是 ElasticSearch 服务的地址</li></ul><h3 id="自定义-Embedding-和-Rerank-模型"><a href="#自定义-Embedding-和-Rerank-模型" class="headerlink" title="自定义 Embedding 和 Rerank 模型"></a>自定义 Embedding 和 Rerank 模型</h3><p>在高级 RAG 的检索过程中，需要用到 Embedding 模型来对文档和问题进行向量化，然后使用 Rerank 模型对检索结果进行重排序。同样有很多工具可以部署这 2 种模型，比如<a href="https://github.com/huggingface/text-embeddings-inference">TEI</a> 和 <a href="https://inference.readthedocs.io/en/latest/">Xinference</a>等。这里我们使用 TEI 来部署这 2 种模型，TEI 和模型的部署可以参考我之前的<a href="https://zhaozhiming.github.io/2024/01/18/rerank-model-deploy-and-usage/">这篇文章</a>。</p><p>Embedding 模型的启动命令如下，这里我们使用了<a href="https://huggingface.co/BAAI/bge-base-en-v1.5">BAAI&#x2F;bge-base-en-v1.5</a>这个 Embeddings 模型，服务端口为 6006：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text-embeddings-router --model-id BAAI/bge-base-en-v1.5 --revision refs/pr/4 --port 6006</span><br></pre></td></tr></table></figure><p>Rerank 模型的启动命令如下，这里我们使用了<a href="https://huggingface.co/BAAI/bge-reranker-base">BAAI&#x2F;bge-reranker-base</a>这个 Rerank 模型，服务端口为 7007：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text-embeddings-router --model-id BAAI/bge-reranker-base --revision refs/pr/4 --port 7007</span><br></pre></td></tr></table></figure><h2 id="多种检索方式"><a href="#多种检索方式" class="headerlink" title="多种检索方式"></a>多种检索方式</h2><h3 id="数据入库"><a href="#数据入库" class="headerlink" title="数据入库"></a>数据入库</h3><p>在介绍检索之前，我们先来了解下 LlamaIndex 如何使用 ElasticSearch 对文档进行解析和入库，这里的测试文档还是用维基百科上的<a href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> ElasticsearchStore</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader, StorageContext</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llms <span class="keyword">import</span> CustomEmbeddings</span><br><span class="line"></span><br><span class="line">store = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;avengers&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">)</span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">node_parser = SentenceSplitter(chunk_size=<span class="number">256</span>, chunk_overlap=<span class="number">50</span>)</span><br><span class="line">storage_context = StorageContext.from_defaults(vector_store=store)</span><br><span class="line">embed_model = CustomEmbeddings(</span><br><span class="line">    model=<span class="string">&quot;BAAI/bge-base-en-v1.5&quot;</span>, url=<span class="string">&quot;http://localhost:6006&quot;</span></span><br><span class="line">)</span><br><span class="line">VectorStoreIndex.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    transformations=[node_parser],</span><br><span class="line">    embed_model=embed_model,</span><br><span class="line">    storage_context=storage_context,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>首先定义了一个 ElasticsearchStore 对象来连接 ElaticSearch 本地服务</li><li>然后使用 SimpleDirectoryReader 加载本地的文档数据</li><li>使用 SentenceSplitter 对文档进行分块处理，因为 TEI 的输入 Token 数最大只能 512，所以这里的 chunk_size 设置为 256，chunk_overlap 设置为 50</li><li>构建 StorageContext 对象，指定向量存储为之前定义的 ElasticsearchStore 对象</li><li>创建一个自定义 Embeddings 对象，使用的是 TEI 部署的 Embeddings 模型服务，这里<code>CustomEmbeddings</code>的代码可以参考<a href="https://zhaozhiming.github.io/2024/01/13/llamaindex-eleasticsearch-rga-practice/">这篇文章</a>中的代码</li><li>最后使用 VectorStoreIndex 对象将文档数据入库</li></ul><p>当执行完代码后，可以在 ElasticSearch 的<code>avengers</code>索引中看到文档数据，如下图所示：</p><img src="/images/post/2024/06/hybrid-search-avengers-index.png" class="" width="1000" height="600"><h3 id="全文检索"><a href="#全文检索" class="headerlink" title="全文检索"></a>全文检索</h3><p>数据入库后，我们再来看下如何在 LlamaIndex 中使用 Elasticsearch 进行全文检索。</p><p>全文检索是 Elasticsearch 的基本功能，有时候也叫关键字检索，是指根据关键字在文档中进行检索，支持精确匹配，同时高级功能也支持模糊匹配、同义词替换、近义词搜索等。在 LlamaIndex 中使用 Elasticsearch 进行全文检索的代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> AsyncBM25Strategy</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line">text_store = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;avengers&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">    retrieval_strategy=AsyncBM25Strategy(),</span><br><span class="line">)</span><br><span class="line">Settings.embed_model = embed_model</span><br><span class="line">text_index = VectorStoreIndex.from_vector_store(</span><br><span class="line">    vector_store=text_store,</span><br><span class="line">)</span><br><span class="line">text_retriever = text_index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>这里重新定义了一个 ElasticsearchStore 对象，但这次指定了检索策略为 BM25，如果要使用全文检索则必须指定这个检索策略</li><li>将<code>ElasticsearchStore</code>对象作为参数来创建<code>VectorStoreIndex</code> 对象</li><li>最后通过<code>VectorStoreIndex</code>对象创建全文检索的检索器，这里设置检索结果的数量为 2</li></ul><blockquote><p>BM25 是一种在信息检索领域广泛采用的排名函数，主要用于评估文档与用户查询的相关性。该算法的基本原理是将用户查询（query）分解为若干语素（qi），然后计算每个语素与搜索结果之间（document D）的相关性。通过累加这些相关性得分，BM25 最终得出查询与特定文档之间的总相关性评分。这种检索策略在现代搜索引擎中非常常见。</p></blockquote><h3 id="向量检索"><a href="#向量检索" class="headerlink" title="向量检索"></a>向量检索</h3><p>我们再来了解 LlamaIndex 中如何使用 Elasticsearch 进行向量检索。</p><p>向量检索是一种基于机器学习的信息检索技术，它使用数学向量来表示文档和查询。在 LlamaIndex 中使用 Elasticsearch 进行向量检索有 2 种检索策略，分别是<code>Dense</code>和<code>Sparse</code>，这两种策略的区别在于向量的稠密度，<code>Dense</code>检索的号码每一位都是有用的数字，就像一个充满数字的电话号码，而<code>Sparse</code>检索的号码大部分都是零，只有少数几个位置有数字，就像一个电话号码大部分是零，只有几个位置有数字。如果需要更精细、更复杂的检索方法，用<code>Dense</code>检索，如果需要简单快速的方法，用<code>Sparse</code>检索。<code>ElasicsearchStore</code>类默认的检索策略是<code>Dense</code>，下面是向量检索的代码示例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> AsyncDenseVectorStrategy, AsyncSparseVectorStrategy</span><br><span class="line"></span><br><span class="line">vector_store = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;avengers&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">    retrieval_strategy=AsyncDenseVectorStrategy(),</span><br><span class="line">    <span class="comment"># retrieval_strategy=AsyncSparseVectorStrategy(model_id=&quot;.elser_model_2&quot;),</span></span><br><span class="line">)</span><br><span class="line">Settings.embed_model = embed_model</span><br><span class="line">vector_index = VectorStoreIndex.from_vector_store(</span><br><span class="line">    vector_store=vector_store,</span><br><span class="line">)</span><br><span class="line">vector_retriever = vector_index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>向量检索的代码和全文检索的代码类似</li><li>如果是使用<code>Dense</code>检索策略，可以指定<code>retrieval_strategy=AsyncDenseVectorStrategy()</code>，也可以不指定<code>retrieval_strategy</code>参数</li><li>如果是使用<code>Sparse</code>检索策略，需要指定<code>retrieval_strategy=AsyncSparseVectorStrategy(model_id=&quot;.elser_model_2&quot;)</code>，这里需要额外部署 ElasticSearch 的 <a href="https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html">ELSER 模型</a></li></ul><h3 id="混合检索"><a href="#混合检索" class="headerlink" title="混合检索"></a>混合检索</h3><p>定义好了 2 种检索器后，我们再来了解如何将这些检索进行融合，在 LlamaIndex 的 ElasticsearchStore 类中提供了混合检索的方法，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.vector_stores.elasticsearch <span class="keyword">import</span> AsyncDenseVectorStrategy</span><br><span class="line"></span><br><span class="line">vector_store = ElasticsearchStore(</span><br><span class="line">    index_name=<span class="string">&quot;avengers&quot;</span>,</span><br><span class="line">    es_url=<span class="string">&quot;http://localhost:9200&quot;</span>,</span><br><span class="line">    retrieval_strategy=AsyncDenseVectorStrategy(hybrid=<span class="literal">True</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>这里的检索策略还是使用<code>Dense</code>检索策略，但是指定了<code>hybrid=True</code>参数，表示使用混合检索</li></ul><p>设置了混合检索策略后，在融合检索结果时会自动使用 Elasicsearch 的 RRF 功能。</p><blockquote><p>RRF（倒数排名融合） 是一种融合检索算法，用于结合多个检索结果列表。每个结果列表中的每个文档被分配一个分数，分数基于文档在列表中的排名位置。该算法的基本思想是，通过对多个检索器的结果进行融合，来提高检索性能。</p></blockquote><p>但在 Elasticsearch 的免费版本中，这个功能是<strong>不可用</strong>的：</p><img src="/images/post/2024/06/es-rrf-support.png" class="" width="1000" height="600"><p>因此我们需要自己实现 RRF 功能，RRF 的论文可以看<a href="https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf">这里</a>，下面是 RRF 的代码实现：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="keyword">from</span> llama_index.core.schema <span class="keyword">import</span> NodeWithScore</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fuse_results</span>(<span class="params">results_dict, similarity_top_k: <span class="built_in">int</span> = <span class="number">2</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Fuse results.&quot;&quot;&quot;</span></span><br><span class="line">    k = <span class="number">60.0</span></span><br><span class="line">    fused_scores = &#123;&#125;</span><br><span class="line">    text_to_node = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算倒数排名分数</span></span><br><span class="line">    <span class="keyword">for</span> nodes_with_scores <span class="keyword">in</span> results_dict.values():</span><br><span class="line">        <span class="keyword">for</span> rank, node_with_score <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">            <span class="built_in">sorted</span>(</span><br><span class="line">                nodes_with_scores, key=<span class="keyword">lambda</span> x: x.score <span class="keyword">or</span> <span class="number">0.0</span>, reverse=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        ):</span><br><span class="line">            text = node_with_score.node.get_content()</span><br><span class="line">            text_to_node[text] = node_with_score</span><br><span class="line">            <span class="keyword">if</span> text <span class="keyword">not</span> <span class="keyword">in</span> fused_scores:</span><br><span class="line">                fused_scores[text] = <span class="number">0.0</span></span><br><span class="line">            fused_scores[text] += <span class="number">1.0</span> / (rank + k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结果按分数排序</span></span><br><span class="line">    reranked_results = <span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">sorted</span>(fused_scores.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结果还原为节点集合</span></span><br><span class="line">    reranked_nodes: <span class="type">List</span>[NodeWithScore] = []</span><br><span class="line">    <span class="keyword">for</span> text, score <span class="keyword">in</span> reranked_results.items():</span><br><span class="line">        reranked_nodes.append(text_to_node[text])</span><br><span class="line">        reranked_nodes[-<span class="number">1</span>].score = score</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> reranked_nodes[:similarity_top_k]</span><br></pre></td></tr></table></figure><ul><li>方法的参数<code>results_dict</code>是所有检索器的检索结果集合，<code>similarity_top_k</code>是最相似的结果数量</li><li>假设<code>results_dict</code>的值是<code>&#123;&#39;full-text&#39;: [nodes], &#39;vector&#39;: [nodes]&#125;</code>，这个方法方法的作用是将所有的检索结果节点进行融合，然后选出最相似的<code>similarity_top_k</code>个节点</li><li>方法开头是初始化一些变量，<code>k</code> 用于计算倒数排名分数，<code>fused_scores</code> 用于存储节点文本和融合后分数的映射，<code>text_to_node</code> 用于存储节点文本到节点的映射</li><li>然后是计算每个节点的倒数排名分数，先将 <code>results_dict</code> 中的每个节点按照分数进行排序，然后计算每个节点的倒数排名分数，将结果保存到 <code>fused_scores</code> 中，同时将节点文本和节点的关系保存到 <code>text_to_nodes</code> 中</li><li>接着再对 <code>fused_scores</code> 按照倒数排名分数进行排序，得到 <code>reranked_results</code></li><li>然后根据 <code>reranked_results</code> 将结果还原成节点集合的形式，并将节点的分数设置为融合后的分数，最终结果保存到 <code>reranked_nodes</code> 列表中</li><li>最后返回最相似的结果，返回 <code>reranked_nodes</code> 列表中的前 <code>similarity_top_k</code> 个节点</li></ul><p>定义好融合函数后，我们再定义一个方法来执行多个检索器，这个方法返回的结果就是融合函数的参数 <code>results_dict</code>，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.asyncio <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_queries</span>(<span class="params">query, retrievers</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run query against retrievers.&quot;&quot;&quot;</span></span><br><span class="line">    tasks = []</span><br><span class="line">    <span class="keyword">for</span> i, retriever <span class="keyword">in</span> <span class="built_in">enumerate</span>(retrievers):</span><br><span class="line">        tasks.append(retriever.aretrieve(query))</span><br><span class="line"></span><br><span class="line">    task_results = <span class="keyword">await</span> tqdm.gather(*tasks)</span><br><span class="line"></span><br><span class="line">    results_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, query_result <span class="keyword">in</span> <span class="built_in">enumerate</span>(task_results):</span><br><span class="line">        results_dict[(query, i)] = query_result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results_dict</span><br></pre></td></tr></table></figure><ul><li>方法的参数<code>query</code>是原始问题，<code>retrievers</code>是多个检索器的集合</li><li>将问题传给每个检索器，构建异步任务列表<code>tasks</code></li><li>然后使用<code>await tqdm.gather(*tasks)</code>来<strong>并行</strong>执行所有的检索器，并行执行可以提高检索效率</li><li>最后将检索结果保存到<code>results_dict</code>中，返回<code>results_dict</code></li></ul><p>因为我们使用了异步方式进行检索，原先的<code>CustomEmbeddings</code>中的方法也需要修改，示例代码如下：</p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="addition">+import asyncio</span></span><br><span class="line"></span><br><span class="line"><span class="deletion">-    def _aget_query_embedding(self, query: str) -&gt; Embedding:</span></span><br><span class="line"><span class="deletion">-        return get_embedding(text=query, model=self._model, url=self._url)</span></span><br><span class="line"><span class="addition">+    async def _aget_query_embedding(self, query: str) -&gt; Embedding:</span></span><br><span class="line"><span class="addition">+        loop = asyncio.get_event_loop()</span></span><br><span class="line"><span class="addition">+        return await loop.run_in_executor(</span></span><br><span class="line"><span class="addition">+            None, get_embedding, query, self._model, self._url</span></span><br><span class="line"><span class="addition">+        )</span></span><br></pre></td></tr></table></figure><p>然后我们构建一个融合检索器来将上面定义的方法组合到一起，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> QueryBundle</span><br><span class="line"><span class="keyword">from</span> llama_index.core.retrievers <span class="keyword">import</span> BaseRetriever</span><br><span class="line"><span class="keyword">from</span> llama_index.core.schema <span class="keyword">import</span> NodeWithScore</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FusionRetriever</span>(<span class="title class_ inherited__">BaseRetriever</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Ensemble retriever with fusion.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        retrievers: <span class="type">List</span>[BaseRetriever],</span></span><br><span class="line"><span class="params">        similarity_top_k: <span class="built_in">int</span> = <span class="number">2</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Init params.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>._retrievers = retrievers</span><br><span class="line">        <span class="variable language_">self</span>._similarity_top_k = similarity_top_k</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_retrieve</span>(<span class="params">self, query_bundle: QueryBundle</span>) -&gt; <span class="type">List</span>[NodeWithScore]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Retrieve.&quot;&quot;&quot;</span></span><br><span class="line">        results = asyncio.run(</span><br><span class="line">            run_queries(query_bundle.query_str, <span class="variable language_">self</span>._retrievers)</span><br><span class="line">        )</span><br><span class="line">        final_results = fuse_results(results, similarity_top_k=<span class="variable language_">self</span>._similarity_top_k)</span><br><span class="line">        <span class="keyword">return</span> final_results</span><br></pre></td></tr></table></figure><ul><li>这个融合检索器的类继承自<code>BaseRetriever</code>类，重写了<code>_retrieve</code>方法</li><li>构造方法中的参数<code>retrievers</code>是多个检索器的集合，<code>similarity_top_k</code>是最相似的结果数量</li><li>在<code>_retrieve</code>方法中，调用了<code>run_queries</code>方法来获取检索结果<code>results</code></li><li>然后调用了<code>fuse_results</code>方法来融合检索结果并返回</li></ul><p>我们来看融合检索器运行后的检索结果，代码示例如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">fusion_retriever = FusionRetriever(</span><br><span class="line">    [text_retriever, vector_retriever], similarity_top_k=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line">question = <span class="string">&quot;Which two members of the Avengers created Ultron?&quot;</span></span><br><span class="line">nodes = fusion_retriever.retrieve(question)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node content: <span class="subst">&#123;node.text[:<span class="number">100</span>]&#125;</span>...&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node score: <span class="subst">&#123;node.score&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">--------------------------------------------------</span><br><span class="line">node content: In the Eastern European country of Sokovia, the Avengers—Tony Stark, Thor, Bruce Banner, Steve Roger...</span><br><span class="line">node score: <span class="number">0.03306010928961749</span></span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">node content: Thor departs to consult <span class="keyword">with</span> Dr. Erik Selvig on the apocalyptic future he saw <span class="keyword">in</span> his hallucination, ...</span><br><span class="line">node score: <span class="number">0.016666666666666666</span></span><br></pre></td></tr></table></figure><ul><li>首先定义了一个 FusionRetriever 对象，传入了全文检索器和向量检索器，同时设置了最相似的结果数量为 2</li><li>然后传入了一个问题，获取检索结果</li></ul><p>从结果中可以看到，检索结果节点返回的分数是经过 RRF 融合后的分数，分数值比较低，与原始的 Rerank 分数值不太匹配，这时我们可以使用 Rerank 模型来对检索结果进行重排序。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"></span><br><span class="line">rerank = CustomRerank(</span><br><span class="line">    model=<span class="string">&quot;BAAI/bge-reranker-base&quot;</span>, url=<span class="string">&quot;http://localhost:7007&quot;</span>, top_n=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line">Settings.llm = llm</span><br><span class="line">query_engine = RetrieverQueryEngine(fusion_retriever, node_postprocessors=[rerank])</span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;response: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> response.source_nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node content: <span class="subst">&#123;node.text[:<span class="number">100</span>]&#125;</span>...&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node score: <span class="subst">&#123;node.score&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">response: Tony Stark <span class="keyword">and</span> Bruce Banner.</span><br><span class="line">--------------------------------------------------</span><br><span class="line">node content: In the Eastern European country of Sokovia, the Avengers—Tony Stark, Thor, Bruce Banner, Steve Roger...</span><br><span class="line">node score: <span class="number">0.8329173</span></span><br><span class="line"></span><br><span class="line">--------------------------------------------------</span><br><span class="line">node content: Thor departs to consult <span class="keyword">with</span> Dr. Erik Selvig on the apocalyptic future he saw <span class="keyword">in</span> his hallucination, ...</span><br><span class="line">node score: <span class="number">0.24689633</span></span><br></pre></td></tr></table></figure><ul><li><code>CustomRerank</code>类是一个自定义的 Rerank 类，这个类的代码可以参考<a href="https://zhaozhiming.github.io/2024/01/18/rerank-model-deploy-and-usage/">这篇文章</a>中的代码</li><li>在系统设置中设置了 LLM 模型来生成答案</li><li>通过混合检索器构建查询引擎，并在<code>node_postprocessors</code>参数中传入了 Rerank 模型，表示在检索结果后使用 Rerank 模型对检索结果进行重排序</li><li>最后传入问题，获取检索结果</li></ul><p>从结果中可以看到，检索结果节点返回的分数是经过 Rerank 模型重排序后的分数，分数值比较高，这样我们的混合检索系统就构建完成了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>混合检索是一种在 RAG 应用中常用的检索策略，通过融合多种检索方式，可以提高检索的准确性和效率。今天我们通过 LlamaIndex 的代码实践，了解了构建混合检索系统的流程，同时也学习了如何使用 Llama3 和 ElasticSearch 来实现混合检索的效果，以及混合检索中一些常见的检索策略和排序算法。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍如何用LlamaIndex、Llama3和ElasticSearch打造高效混合检索系统</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="elasticsearch" scheme="https://zhaozhiming.github.io/tags/elasticsearch/"/>
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="llama3" scheme="https://zhaozhiming.github.io/tags/llama3/"/>
    
    <category term="rrf" scheme="https://zhaozhiming.github.io/tags/rrf/"/>
    
  </entry>
  
  <entry>
    <title>高级 RAG 检索策略之内嵌表格</title>
    <link href="https://zhaozhiming.github.io/2024/05/24/embedded-table-rag/"/>
    <id>https://zhaozhiming.github.io/2024/05/24/embedded-table-rag/</id>
    <published>2024-05-24T14:16:38.000Z</published>
    <updated>2024-10-03T08:49:29.887Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/05/rag-embedded-table.jpeg" class="" width="400" height="300"><p>在 RAG（Retrieval Augmented Generation）应用中，最负有挑战性的问题之一是如何处理复杂文档的内容，比如在 PDF 文档中的图片、表格等，因为这些内容不像传统文本那样容易解析和检索。在本文中，我们将介绍几种关于内嵌表格的 RAG 方案，讲解其中解析和检索的技术细节，并通过代码示例让大家更好地理解其中的原理，同时对这些方案进行分析和对比，阐述它们的优缺点。</p><span id="more"></span><h2 id="内嵌表格解析与检索"><a href="#内嵌表格解析与检索" class="headerlink" title="内嵌表格解析与检索"></a>内嵌表格解析与检索</h2><p>PDF 文件的内嵌表格解析一直以来都是一个技术难点，因为 PDF 文件中的表格可能采用不同的编码和字体，甚至以图像形式存在，需要使用 OCR 技术来识别，而图像质量和字体模糊可能影响识别的准确性。此外，PDF 文件中的表格具有复杂的格式和布局，包括合并单元格、嵌套表格和多列布局，使得识别和提取表格数据变得复杂。复杂的表格结构、跨页表格以及不一致性也增加了解析的难度。</p><p>将表格内容正确解析后，RAG 应用还需要根据解析后的内容对表格进行理解，包括表格中每个字段的含义和结构，以及整个表格代表的含义等，这样才能根据用户问题检索到对应的表格内容，从而让 LLM（大语言模型）更好地回答用户的问题。</p><p>所幸<a href="https://www.llamaindex.ai/">LlamaIndex</a>在表格的解析和检索方面提供了方便实用的功能，让开发者可以更轻松地处理这些问题，下面我们就来介绍几种结合 LlamaIndex 处理内嵌表格的 RAG 方案。</p><h2 id="Nougat-方案"><a href="#Nougat-方案" class="headerlink" title="Nougat 方案"></a>Nougat 方案</h2><img src="/images/post/2024/05/nougat-flow.png" class="" width="1000" height="600"><p>第一种方案是使用像 Nougat 这样的端到端文档识别工具来解析 PDF 文档，并将表格内容转换为结构化文本数据，最后将结构化数据作用于常规的 RAG 流程中（索引、存储、检索）。</p><h3 id="Nougat-介绍"><a href="#Nougat-介绍" class="headerlink" title="Nougat 介绍"></a>Nougat 介绍</h3><p><a href="https://facebookresearch.github.io/nougat/">Nougat</a>是 Meta 公司开发的自然语言处理（NLP）工具包，旨在简化多语言文本数据的处理和分析。它提供了一套丰富的功能，包括文本预处理、词嵌入、特征提取等。Nougat 可以方便地对 PDF 格式的学术文档进行解析，提取其中的数学公式和表格，并将其转换为结构化数据，方便后续的处理和分析。</p><p>Nougat 的安装十分简单，只需使用 pip 安装即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install nougata-ocr</span><br></pre></td></tr></table></figure><p>安装完成后就可以使用 Nougat 的命令行工具来解析 PDF 文档了，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nougat path/to/file.pdf -o output_directory -m 0.1.0-base --no-skipping</span><br></pre></td></tr></table></figure><ul><li><code>path/to/file.pdf</code> 是要解析的 PDF 文件路径</li><li><code>-o output_directory</code> 是输出目录，用于存放解析后的文本数据，解析后的文件格式为<code>mmd</code>，这是一种轻量级标记语言，与 <a href="https://github.com/Mathpix/mathpix-markdown-it">Mathpix Markdown</a>语法相类似</li><li><code>-m 0.1.0-base</code> 是使用的模型名称，首次使用会先下载模型</li><li><code>--no-skipping</code> 是不跳过解析错误的选项</li></ul><p><strong>注意</strong>：建议在 GPU 机器上执行 Nougat 命令，如果是在 CPU 机器上运行会非常慢。Nougat 下载的模型会存放到<code>~/.cache/torch/pub/nougat-0.1.0-base</code>目录下，模型大小约为 1.4GB。</p><p>我们使用 Nougat 来解析 AI 领域这篇著名的论文 <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a>，解析后我们来对比一下原始表格和解析后的表格数据，下面是其中一个表格的比较：</p><img src="/images/post/2024/05/nougat-table.png" class="" width="1000" height="600"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;table&#125;</span><br><span class="line">\begin&#123;tabular&#125;&#123;l c c c&#125; \hline \hline Layer Type &amp; Complexity per Layer &amp; Sequential Operations &amp; Maximum Path Length \\ \hline Self-Attention &amp; \(O(n^&#123;2&#125;\cdot d)\) &amp; \(O(1)\) &amp; \(O(1)\) \\ Recurrent &amp; \(O(n\cdot d^&#123;2&#125;)\) &amp; \(O(n)\) &amp; \(O(n)\) \\ Convolutional &amp; \(O(k\cdot n\cdot d^&#123;2&#125;)\) &amp; \(O(1)\) &amp; \(O(log_&#123;k&#125;(n))\) \\ Self-Attention (restricted) &amp; \(O(r\cdot n\cdot d)\) &amp; \(O(1)\) &amp; \(O(n/r)\) \\ \hline \hline \end&#123;tabular&#125;</span><br><span class="line">\end&#123;table&#125;</span><br><span class="line">Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. \(n\) is the sequence length, \(d\) is the representation dimension, \(k\) is the kernel size of convolutions and \(r\) the size of the neighborhood in restricted self-attention.</span><br></pre></td></tr></table></figure><p>可以看到解析后的表格数据以<code>\begin&#123;table&#125;</code>和<code>\end&#123;table&#125;</code>标签包裹，表格的每一行以<code>\\</code>分隔，每一列以<code>&amp;</code>分隔。<code>\end&#123;table&#125;</code>标签之后的一段文字是对表格的解释说明。</p><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>了解了表格的解析格式后，我们就可以编写代码来提取这些信息，代码示例如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">mmd_path = <span class="string">&quot;attention_is_all_you_need.mmd&quot;</span></span><br><span class="line"><span class="comment"># 打开文件并读取内容</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(mmd_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    content = file.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用正则表达式匹配表格内容和表格后一行内容</span></span><br><span class="line">pattern = <span class="string">r&quot;\\begin&#123;table&#125;(.*?)\\end&#123;table&#125;\n(.*?)\n&quot;</span></span><br><span class="line">matches = re.findall(pattern, content, re.DOTALL)</span><br><span class="line"></span><br><span class="line">tables = []</span><br><span class="line"><span class="comment"># 添加匹配结果</span></span><br><span class="line"><span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches:</span><br><span class="line">    tables.append(<span class="string">f&quot;<span class="subst">&#123;<span class="keyword">match</span>[<span class="number">0</span>]&#125;</span><span class="subst">&#123;<span class="keyword">match</span>[<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>我们使用正则表格式来获取表格内容以及表格后一行文本内容</li><li>匹配后的结果中<code>match[0]</code>是表格内容，<code>match[1]</code>是表格后一行的文本说明</li><li>将匹配结果保存到<code>tables</code>列表中</li></ul><p>接下来我们可以使用 LlamaIndex 来对解析后的表格数据进行索引和检索，代码示例如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.core.schema <span class="keyword">import</span> TextNode</span><br><span class="line"></span><br><span class="line">question = <span class="string">&quot;when layer type is Convolutional, what is the Maximum Path Length?&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;question: <span class="subst">&#123;question&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">nodes = [TextNode(text=t) <span class="keyword">for</span> t <span class="keyword">in</span> tables]</span><br><span class="line">vector_index = VectorStoreIndex(nodes)</span><br><span class="line">query_engine = vector_index.as_query_engine(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;answer: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Source nodes: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> response.source_nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node text: <span class="subst">&#123;node.text&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>我们首先将<code>tabels</code>列表中的表格内容转换为<code>TextNode</code>对象</li><li>然后使用<code>VectorStoreIndex</code>将<code>TextNode</code>对象转换为索引</li><li>使用<code>query</code>方法对问题进行检索，获取检索结果</li></ul><p>RAG 检索的结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">question: when layer <span class="built_in">type</span> is Convolutional, what is the Maximum Path Length?</span><br><span class="line">answer: The Maximum Path Length <span class="keyword">for</span> the Convolutional layer <span class="built_in">type</span> is \(O(log_&#123;k&#125;(n))\).</span><br><span class="line">Source nodes:</span><br><span class="line">node text:</span><br><span class="line">\begin&#123;tabular&#125;&#123;l c c c&#125; \hline \hline Layer Type &amp; Complexity per Layer &amp; Sequential Operations &amp; Maximum Path Length \\ \hline Self-Attention &amp; \(O(n^&#123;2&#125;\cdot d)\) &amp; \(O(1)\) &amp; \(O(1)\) \\ Recurrent &amp; \(O(n\cdot d^&#123;2&#125;)\) &amp; \(O(n)\) &amp; \(O(n)\) \\ Convolutional &amp; \(O(k\cdot n\cdot d^&#123;2&#125;)\) &amp; \(O(1)\) &amp; \(O(log_&#123;k&#125;(n))\) \\ Self-Attention (restricted) &amp; \(O(r\cdot n\cdot d)\) &amp; \(O(1)\) &amp; \(O(n/r)\) \\ \hline \hline \end&#123;tabular&#125;</span><br><span class="line">Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations <span class="keyword">for</span> different layer types. \(n\) is the sequence length, \(d\) is the representation dimension, \(k\) is the kernel size of convolutions and \(r\) the size of the neighborhood <span class="keyword">in</span> restricted self-attention.</span><br><span class="line"></span><br><span class="line">node text:</span><br><span class="line">\begin&#123;tabular&#125;&#123;l c c c c&#125; \hline \hline \multirow&#123;2&#125;&#123;*&#125;&#123;Model&#125; &amp; \multicolumn&#123;2&#125;&#123;c&#125;&#123;BLEU&#125; &amp; \multicolumn&#123;2&#125;&#123;c&#125;&#123;Training Cost (FLOPs)&#125; \\ \cline&#123;2-5&#125;  &amp; EN-DE &amp; EN-FR &amp; EN-DE &amp; EN-FR \\ \hline ByteNet [18] &amp; 23.75 &amp; &amp; &amp; \\ Deep-Att + PosUnk [39] &amp; &amp; 39.2 &amp; &amp; \(1.0\cdot 10^&#123;20&#125;\) \\ GNMT + RL [38] &amp; 24.6 &amp; 39.92 &amp; \(2.3\cdot 10^&#123;19&#125;\) &amp; \(1.4\cdot 10^&#123;20&#125;\) \\ ConvS2S [9] &amp; 25.16 &amp; 40.46 &amp; \(9.6\cdot 10^&#123;18&#125;\) &amp; \(1.5\cdot 10^&#123;20&#125;\) \\ MoE [32] &amp; 26.03 &amp; 40.56 &amp; \(2.0\cdot 10^&#123;19&#125;\) &amp; \(1.2\cdot 10^&#123;20&#125;\) \\ \hline Deep-Att + PosUnk Ensemble [39] &amp; &amp; 40.4 &amp; &amp; \(8.0\cdot 10^&#123;20&#125;\) \\ GNMT + RL Ensemble [38] &amp; 26.30 &amp; 41.16 &amp; \(1.8\cdot 10^&#123;20&#125;\) &amp; \(1.1\cdot 10^&#123;21&#125;\) \\ ConvS2S Ensemble [9] &amp; 26.36 &amp; **41.29** &amp; \(7.7\cdot 10^&#123;19&#125;\) &amp; \(1.2\cdot 10^&#123;21&#125;\) \\ \hline Transformer (base model) &amp; 27.3 &amp; 38.1 &amp; &amp; \(\mathbf&#123;3.3\cdot 10^&#123;18&#125;&#125;\) \\ Transformer (big) &amp; **28.4** &amp; **41.8** &amp; &amp; \(2.3\cdot 10^&#123;19&#125;\) \\ \hline \hline \end&#123;tabular&#125;</span><br><span class="line">Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.</span><br></pre></td></tr></table></figure><p>根据我们的问题，RAG 的结果为<code>O(log_&#123;k&#125;(n)</code>，这与原始表格中的内容一致（见下图），同时可以看到 RAG 过程中根据问题检索到的文档信息包括了表格 1 和表格 2，其中表格 1 是我们问题的答案来源。</p><img src="/images/post/2024/05/nougat-table-verify.png" class="" width="1000" height="600"><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>这种方案有如下的优点和缺点：</p><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>可以完美支持学术论文文档的解析</li><li>解析结果清晰易理解且容易处理</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>Nougat 是用学术论文进行训练的模型，因此对学术论文文档解析效果很好，但其他类型的 PDF 文档解析效果可能不尽人意</li><li>只对英文文档支持较好，对其他语言的支持有限</li><li>需要 GPU 机器进行解析加速</li></ul><h2 id="UnstructuredIO-方案"><a href="#UnstructuredIO-方案" class="headerlink" title="UnstructuredIO 方案"></a>UnstructuredIO 方案</h2><img src="/images/post/2024/05/uio-flow.png" class="" width="1000" height="600"><p>这种方案是先将 PDF 文件转换成 HTML 文件，然后使用 <a href="https://github.com/Unstructured-IO/unstructured">UnstructuredIO</a> 来解析 HTML 文件，LlamaIndex 已经对 UnstructuredIO 进行了集成，因此可以很方便地将对 HTML 文件进行 RAG 的流程处理，包括文件的索引、存储和检索。</p><p><strong>为什么要转成 HTML 文件？</strong>在 PDF 文件中表格的内容不容易识别，而在 HTML 文件中表格的内容一般以<code>table</code>的标签来表示，可以很容易地解析和提取表格数据。LlamaIndex 在集成 UnstructuredIO 时只实现了对 HTML 文件的解析，我猜测是因为 HTML 文件的解析相对简单，虽然 UnstructuredIO 本身也支持 PDF 文件的解析，但是 PDF 文件的解析需要依赖第三方的模型和工具，整体实施起来会比较复杂。</p><h3 id="PDF-转-HTML"><a href="#PDF-转-HTML" class="headerlink" title="PDF 转 HTML"></a>PDF 转 HTML</h3><p>在开源社区中有很多工具可以将 PDF 文件转换成 HTML 文件，其中比较出名的是 <a href="https://github.com/pdf2htmlEX/pdf2htmlEX">pdf2htmlEX</a>，但经过测试发现在 pdf2htmlEX 解析出来的 HTML 文件中，表格的内容并没有以<code>table</code>标签进行展示，而是以<code>div</code>标签来表示（如下图所示），这使得我们无法使用 UnstructuredIO 来解析表格内容，因此我们需要使用其他工具来转换 PDF。</p><img src="/images/post/2024/05/pdf2htmlEX-table.png" class="" width="1000" height="600"><p>这里推荐一个名为 <a href="https://apryse.com/">WebViewer</a> 的文档工具，提供了常用文档的编辑功能，其中包括我们需要的 PDF 转 HTML 功能，并且它提供了多种开发语言的 SDK 包，方便在各种项目中集成使用。下面我们就以 Python 为例来介绍如何使用这个工具转换 PDF 文件为 HTML 文件。</p><p>首先在其<a href="https://apryse.com/">官网</a>进行注册，注册后在<a href="https://dev.apryse.com/">这个页面</a>可以获得<code>trial key</code>，后面使用 SDK 包时需要填写这个 key。</p><img src="/images/post/2024/05/apryse-key.png" class="" width="1000" height="600"><p>然后使用 pip 安装 SDK 包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install apryse-sdk --extra-index-url=https://pypi.apryse.com</span><br></pre></td></tr></table></figure><p>另外还需要下载 SDK 包关联的结构化输出模块包，Mac OS 系统的包下载地址是<a href="https://docs.apryse.com/downloads/StructuredOutputMac.zip">这里</a>，下载完成后解压缩，然后将解压后的文件夹放到项目的根目录下，解压后的目录名为<code>Lib</code>。</p><p>下面是示例代码：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apryse_sdk <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">PDFNet.Initialize(<span class="string">&quot;your_trial_key&quot;</span>)</span><br><span class="line"></span><br><span class="line">file_name = <span class="string">&quot;demo&quot;</span></span><br><span class="line">input_filename = <span class="string">f&quot;<span class="subst">&#123;file_name&#125;</span>.pdf&quot;</span></span><br><span class="line">output_dir = <span class="string">&quot;output&quot;</span></span><br><span class="line"></span><br><span class="line">PDFNet.AddResourceSearchPath(<span class="string">&quot;./Lib&quot;</span>)</span><br><span class="line"></span><br><span class="line">htmlOutputOptions = HTMLOutputOptions()</span><br><span class="line">htmlOutputOptions.SetContentReflowSetting(HTMLOutputOptions.e_reflow_full)</span><br><span class="line"></span><br><span class="line">Convert.ToHtml(input_filename, <span class="string">f&quot;<span class="subst">&#123;output_dir&#125;</span>/<span class="subst">&#123;file_name&#125;</span>.html&quot;</span>, htmlOutputOptions)</span><br></pre></td></tr></table></figure><ul><li>首先通过<code>PDFNet.Initialize</code>函数初始化 SDK 包，填写之前注册后得到的<code>trial key</code></li><li>使用<code>PDFNet.AddResourceSearchPath</code>添加解压后的结构化输出模块包路径，这里的目录名为<code>Lib</code></li><li>使用<code>HTMLOutputOptions</code> 设置 HTML 输出选项，这里的设置表示输出的 HTML 会整合成一个完整的页面</li><li>最后使用<code>Convert.ToHtml</code>函数对 PDF 文件进行转换，转换后的 HTML 文件会保存在<code>output</code>目录下</li></ul><p>转换后的 HTML 文件我们可以看到，其中的表格内容是以<code>table</code>的标签来表示的，关于使用 WebViewer 来转换 PDF 文件为 HTML 文件的更多信息可以参考<a href="https://docs.apryse.com/documentation/mac/guides/features/conversion/convert-pdf-to-html/">这里</a>。</p><h3 id="HTML-文件处理"><a href="#HTML-文件处理" class="headerlink" title="HTML 文件处理"></a>HTML 文件处理</h3><p>得到 HTML 文件后，我们就可以使用 LlamaIndex 中集成的 UnstructuredIO 解析功能来解析 HTML 中的表格内容了，代码示例如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> FlatReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> UnstructuredElementNodeParser</span><br><span class="line"></span><br><span class="line">reader = FlatReader()</span><br><span class="line">demo_file = reader.load_data(Path(<span class="string">&quot;demo.html&quot;</span>))</span><br><span class="line">node_parser = UnstructuredElementNodeParser()</span><br><span class="line"></span><br><span class="line">pkl_file = <span class="string">&quot;demo.pkl&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(pkl_file):</span><br><span class="line">    raw_nodes = node_parser.get_nodes_from_documents(demo_file)</span><br><span class="line">    pickle.dump(raw_nodes, <span class="built_in">open</span>(pkl_file, <span class="string">&quot;wb&quot;</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    raw_nodes = pickle.load(<span class="built_in">open</span>(pkl_file, <span class="string">&quot;rb&quot;</span>))</span><br><span class="line"></span><br><span class="line">base_nodes, node_mappings = node_parser.get_base_nodes_and_mappings(raw_nodes)</span><br></pre></td></tr></table></figure><ul><li>代码中使用<code>FlatReader</code>读取 HTML 文件内容</li><li>使用<code>UnstructuredElementNodeParser</code>解析 HTML 文件内容，得到原始节点数据</li><li>将解析后的节点数据保存到<code>demo.pkl</code>文件中，方便后续使用</li><li>最后通过原始节点数据得到解析后的节点数据<code>base_nodes</code>和节点映射<code>node_mappings</code></li></ul><p>解析完 HTML 文件后会得到普通文本的节点和包含表格的节点，这里我们使用这个<a href="https://qwenlm.github.io/blog/qwen-vl/">介绍 Qwen-VL 多模态模型的 HTML 页面</a>作为测试数据，因为里面有不少表格，来看看解析后的表格具体内容：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.schema <span class="keyword">import</span> IndexNode, TextNode</span><br><span class="line"></span><br><span class="line">example_index_nodes = [b <span class="keyword">for</span> b <span class="keyword">in</span> base_nodes <span class="keyword">if</span> <span class="built_in">isinstance</span>(b, IndexNode)]</span><br><span class="line">example_index_node = example_index_nodes[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">f&quot;\n--------\n<span class="subst">&#123;example_index_node.get_content(metadata_mode=<span class="string">&#x27;all&#x27;</span>)&#125;</span>\n--------\n&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n--------\nIndex ID: <span class="subst">&#123;example_index_node.index_id&#125;</span>\n--------\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">f&quot;\n--------\n<span class="subst">&#123;node_mappings[example_index_node.index_id].get_content()&#125;</span>\n--------\n&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>从解析后的节点数据中找到包含表格的节点，其中<code>IndexNode</code>是包含表格的节点</li><li>我们通过<code>example_index_nodes[1]</code>来获取第 2 个表格的数据</li><li>分别打印出表格的内容、索引 ID 和节点映射的内容</li></ul><p>打印出来的节点信息如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 表格字段</span></span><br><span class="line">--------</span><br><span class="line">col_schema: Column: Model</span><br><span class="line">Type: string</span><br><span class="line">Summary: Names of the AI models compared</span><br><span class="line"></span><br><span class="line">...other columns...</span><br><span class="line"></span><br><span class="line">filename: Qwen-VL.html</span><br><span class="line">extension: .html</span><br><span class="line"><span class="comment"># 表格的总结信息</span></span><br><span class="line">Comparison of performance metrics <span class="keyword">for</span> different AI models across various tasks such as DocVQA, ChartQA, AI2D, TextVQA, MMMU, MathVista, and MM-Bench-CN.,</span><br><span class="line">with the following table title:</span><br><span class="line">AI Model Performance Comparison,</span><br><span class="line">with the following columns:</span><br><span class="line">- Model: Names of the AI models compared</span><br><span class="line">...other columns...</span><br><span class="line">--------</span><br><span class="line"><span class="comment"># 表格节点ID</span></span><br><span class="line">--------</span><br><span class="line">Index ID: 41edc9a6-30ed-44cf-967e-685f7dfce8df</span><br><span class="line">--------</span><br><span class="line"><span class="comment"># mapping中的表格数据</span></span><br><span class="line">--------</span><br><span class="line">Comparison of performance metrics <span class="keyword">for</span> different AI models across various tasks such as DocVQA, ChartQA, AI2D, TextVQA, MMMU, MathVista, and MM-Bench-CN.,</span><br><span class="line">with the following table title:</span><br><span class="line">AI Model Performance Comparison,</span><br><span class="line">with the following columns:</span><br><span class="line">- Model: Names of the AI models compared</span><br><span class="line">...other columns...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Markdown格式的表格内容</span></span><br><span class="line">|Model|DocVQA|ChartQA|AI2D|TextVQA|MMMU|MathVista|MM-Bench-CN|</span><br><span class="line">|---|---|---|---|---|---|---|---|</span><br><span class="line">|Other Best Open-<span class="built_in">source</span> LVLM|81.6% (CogAgent)|68.4% (CogAgent)|73.7% (Fuyu-Medium)|76.1% (CogAgent)|45.9% (Yi-VL-34B)|36.7% (SPHINX-V2)|72.4% (InternLM-XComposer-VL)|</span><br><span class="line">|Gemini Pro|88.1%|74.1%|73.9%|74.6%|47.9%|45.2%|74.3%|</span><br><span class="line">|Gemini Ultra|90.9%|80.8% 1|79.5% 1|82.3% 1|59.4% 1|53.0% 1|-|</span><br><span class="line">|GPT-4V|88.4%|78.5%|78.2%|78.0%|56.8%|49.9%|73.9%|</span><br><span class="line">|Qwen-VL-Plus|91.4%|78.1%|75.9%|78.9%|45.2%|43.3%|68.0%|</span><br><span class="line">|Qwen-VL-Max|93.1% 1|79.8% 2|79.3% 2|79.5% 2|51.4% 3|51.0% 2|75.1% 1|</span><br><span class="line">--------</span><br></pre></td></tr></table></figure><p>从打印结果中我们可以看到，LlamaIndex 对表格的每个字段进行了总结，然后对整个表也进行了总结，最后还将表格内容转换成了 Markdown 格式。</p><p>接下来我们使用 LlamaIndex 的递归检索器来检索表格内容，代码示例如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.core.retrievers <span class="keyword">import</span> RecursiveRetriever</span><br><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"></span><br><span class="line">vector_index = VectorStoreIndex(base_nodes)</span><br><span class="line">vector_retriever = vector_index.as_retriever(similarity_top_k=<span class="number">1</span>)</span><br><span class="line">vector_query_engine = vector_index.as_query_engine(similarity_top_k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">recursive_retriever = RecursiveRetriever(</span><br><span class="line">    <span class="string">&quot;vector&quot;</span>,</span><br><span class="line">    retriever_dict=&#123;<span class="string">&quot;vector&quot;</span>: vector_retriever&#125;,</span><br><span class="line">    node_dict=node_mappings,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">query_engine = RetrieverQueryEngine.from_args(recursive_retriever)</span><br><span class="line">question = <span class="string">&quot;In the comparison of performance metrics for different AI models across various tasks. What is the performance metric of the model &#x27;Qwen-VL-Plus&#x27; in task &#x27;MMMU&#x27;? Tell me the exact number.&quot;</span></span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;answer: <span class="subst">&#123;<span class="built_in">str</span>(response)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Retrieving <span class="keyword">with</span> query <span class="built_in">id</span> <span class="literal">None</span>: In the comparison of performance metrics <span class="keyword">for</span> different AI models across various tasks. What <span class="keyword">is</span> the performance metric of the model <span class="string">&#x27;Qwen-VL-Plus&#x27;</span> <span class="keyword">in</span> task <span class="string">&#x27;MMMU&#x27;</span>? Tell me the exact number.</span><br><span class="line">Retrieved node <span class="keyword">with</span> <span class="built_in">id</span>, entering: 41edc9a6-30ed-44cf-967e-685f7dfce8df</span><br><span class="line">Retrieving <span class="keyword">with</span> query <span class="built_in">id</span> 41edc9a6-30ed-44cf-967e-685f7dfce8df: In the comparison of performance metrics <span class="keyword">for</span> different AI models across various tasks. What <span class="keyword">is</span> the performance metric of the model <span class="string">&#x27;Qwen-VL-Plus&#x27;</span> <span class="keyword">in</span> task <span class="string">&#x27;MMMU&#x27;</span>? Tell me the exact number.</span><br><span class="line"></span><br><span class="line">answer: The performance metric of the model <span class="string">&#x27;Qwen-VL-Plus&#x27;</span> <span class="keyword">in</span> the task <span class="string">&#x27;MMMU&#x27;</span> <span class="keyword">is</span> <span class="number">45.2</span>%.</span><br></pre></td></tr></table></figure><ul><li>代码中首先使用<code>VectorStoreIndex</code>将解析后的节点数据转换为索引</li><li>然后使用索引构建检索器和查询引擎，这里将<code>similarity_top_k</code>同时设置为 1，表示只返回最相似的一个结果</li><li>使用<code>RecursiveRetriever</code>构建递归检索器，传入检索器和节点映射信息，然后构建查询引擎</li><li>最后使用查询引擎对问题进行检索，获取检索结果</li></ul><p>显示结果上半部分是递归检索的调试信息，从调试信息中我们可以看到，根据问题检索到的表格内容（返回了表格的节点 ID），然后根据表格内容回答了问题，答案是 45.2%，对比原表格数据（如下图所示），结果正确。</p><img src="/images/post/2024/05/uio-verify.png" class="" width="1000" height="600"><p><strong>注意</strong>：如果在调试信息中没有看到节点 ID，表示根据问题检索不到相关的表格内容，这种情况最终的回答可能是错误的，这可能是用户问题与表格的总结信息不匹配导致检索失败，可以调整问题然后重新检索。</p><h3 id="准确率验证"><a href="#准确率验证" class="headerlink" title="准确率验证"></a>准确率验证</h3><p>我们只验证了表格其中一个单元格的内容，下面我们来验证表格所有单元格的内容，这样我们可以大致得到这种方案的准确率，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">models = [</span><br><span class="line">    <span class="string">&quot;Other BestOpen-source LVLM&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Gemini Pro&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Gemini Ultra&quot;</span>,</span><br><span class="line">    <span class="string">&quot;GPT-4V&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Qwen-VL-Plus&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Qwen-VL-Max&quot;</span>,</span><br><span class="line">]</span><br><span class="line">metrics = [<span class="string">&quot;DocVQA&quot;</span>, <span class="string">&quot;ChartQA&quot;</span>, <span class="string">&quot;AI2D&quot;</span>, <span class="string">&quot;TextVQA&quot;</span>, <span class="string">&quot;MMMU&quot;</span>, <span class="string">&quot;MathVista&quot;</span>, <span class="string">&quot;MM-Bench-CN&quot;</span>]</span><br><span class="line">questions = []</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    <span class="keyword">for</span> metric <span class="keyword">in</span> metrics:</span><br><span class="line">        questions.append(</span><br><span class="line">            <span class="string">f&quot;In the comparison of performance metrics for different AI models across various tasks. What is the performance metric of the model &#x27;<span class="subst">&#123;model&#125;</span>&#x27; in task &#x27;<span class="subst">&#123;metric&#125;</span>&#x27;? Tell me the exact number.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">actual_metrics = [</span><br><span class="line">    <span class="number">81.6</span>, <span class="number">68.4</span>, <span class="number">73.7</span>, <span class="number">76.1</span>, <span class="number">45.9</span>, <span class="number">36.7</span>, <span class="number">72.4</span>,</span><br><span class="line">    <span class="number">88.1</span>, <span class="number">74.1</span>, <span class="number">73.9</span>, <span class="number">74.6</span>, <span class="number">47.9</span>, <span class="number">45.2</span>, <span class="number">74.3</span>,</span><br><span class="line">    <span class="number">90.9</span>, <span class="number">80.8</span>, <span class="number">75.9</span>, <span class="number">82.3</span>, <span class="number">59.4</span>, <span class="number">53</span>, <span class="number">0</span>,</span><br><span class="line">    <span class="number">88.4</span>, <span class="number">78.5</span>, <span class="number">78.2</span>, <span class="number">78</span>, <span class="number">56.8</span>, <span class="number">49.9</span>, <span class="number">73.9</span>,</span><br><span class="line">    <span class="number">91.4</span>, <span class="number">78.1</span>, <span class="number">75.9</span>, <span class="number">78.9</span>, <span class="number">45.2</span>, <span class="number">43.3</span>, <span class="number">68</span>,</span><br><span class="line">    <span class="number">93.1</span>, <span class="number">79.8</span>, <span class="number">79.3</span>, <span class="number">79.5</span>, <span class="number">51.4</span>, <span class="number">51</span>, <span class="number">75.1</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">actual_answers = <span class="built_in">dict</span>(<span class="built_in">zip</span>(questions, actual_metrics))</span><br><span class="line"></span><br><span class="line">result = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> questions:</span><br><span class="line">  response = query_engine.query(q)</span><br><span class="line">  answer = <span class="built_in">str</span>(response)</span><br><span class="line">  result[q] = <span class="built_in">str</span>(actual_answers[q]) <span class="keyword">in</span> answer</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&quot;question: <span class="subst">&#123;q&#125;</span>\nresponse: <span class="subst">&#123;answer&#125;</span>\nactual:<span class="subst">&#123;actual_answers[q]&#125;</span>\nresult:<span class="subst">&#123;result[q]&#125;</span>\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">correct = <span class="built_in">sum</span>(result.values())</span><br><span class="line">total = <span class="built_in">len</span>(result)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Percentage of True values: <span class="subst">&#123;correct / total * <span class="number">100</span>&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>代码中我们构造了 42 个问题，每个问题都是关于表格中不同 AI 模型在不同任务中的性能指标</li><li>然后我们通过查询引擎对这些问题进行检索，获取检索结果</li><li>最后我们将检索结果与实际的性能指标进行比较，计算准确率</li></ul><p>计算结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Retrieving with query <span class="built_in">id</span> None: In comparison of performance metrics <span class="keyword">for</span> different AI models across various tasks. What is the performance metric of the model <span class="string">&#x27;Other BestOpen-source LVLM&#x27;</span> <span class="keyword">in</span> task <span class="string">&#x27;DocVQA&#x27;</span>? Tell me the exact number.</span><br><span class="line">Retrieved node with <span class="built_in">id</span>, entering: 41edc9a6-30ed-44cf-967e-685f7dfce8df</span><br><span class="line">Retrieving with query <span class="built_in">id</span> 41edc9a6-30ed-44cf-967e-685f7dfce8df: In comparison of performance metrics <span class="keyword">for</span> different AI models across various tasks. What is the performance metric of the model <span class="string">&#x27;Other BestOpen-source LVLM&#x27;</span> <span class="keyword">in</span> task <span class="string">&#x27;DocVQA&#x27;</span>? Tell me the exact number.</span><br><span class="line"></span><br><span class="line">question: In the comparison of performance metrics <span class="keyword">for</span> different AI models across various tasks. What is the performance metric of the model <span class="string">&#x27;Other BestOpen-source LVLM&#x27;</span> <span class="keyword">in</span> task <span class="string">&#x27;DocVQA&#x27;</span>? Tell me the exact number.</span><br><span class="line">response: 81.6%</span><br><span class="line">actual:81.6</span><br><span class="line">result:True</span><br><span class="line"></span><br><span class="line">...other questions...</span><br><span class="line"></span><br><span class="line">Percentage of True values: 66.66666666666666%</span><br></pre></td></tr></table></figure><p>当验证了表格中所有单元单元格的内容后，我们得到的准确率为 <strong>66.67</strong>%，这说明这种方案在检索表格内容时并不是百分之百正确，但这个准确率在现有方案中已经算比较高的了。</p><h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><p>这种方案有如下的优点和缺点：</p><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul><li>无需使用 OCR 技术</li><li>无需使用 GPU 服务器进行来转换 PDF 文件</li></ul><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul><li>需要使用第三方工具将 PDF 文件转换为 HTML 文件</li><li>用户问题要与表格的总结信息匹配才能获得正确的检索结果</li></ul><h2 id="GPT4o-方案"><a href="#GPT4o-方案" class="headerlink" title="GPT4o 方案"></a>GPT4o 方案</h2><img src="/images/post/2024/05/gpt4o-flow.png" class="" width="1000" height="600"><p>最后一种方案是使用 OpenAI 的最新模型 GPT4o 来处理表格内容，GPT4o 在图片识别能力上得到了很大的提升，可以轻松识别出以前 GPT4 模型无法识别的内容。LlamaIndex 的 LlamaParse 工具已经对 GPT4o 进行了集成，可以将 PDF 文件转换成 Markdown 格式的内容，然后进行 RAG 的检索流程。</p><p>首先需要到<a href="https://cloud.llamaindex.ai/">LlamaCloud</a>上注册账号，注册完成后可以创建 API Key，后面的代码示例中需要用到这个 Key。</p><img src="/images/post/2024/05/llama-parse-key.png" class="" width="1000" height="600"><p>然后使用 pip 安装 LlamaParse：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install llama-parse</span><br></pre></td></tr></table></figure><p>接下来我们使用 LlamaParse 将 PDF 文件转换为 Markdown 格式的内容，代码示例如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_parse <span class="keyword">import</span> LlamaParse</span><br><span class="line"></span><br><span class="line">parser_gpt4o = LlamaParse(</span><br><span class="line">    result_type=<span class="string">&quot;markdown&quot;</span>,</span><br><span class="line">    api_key=<span class="string">&quot;&lt;llama_parse_api_key&gt;&quot;</span>,</span><br><span class="line">    gpt4o_mode=<span class="literal">True</span>,</span><br><span class="line">    gpt4o_api_key=<span class="string">&quot;&lt;openai_api_key&gt;&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">pdf_file = <span class="string">&quot;demo.pdf&quot;</span></span><br><span class="line">pkl_file = <span class="string">&quot;demo.pkl&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(pkl_file):</span><br><span class="line">    documents_gpt4o = parser_gpt4o.load_data(pdf_file)</span><br><span class="line">    pickle.dump(documents_gpt4o, <span class="built_in">open</span>(pkl_file, <span class="string">&quot;wb&quot;</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    documents_gpt4o = pickle.load(<span class="built_in">open</span>(pkl_file, <span class="string">&quot;rb&quot;</span>))</span><br></pre></td></tr></table></figure><ul><li>代码中首先创建一个 LlamaParse 对象，传入 OpenAI API Key 以及我们刚才注册后获得的 LlamaParse API Key</li><li>然后使用<code>load_data</code>方法将 PDF 文件转换为 Markdown 格式的内容，转换后的 Markdown 内容会保存在<code>demo.pkl</code>文件中</li><li>最后将转换后的 Markdown 内容保存到<code>documents_gpt4o</code>变量中</li></ul><p>执行完程序后，LlamaParse 会将整个 PDF 文件转换为 Markdown 格式，我们来看下转换后的 Markdown 中的表格内容：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">| Model          | DocVQA | ChartQA | AI2D  | TextVQA | MMMU  | MathVista | MM-Bench-CN |</span><br><span class="line">|----------------|--------|---------|-------|---------|-------|-----------|-------------|</span><br><span class="line">| Other Best Open-source LLM | 81.6% (Capypage) | 68.4% (Capypage) | 73.7% (Capypage) | 74.3% (Capypage) | 76.1% (Capypage) | 45.9% (Capypage) | 36.7% (Capypage) | 72.4% (Capypage) |</span><br><span class="line">| Gemini Pro     | 88.1%  | 74.1%   | 73.9% | 74.6%   | 47.9% | 45.2%     | 74.3%       |</span><br><span class="line">| Gemini Ultra   | 90.9%  | 80.8%   | 75.9% | 82.3%   | 59.4% | 53.0%     | 75.1%       |</span><br><span class="line">| GPT-4V         | 88.8%  | 78.4%   | 75.9% | 80.9%   | 53.9% | 51.0%     | 75.1%       |</span><br><span class="line">| Qwen-VL-Plus   | 88.2%  | 78.1%   | 75.9% | 80.9%   | 45.2% | 51.0%     | 75.1%       |</span><br><span class="line">| Qwen-VL-Max    | 79.8%  | 79.8%   | 79.3% | 79.2%   | 51.4% | 51.0%     | 75.1%       |</span><br></pre></td></tr></table></figure><p>我们再使用 LlamaIndex 对 Markdown 内容进行索引和检索，代码示例如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_nodes</span>(<span class="params">docs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Split docs into nodes, by separator.&quot;&quot;&quot;</span></span><br><span class="line">    nodes = []</span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">        doc_chunks = doc.text.split(<span class="string">&quot;\n---\n&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> doc_chunk <span class="keyword">in</span> doc_chunks:</span><br><span class="line">            node = TextNode(</span><br><span class="line">                text=doc_chunk,</span><br><span class="line">                metadata=deepcopy(doc.metadata),</span><br><span class="line">            )</span><br><span class="line">            nodes.append(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nodes</span><br><span class="line"></span><br><span class="line">nodes = get_nodes(documents_gpt4o)</span><br><span class="line">vector_index = VectorStoreIndex(nodes)</span><br><span class="line">query_engine = vector_index.as_query_engine(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">question = <span class="string">&quot;In the comparison of performance metrics for different AI models across various tasks. What is the performance metric of the model &#x27;Qwen-VL-Plus&#x27; in task &#x27;MMMU&#x27;? Tell me the exact number.&quot;</span></span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;answer: <span class="subst">&#123;<span class="built_in">str</span>(response)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">answer: The performance metric of the model <span class="string">&#x27;Qwen-VL-Plus&#x27;</span> <span class="keyword">in</span> the task <span class="string">&#x27;MMMU&#x27;</span> <span class="keyword">is</span> <span class="number">45.2</span>%.</span><br></pre></td></tr></table></figure><ul><li>LlamaParse 在解析 PDF 文件时会在 Markdown 内容中添加<code>---</code>这样的分页标签，我们通过这个标签将 Markdown 内容分割成多个节点，然后将这些节点转换为<code>TextNode</code>对象</li><li>剩下的代码就是常规的索引和检索流程</li><li>可以看到 GPT4o 的检索结果也同样正确</li></ul><p>我们再对 GPT4o 方案的准确率进行验证，也就是验证表格中每个单元格的内容，代码可以参考前面的示例代码，计算结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Percentage of True values: 47.61904761904761%</span><br></pre></td></tr></table></figure><p>当验证了表格中所有单元单元格的内容后，我们得到的准确率为 <strong>47.62</strong>%，与 UnstructuredIO 方案相比，这种方案的准确率较低。</p><h3 id="优缺点-2"><a href="#优缺点-2" class="headerlink" title="优缺点"></a>优缺点</h3><p>这种方案有如下的优点和缺点：</p><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul><li>可以直接解析 PDF 文件，无需转换成其他格式的文件</li><li>不管文件中的内容是文字还是图片，都可以进行解析</li></ul><h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ul><li>LlamaParse 虽然每天有免费的调用次数，但是如果需要大量调用，还是需要付费</li><li>目前使用多模态模型解析 PDF 文件的准确率还是比较低，需要进一步优化</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了三种方案来解析 PDF 文件中的表格内容，分别是 Nougat 方案、UnstructuredIO 方案和 GPT4o 方案，这三种方案各有优缺点，目前还没有一种方案可以完美地满足所有的业务需求，但相信在不远的将来会有更多的新技术出现，来解决这个问题。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p><h2 id="引用参考"><a href="#引用参考" class="headerlink" title="引用参考"></a>引用参考</h2><ul><li><a href="https://ai.plainenglish.io/advanced-rag-07-exploring-rag-for-tables-5c3fc0de7af6">Advanced RAG 07: Exploring RAG for Tables</a></li><li><a href="https://levelup.gitconnected.com/a-guide-to-processing-tables-in-rag-pipelines-with-llamaindex-and-unstructuredio-3500c8f917a7">A Guide to Processing Tables in RAG Pipelines with LlamaIndex and UnstructuredIO</a></li></ul>]]></content>
    
    
    <summary type="html">介绍高级 RAG 检索中几种内嵌表格的解析检索方案</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="embedded-table" scheme="https://zhaozhiming.github.io/tags/embedded-table/"/>
    
    <category term="llama-parser" scheme="https://zhaozhiming.github.io/tags/llama-parser/"/>
    
    <category term="gpt4o" scheme="https://zhaozhiming.github.io/tags/gpt4o/"/>
    
  </entry>
  
  <entry>
    <title>高级 RAG 检索策略之查询重写</title>
    <link href="https://zhaozhiming.github.io/2024/05/13/query-rewrite-rag/"/>
    <id>https://zhaozhiming.github.io/2024/05/13/query-rewrite-rag/</id>
    <published>2024-05-13T14:18:11.000Z</published>
    <updated>2024-10-03T08:49:29.887Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/05/rag-query-rewrite.jpeg" class="" width="400" height="300"><p>在 RAG（Retrieval Augmented Generation）应用中，文档检索是保证 RAG 应用高质量回答的关键环节，我们在之前的文章中也有所介绍，但除此之外，对用户问题的优化也同样重要，有时候用户的问题可能不够清晰或者不够具体，这时候就需要对用户问题进行查询重写，这样才能更好地提高检索的准确性。今天我们就来介绍一些 RAG 应用中查询重写的策略，以及了解如何在实际项目中使用它们。</p><span id="more"></span><h2 id="子问题查询"><a href="#子问题查询" class="headerlink" title="子问题查询"></a>子问题查询</h2><p>子问题策略，也称为子查询，是一种用于生成子问题的技术。子问题策略的核心思想是在问答过程中，为了更好地理解和回答主问题，系统会自动生成并提出与主问题相关的子问题。这些子问题通常具有更具体的细节，可以帮助系统更深入地理解主问题，从而进行更加准确的检索并提供正确的答案。</p><img src="/images/post/2024/05/sub-question-flow.png" class="" width="1000" height="400"><ul><li>子问题策略首先将用户问题通过 LLM（大语言模型）生成多个子问题</li><li>然后将每个子问题经过 RAG 流程得到各自的答案（检索-生成）</li><li>最后将所有子问题的答案合并，得到最终的答案</li></ul><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>在<a href="https://www.llamaindex.ai/">LlamaIndex</a>中已经对子问题查询进行了实现，但在查看子问题查询的效果之前，我们先看普通 RAG 检索对于复杂问题的效果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader</span><br><span class="line"></span><br><span class="line">question = <span class="string">&quot;哈莉·奎因和灭霸在《复仇者联盟》中是正义的角色吗？&quot;</span></span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">node_parser = VectorStoreIndex.from_documents(documents)</span><br><span class="line">query_engine = node_parser.as_query_engine()</span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;base query result: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">base query result: 不，哈莉·奎茵和灭霸在《复仇者联盟》系列中并非被描绘为正义的角色。</span><br></pre></td></tr></table></figure><p>以上代码是 LlamaIndex 的普通 RAG 检索过程，文档数据我们还是使用之前维基百科上的<a href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情来进行测试，这里我们问了一个复合问题，包括 2 个人物角色，一个是 DC 漫画中的<code>哈莉·奎茵</code>，另一个是漫威电影中的<code>灭霸</code>，问题是他们在复仇者联盟中是否为正义角色，查询结果虽然可以说正确，但并没有指出其中一个人物不是漫威电影中的人物。</p><p>我们再来看子问题查询的效果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.tools <span class="keyword">import</span> QueryEngineTool, ToolMetadata</span><br><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> SubQuestionQueryEngine</span><br><span class="line"></span><br><span class="line">query_engine_tools = [</span><br><span class="line">    QueryEngineTool(</span><br><span class="line">        query_engine=query_engine,</span><br><span class="line">        metadata=ToolMetadata(</span><br><span class="line">            name=<span class="string">&quot;Avengers&quot;</span>,</span><br><span class="line">            description=<span class="string">&quot;Marvel movie The Avengers&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line">query_engine = SubQuestionQueryEngine.from_defaults(</span><br><span class="line">    query_engine_tools=query_engine_tools</span><br><span class="line">)</span><br><span class="line">response = query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;sub question query result: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Generated <span class="number">2</span> sub questions.</span><br><span class="line">[Avengers] Q: 哈莉·奎茵在《复仇者联盟》电影中扮演什么角色？</span><br><span class="line">[Avengers] Q: 灭霸在《复仇者联盟》电影中扮演什么角色？</span><br><span class="line">[Avengers] A: 在提供的有关《复仇者联盟》电影的背景中未提到哈莉·奎茵。</span><br><span class="line">[Avengers] A: 灭霸是《复仇者联盟》电影中的主要反派。他是一个强大的战争领主，试图利用无限宝石按照自己的愿景重塑宇宙。灭霸被描绘为强大而无情的敌人，对复仇者联盟和整个宇宙构成重大威胁。</span><br><span class="line">sub question query result: 在提供的有关《复仇者联盟》电影的背景中未提到哈莉·奎茵。灭霸是《复仇者联盟》电影中的主要反派，被描绘为一个强大而无情的敌人。</span><br></pre></td></tr></table></figure><ul><li>首先构建查询引擎工具，在工具中传入普通的查询引擎，并设置工具的元数据，元数据信息在 Debug 信息中会进行展示</li><li>使用<code>SubQuestionQueryEngine</code>类构建子问题查询引擎，传入查询引擎工具</li><li>查询结果中会显示生成的子问题以及子问题的答案，最终答案基于所有子问题的答案进行生成</li></ul><p>从上面的代码可以看出，对于复杂问题，子问题查询的效果要比普通查询更加准确。</p><p>上面的示例中，生成的子问题及答案是随着 Debug 信息展示出来的，我们也可以在检索过程中获取这些数据：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.callbacks <span class="keyword">import</span> (</span><br><span class="line">    CallbackManager,</span><br><span class="line">    LlamaDebugHandler,</span><br><span class="line">    CBEventType,</span><br><span class="line">    EventPayload,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line">llama_debug = LlamaDebugHandler(print_trace_on_end=<span class="literal">True</span>)</span><br><span class="line">callback_manager = CallbackManager([llama_debug])</span><br><span class="line">Settings.callback_manager = callback_manager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子问题查询代码</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (start_event, end_event) <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">    llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)</span><br><span class="line">):</span><br><span class="line">    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sub Question &quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&quot;: &quot;</span> + qa_pair.sub_q.sub_question.strip())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Answer: &quot;</span> + qa_pair.answer.strip())</span><br></pre></td></tr></table></figure><ul><li>在子问题查询的代码前面加上回调管理器，用来记录子问题查询的调试信息</li><li>在查询结束后，通过回调管理器获取子问题查询的调试信息，然后得到子问题和答案</li></ul><h3 id="提示词"><a href="#提示词" class="headerlink" title="提示词"></a>提示词</h3><p>LlamaIndex 使用单独的 Python 包<code>llama-index-question-gen-openai</code>来生成子问题，它的内部默认使用 OpenAI 的模型来生成子问题，提示词模板可以在 <a href="https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/question_gen/llama-index-question-gen-openai/llama_index/question_gen/openai/base.py#L18-L45">LlamaIndex 的官方仓库</a>中查看。</p><p>我们也可以通过以下方法来打印 LlamaIndex 中的提示词，第一种方法是通过<code>get_prompts()</code>方法来打印，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prompts = query_engine.get_prompts()</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> prompts.keys():</span><br><span class="line">    sub_question_prompt = prompts[key]</span><br><span class="line">    template = sub_question_prompt.get_template()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;prompt: <span class="subst">&#123;template&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>首先通过<code>get_prompts()</code>方法获取查询引擎的 prompts 对象，基本上每个 LlamaIndex 对象都有这个方法</li><li>prompts 对象是个 JSON 对象，它的每个 Key 代表一个提示词模板</li><li>遍历 prompts 对象的每个 Key，获取每个 Key 对应的提示词模板，然后打印出来</li><li>子问题查询会包含 2 个提示词模板，一个是子问题生成的提示词模板，另一个是普通 RAG 的提示词模板</li></ul><p>另外一种方式是通过<code>set_global_handler</code>进行全局设置，示例代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> set_global_handler</span><br><span class="line"></span><br><span class="line">set_global_handler(<span class="string">&quot;simple&quot;</span>)</span><br></pre></td></tr></table></figure><p>在文件开头加上以上代码，这样在执行代码的过程中就会打印出 RAG 检索过程中的提示词，打印出的提示词不是提示词模板，而是加入了具体变量值之后的<strong>完整提示词</strong>。</p><h2 id="HyDE-查询转换"><a href="#HyDE-查询转换" class="headerlink" title="HyDE 查询转换"></a>HyDE 查询转换</h2><img src="/images/post/2024/05/hyde_paper.jpeg" class="" width="1000" height="400"><p>HyDE（Hypothetical Document Embeddings）的本质是通过 LLM 对用户问题生成假设性文档，这些文档基于 LLM 本身的知识生成，可能存在错误或者不准确，但是跟 RAG 中知识库的文档相关联，然后通过假设性文档去检索向量相近的真实文档，通过这种方式来提高检索的准确性，HyDE 的论文可以参考<a href="https://arxiv.org/pdf/2212.10496.pdf">这里</a>。</p><h3 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h3><p>在 LlamaIndex 中已经实现了 HyDE 的查询重写，我们先来看 LlamaIndex 如何生成假设性文档：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.indices.query.query_transform <span class="keyword">import</span> HyDEQueryTransform</span><br><span class="line"></span><br><span class="line">question = <span class="string">&quot;洛基使用了哪种神秘物品试图征服地球？&quot;</span></span><br><span class="line"></span><br><span class="line">hyde = HyDEQueryTransform(include_original=<span class="literal">True</span>)</span><br><span class="line">query_bundle = hyde(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;query_bundle embedding len: <span class="subst">&#123;<span class="built_in">len</span>(query_bundle.embedding_strs)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> idx, embedding <span class="keyword">in</span> <span class="built_in">enumerate</span>(query_bundle.embedding_strs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;embedding <span class="subst">&#123;idx&#125;</span>: <span class="subst">&#123;embedding[:<span class="number">100</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">query_bundle embedding <span class="built_in">len</span>: <span class="number">2</span></span><br><span class="line">embedding <span class="number">0</span>: 在他试图征服地球时，洛基使用了立方体，也被称为宇宙立方。这个神秘的...</span><br><span class="line">embedding <span class="number">1</span>: 洛基使用了哪种神秘物品试图征服地球？</span><br></pre></td></tr></table></figure><ul><li>首先构建<code>HyDEQueryTransform</code>对象，传入参数<code>include_original=True</code>，表示在生成的假设性文档中包含原始问题，其实<code>include_original</code>的默认值就是<code>True</code>，这里传入参数只是为了演示</li><li>然后调用<code>hyde</code>对象，传入问题，返回一个<code>QueryBundle</code>对象</li><li><code>QueryBundle</code>对象的<code>embedding_strs</code>属性值是一个数组，数组第一个元素是生成的假设性文档，如果<code>include_original</code>为<code>True</code>，那么数组的第 2 个元素会包含原始问题</li></ul><p>可以看到 LLM 基于自己的知识很好地回答了用户的问题，生成的假设性文档和电影剧情基本一致。</p><p>LlamaIndex 中生成假设性文档的提示词模板如下，大意就是为问题生成一段内容，其中<code>&#123;context_str&#125;</code>为用户问题：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">HYDE_TMPL = (</span><br><span class="line">    <span class="string">&quot;Please write a passage to answer the question\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Try to include as many key details as possible.\n&quot;</span></span><br><span class="line">    <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;&#123;context_str&#125;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="string">&#x27;Passage:&quot;&quot;&quot;\n&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>下面我们再用查询引擎对问题进行检索：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> TransformQueryEngine</span><br><span class="line"></span><br><span class="line">hyde_query_engine = TransformQueryEngine(query_engine, hyde)</span><br><span class="line">response = hyde_query_engine.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;hyde query result: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">hyde query result: 洛基在试图征服地球时使用了立方体，这是一个未知潜力的强大能源。</span><br></pre></td></tr></table></figure><ul><li>基于<code>HyDEQueryTransform</code>构建一个<code>TransformQueryEngine</code></li><li>查询引擎的<code>query</code>方法会先对原始问题生成假设性文档，然后用假设性文档进行检索并生成答案</li></ul><p>虽然我们得到了正确的结果，但我们不清楚 LlamaIndex 内部在检索过程中是否用假设性文档去检索，我们可以通过以下代码来验证：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.retrievers.transform_retriever <span class="keyword">import</span> TransformRetriever</span><br><span class="line"></span><br><span class="line">retriever = node_parser.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">hyde_retriever = TransformRetriever(retriever, hyde)</span><br><span class="line">nodes = hyde_retriever.retrieve(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;hyde retriever nodes len: <span class="subst">&#123;<span class="built_in">len</span>(nodes)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node id: <span class="subst">&#123;node.id_&#125;</span>, score: <span class="subst">&#123;node.get_score()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">50</span>)</span><br><span class="line">nodes = retriever.retrieve(<span class="string">&quot;\n&quot;</span>.join(<span class="string">f&quot;<span class="subst">&#123;n&#125;</span>&quot;</span> <span class="keyword">for</span> n <span class="keyword">in</span> query_bundle.embedding_strs))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;hyde documents retrieve len: <span class="subst">&#123;<span class="built_in">len</span>(nodes)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;node id: <span class="subst">&#123;node.id_&#125;</span>, score: <span class="subst">&#123;node.get_score()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>上半部分使用<code>TransformRetriever</code>结合原始检索器和<code>HyDEQueryTransform</code>对象构建一个新的检索器</li><li>然后用<strong>新的检索器对用户问题</strong>进行检索，打印出检索到的文档 ID 和分数</li><li>下半部分使用<strong>原始检索器对假设性文档</strong>进行检索，假设性文档取自<code>QueryBundle</code>对象的<code>embedding_strs</code>属性，这里的<code>embedding_strs</code>有 2 个元素，一个是假设性文档，另一个是原始问题</li><li>打印出用假设性文档检索到的文档 ID 和分数</li></ul><p>下面是显示的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hyde retriever nodes len: 2</span><br><span class="line">node <span class="built_in">id</span>: 51e9381a-ef93-49ee-ae22-d169eba95549, score: 0.8895532276574978</span><br><span class="line">node <span class="built_in">id</span>: 5ef8a87e-1a72-4551-9801-ae7e792fdad2, score: 0.8499209871867581</span><br><span class="line">==================================================</span><br><span class="line">hyde documents retrieve nodes len: 2</span><br><span class="line">node <span class="built_in">id</span>: 51e9381a-ef93-49ee-ae22-d169eba95549, score: 0.8842142746289462</span><br><span class="line">node <span class="built_in">id</span>: 5ef8a87e-1a72-4551-9801-ae7e792fdad2, score: 0.8460828835028101</span><br></pre></td></tr></table></figure><p>可以看到两者的结果基本一致，证明检索所用的<strong>输入</strong>是相似的，也就是假设性文档，我们再把<code>HyDEQueryTransform</code>对象中的<code>include_original</code>属性设置为<code>False</code>，这意味着生成的假设性文档不包含原始问题，然后再次运行代码，结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hyde retriever nodes len: 2</span><br><span class="line">node <span class="built_in">id</span>: cfaea328-16d8-4eb8-87ca-8eeccad28263, score: 0.7548985780343257</span><br><span class="line">node <span class="built_in">id</span>: f47bc6c7-d8e1-421f-b9b8-a8006e768c04, score: 0.7508234876205329</span><br><span class="line">==================================================</span><br><span class="line">hyde documents retrieve nodes len: 2</span><br><span class="line">node <span class="built_in">id</span>: 6c2bb8cc-3c7d-4f92-b039-db925dd60d53, score: 0.7498683385309097</span><br><span class="line">node <span class="built_in">id</span>: f47bc6c7-d8e1-421f-b9b8-a8006e768c04, score: 0.7496147322045141</span><br></pre></td></tr></table></figure><p>可以看到两者的结果也是基本一致，但是由于缺少原始问题，检索到的文档分数较低。</p><h3 id="HyDE-的限制"><a href="#HyDE-的限制" class="headerlink" title="HyDE 的限制"></a>HyDE 的限制</h3><p>HyDE 生成的假设性文档是基于 LLM 的知识生成的，可能存在错误或者不准确，LlamaIndex 在<a href="https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo/?h=hyde">官方文档</a>中指出 HyDE 可能会误导查询和引起偏见，所以在实际应用中需要谨慎使用。</p><h2 id="回溯提示（STEP-BACK-PROMPTING）"><a href="#回溯提示（STEP-BACK-PROMPTING）" class="headerlink" title="回溯提示（STEP-BACK PROMPTING）"></a>回溯提示（STEP-BACK PROMPTING）</h2><img src="/images/post/2024/05/stepback_paper.jpeg" class="" width="1000" height="400"><p>回溯提示是一种简单的提示技术，通过抽象化来引导 LLM 从具体实例中提取高级概念和基本原理，利用这些概念和原理指导推理，可以显著提高 LLM 遵循正确推理路径解决问题的能力。<br>以上图中第一个的问题为例，原始问题是给定温度和体积求压强，在左边的回答中，不管是原始的回答还是思维链方式的回答，结果都不正确。而通过回溯提示的方式，先通过原始问题生成一个更为广泛的问题，比如求问题背后的物理公式，再通过广泛问题得到答案，最后将广泛问题的答案和原始问题一起提交给 LLM，从而得到正确的答案。回溯提示的论文可以参考<a href="https://arxiv.org/pdf/2310.06117.pdf">这里</a>。</p><h3 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h3><p>回溯提示在 LlamaIndex 中没有具体的实现，但我们可以通过原始调用 LLM 结合 LlamaIndex 的方式来进行演示，首先我们来让 LLM 根据原始问题生成一个回溯的问题：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line">examples = [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;Who was the spouse of Anna Karina from 1968 to 1974?&quot;</span>,</span><br><span class="line">            <span class="string">&quot;output&quot;</span>: <span class="string">&quot;Who were the spouses of Anna Karina?&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;Estella Leopold went to whichschool between Aug 1954and Nov 1954?&quot;</span>,</span><br><span class="line">            <span class="string">&quot;output&quot;</span>: <span class="string">&quot;What was Estella Leopold&#x27;seducation history?&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    few_shot_examples = <span class="string">&quot;\n\n&quot;</span>.join(</span><br><span class="line">        [<span class="string">f&quot;human: <span class="subst">&#123;example[<span class="string">&#x27;input&#x27;</span>]&#125;</span>\nAI: <span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> examples]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    step_back_question_system_prompt = PromptTemplate(</span><br><span class="line">        <span class="string">&quot;You are an expert at world knowledge.&quot;</span></span><br><span class="line">        <span class="string">&quot;Your task is to step back and paraphrase a question to a more generic step-back question,&quot;</span></span><br><span class="line">        <span class="string">&quot;which is easier to answer. Here are a few examples:\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;few_shot_examples&#125;&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    completion = client.chat.completions.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">        temperature=<span class="number">0.1</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: step_back_question_system_prompt.<span class="built_in">format</span>(</span><br><span class="line">                    few_shot_examples=few_shot_examples</span><br><span class="line">                ),</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: question&#125;,</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line">    step_back_question = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;step_back_question: <span class="subst">&#123;step_back_question&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>首先我们定义了一些回溯问题的例子，将这些例子放到 LLM 的系统提示词中让 LLM 了解生成问题的规律</li><li>将用户问题和系统提示词一起传给 LLM，让 LLM 生成回溯问题</li></ul><p>生成了回溯问题后，我们再分别对原始问题和回溯问题进行检索，获取它们相关的文档：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">retrievals = retriever.retrieve(question)</span><br><span class="line">normal_context = <span class="string">&quot;\n\n&quot;</span>.join([<span class="string">f&quot;<span class="subst">&#123;n.text&#125;</span>&quot;</span> <span class="keyword">for</span> n <span class="keyword">in</span> retrievals])</span><br><span class="line">retrievals = retriever.retrieve(step_back_question)</span><br><span class="line">step_back_context = <span class="string">&quot;\n\n&quot;</span>.join([<span class="string">f&quot;<span class="subst">&#123;n.text&#125;</span>&quot;</span> <span class="keyword">for</span> n <span class="keyword">in</span> retrievals])</span><br></pre></td></tr></table></figure><p>得到了检索结果后，我们让 LLM 生成最终的答案：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">step_back_qa_prompt_template = PromptTemplate(</span><br><span class="line">        <span class="string">&quot;Context information is below.\n&quot;</span></span><br><span class="line">        <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;normal_context&#125;\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;step_back_context&#125;\n&quot;</span></span><br><span class="line">        <span class="string">&quot;---------------------\n&quot;</span></span><br><span class="line">        <span class="string">&quot;Given the context information and not prior knowledge, &quot;</span></span><br><span class="line">        <span class="string">&quot;answer the question: &#123;question&#125;\n&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    completion = client.chat.completions.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">        temperature=<span class="number">0.1</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Always answer the question, even if the context isn&#x27;t helpful.&quot;</span>,</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: step_back_qa_prompt_template.<span class="built_in">format</span>(</span><br><span class="line">                    normal_context=normal_context,</span><br><span class="line">                    step_back_context=step_back_context,</span><br><span class="line">                    question=question,</span><br><span class="line">                ),</span><br><span class="line">            &#125;,</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line">    step_back_result = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;step_back_result: <span class="subst">&#123;step_back_result&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>在提示词模板中，我们将原始问题和回溯问题的文档信息传给 LLM，并结合原始问题让 LLM 生成答案</li></ul><p>最后我们看下普通 RAG 检索和使用回溯提示后的 RAG 检索两者的区别：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">question: 泰坦星球上有过一场大战吗？</span><br><span class="line">base_result: 没有，泰坦星球上没有发生过大战。它并不是任何已知重大冲突或战争的发生地。</span><br><span class="line">====================================================================================================</span><br><span class="line">step_back_question: 泰坦星球上发生过什么重要事件吗？</span><br><span class="line">step_back_result: 是的，在漫威电影宇宙中，泰坦星球上发生了一场重大的冲突。在《复仇者联盟：无限战争》中，泰坦被描绘成灭霸的毁灭故乡，泰坦上的战斗涉及一群英雄，包括钢铁侠（托尼·斯塔克）、蜘蛛侠（彼得·帕克）、奇异博士（斯蒂芬·斯特兰奇）以及银河护卫队，他们试图阻止灭霸实现他的目标。</span><br></pre></td></tr></table></figure><p>可以看到没有使用回溯提示的结果是错误的，而使用了回溯提示之后，我们得到了问题在知识库文档中的正确答案。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天我们介绍了 RAG 检索中几种查询重写的策略，包括子问题查询、HyDE 查询转换和回溯提示，并通过 LlamaIndex 对这几种策略进行了代码演示，在演示过程中还介绍了一些 LlamaIndex 的使用技巧。还有其他一些查询重写的策略没有在本文中介绍，随着 RAG 技术的发展，查询重写的策略也会越来越多，我们未来在合适的时候再对这一部分进行补充，希望这些内容对大家有所帮助。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍高级 RAG 检索中几种查询重写的策略</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="query-rewrite" scheme="https://zhaozhiming.github.io/tags/query-rewrite/"/>
    
    <category term="hyde" scheme="https://zhaozhiming.github.io/tags/hyde/"/>
    
    <category term="stepback" scheme="https://zhaozhiming.github.io/tags/stepback/"/>
    
  </entry>
  
  <entry>
    <title>使用 Llama3 打造开发团队的私有 Copilot</title>
    <link href="https://zhaozhiming.github.io/2024/05/04/use-llama3-to-build-develop-team-copilot/"/>
    <id>https://zhaozhiming.github.io/2024/05/04/use-llama3-to-build-develop-team-copilot/</id>
    <published>2024-05-04T13:08:41.000Z</published>
    <updated>2024-10-03T08:49:29.887Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/05/llama3-copilot.jpeg" class="" width="400" height="300"><p>相信很多开发人员都使用过 Github Copilot，这种崭新的开发方式可以帮助开发人员极大地提高开发效率，并且也正在逐渐改变开发人员的编程习惯。自从 Meta 开放了最新的开源 LLM（大语言模型） Llama3，业内的各种开发工具和开发框架都在积极地集成 Llama3，以便于使用这个迄今为止功能最强大的开源大模型。今天我们来介绍如何使用 Llama3 构建一个团队专属的私有化 Copilot，不仅可以提高团队的开发效率，还可以保护团队的代码隐私。</p><span id="more"></span><h2 id="编程助手-Copilot"><a href="#编程助手-Copilot" class="headerlink" title="编程助手 Copilot"></a>编程助手 Copilot</h2><p>Copilot 是一种人工智能代码辅助工具，最早由 GitHub 和 OpenAI 共同开发，后面有其他产商也推出了类似的产品。Copilot 能够通过自然语言处理和机器学习技术自动生成高质量代码片段和上下文信息，相比于以前的自动补全工具，Copilot 的代码更加详细和智能，比如自动补全工具只能补全一两行的代码片段，但 Copilot 可以生成整个函数的代码，甚至是整个类，从而减轻程序员的工作量并节省时间和精力。除了代码生成外，Copilot 还是支持 AI 问答、代码解释、语言转换、生成单元测试等功能。目前 Copilot 的使用存在以下几种形式。</p><h3 id="线上服务"><a href="#线上服务" class="headerlink" title="线上服务"></a>线上服务</h3><img src="/images/post/2024/05/copilot-online.png" class="" width="600" height="400"><p>第一种是线上服务，比如 <a href="https://github.com/features/copilot">GitHub Copilot</a>，这种服务用户一般只需安装 IDE 插件即可使用，无需关心模型的部署，优点是可以使用线上性能强大的模型，尤其是 Github Copilot，通过 GitHub 上的代码作为模型训练数据，使得生成的代码质量更高，缺点是无法保护代码隐私，因为你要使用 Copilot 服务，你的代码就会被上传到服务端。</p><p>除了 GitHub Copilot 外，其他类似的产品还有：</p><ul><li><a href="https://codeium.com/">Codeium</a>：一家致力于为开发者提供更智能高效的编程体验的人工智能公司，支持 VSCode、Jetbrains 等 40 多种的 IDE，个人使用完全免费</li><li><a href="https://codegeex.cn/en-US">CodeGeeX</a>：清华大学开发的代码辅助工具，支持多种语言，免费使用，使用了自研的模型</li><li><a href="https://aws.amazon.com/codewhisperer/">CodeWhisperer</a>：AWS 推出的代码辅助工具，免费使用，特点是具有安全扫描功能</li><li><a href="https://tongyi.aliyun.com/lingma/">通义灵码</a>：阿里云推出的代码辅助工具，使用阿里研发的 Qwen 大模型，支持多种语言，免费使用</li></ul><h3 id="本地服务"><a href="#本地服务" class="headerlink" title="本地服务"></a>本地服务</h3><img src="/images/post/2024/05/copilot-local.png" class="" width="600" height="400"><p>第二种是本地服务，这种方式需要在本地部署 LLM，然后通过 IDE 插件调用本地 LLM 的 API 服务。部署本地 LLM 的工具比较多，常用的有<a href="https://ollama.com/">Ollama</a>、<a href="https://localai.io/">LocalAI</a> 等，这些工具支持在 CPU 的机器上运行 LLM，这种方式的优点是无需联网即可使用，并且可以很好地保护代码隐私，缺点是需要每个开发人员都需要安装本地 LLM。</p><h3 id="私有化服务"><a href="#私有化服务" class="headerlink" title="私有化服务"></a>私有化服务</h3><img src="/images/post/2024/05/copilot-team.png" class="" width="600" height="400"><p>私有化服务也是一种本地服务，但与本地服务不同的是，开发人员无需安装本地 LLM，而是通过开发团队统一来部署 LLM 服务，然后开发人员通过 IDE 插件调用团队内部的 LLM 服务。这种方式的优点是可以保护代码隐私，同时也可以提高团队的开发效率，这也是我们今天要介绍的 Copilot 使用方式。</p><h2 id="Llama3-部署"><a href="#Llama3-部署" class="headerlink" title="Llama3 部署"></a>Llama3 部署</h2><p>我们要使用 Llama3 来打造团队的私有 Copilot，首先需要部署 Llama3 ，这里我们使用 <a href="https://github.com/vllm-project/vllm">vllm</a>来部署 Llama3。vllm 是一个高效、易用的库，用于 LLM 的推理和提供服务，它可以部署兼容 OpenAI API 的服务。相比同类产品，vllm 的主要特点是吞<strong>吐率高、延迟低、速度快</strong>。</p><p>首先下载 Llama3 的模型，Llama3 可以在 HuggingFace 上进行下载，但在下载之前需要先提交申请，申请后大约等待一段时间即可审批通过，接着使用 HuggingFace 的 CLI 命令进行下载，我们要下载<a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Meta-Llama-3-8B-Instruct</a>这个模型，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --token YOUR_HF_TOKEN</span><br></pre></td></tr></table></figure><p>然后安装 vllm，vllm 可以通过 pip 安装，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n vllm python=3.9 -y</span><br><span class="line">conda activate vllm</span><br><span class="line">pip install vllm</span><br></pre></td></tr></table></figure><p>安装完成后，我们使用 vllm 的命令来启动兼容 OpenAI 的 API 服务，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python -m vllm.entrypoints.openai.api_server \</span><br><span class="line">--model meta-llama/Meta-Llama-3-8B-Instruct \</span><br><span class="line">--gpu-memory-utilization 0.85</span><br></pre></td></tr></table></figure><p><code>gpu-memory-utilization</code>是 GPU 内存的使用率，这里设置为 0.85，表示服务启动后会占用 85%的 GPU 内存。</p><p>启动服务后，服务地址是<code>http://localhost:8000</code>，我们可以通过 curl 命令来验证 API 服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8000/v1/chat/completions \</span><br><span class="line">  -H <span class="string">&quot;Content-Type: application/json&quot;</span> \</span><br><span class="line">  -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">    &quot;model&quot;: &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,</span></span><br><span class="line"><span class="string">    &quot;messages&quot;: [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        &quot;role&quot;: &quot;user&quot;,</span></span><br><span class="line"><span class="string">        &quot;content&quot;: &quot;Hello!&quot;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">  &#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;id&quot;</span>: <span class="string">&quot;cmpl-01cb80c24d4a4e32992b6328fbf09794&quot;</span>,</span><br><span class="line">  <span class="string">&quot;created&quot;</span>: 1714901485,</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>,</span><br><span class="line">  <span class="string">&quot;object&quot;</span>: <span class="string">&quot;chat.completion&quot;</span>,</span><br><span class="line">  <span class="string">&quot;choices&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;finish_reason&quot;</span>: <span class="string">&quot;stop&quot;</span>,</span><br><span class="line">      <span class="string">&quot;logprobs&quot;</span>: null,</span><br><span class="line">      <span class="string">&quot;index&quot;</span>: 0,</span><br><span class="line">      <span class="string">&quot;message&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Hello! It&#x27;s nice to meet you. Is there something I can help you with, or would you like to chat?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;usage&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;prompt_tokens&quot;</span>: 13,</span><br><span class="line">    <span class="string">&quot;completion_tokens&quot;</span>: 26,</span><br><span class="line">    <span class="string">&quot;total_tokens&quot;</span>: 39</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="IDE-插件-Continue"><a href="#IDE-插件-Continue" class="headerlink" title="IDE 插件 Continue"></a>IDE 插件 Continue</h2><p>部署完服务端后，我们再来安装客户端。<a href="https://www.continue.dev/">Continue</a> 是一个帮助开发人员轻松创建自己的模块化人工智能软件开发系统的 IDE 插件，它支持 VSCode 和 JetBrains 等 IDE，支持一般 Copilot 的功能，包括代码生成、代码解释、AI 问答等。</p><h3 id="插件安装"><a href="#插件安装" class="headerlink" title="插件安装"></a>插件安装</h3><p>我们以 VSCode 为例介绍 Continue 插件的安装，首先进去 VSCode 的插件商店搜索 Continue 插件，然后点击安装即可：</p><img src="/images/post/2024/05/continue-install.png" class="" width="1000" height="600"><h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><p>然后修改 Continue 的配置，使用快捷键打开插件配置文件：cmd&#x2F;ctrl + shift + P，输入 Continue config, 选择<code>Open config.json</code>：</p><img src="/images/post/2024/05/continue-config.png" class="" width="800" height="300"><p>然后修改配置文件，Continue 默认使用 Ollama 来做本地 LLM 部署，但如果我们已经部署好了 LLM 服务，就可以将原来配置文件中的<code>models</code>和<code>tabAutocompleteModel</code>的配置修改为我们自己的 LLM 服务，如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;models&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;llama3-8b&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;apiBase&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://your-llama3-api-host:8000/v1&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;provider&quot;</span><span class="punctuation">:</span> <span class="string">&quot;openai&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;apiKey&quot;</span><span class="punctuation">:</span> <span class="string">&quot;empty&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br><span class="line"><span class="attr">&quot;tabAutocompleteModel&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Tab Autocomplete Model&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;apiBase&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://your-llama3-api-host:8000/v1&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;provider&quot;</span><span class="punctuation">:</span> <span class="string">&quot;openai&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;apiKey&quot;</span><span class="punctuation">:</span> <span class="string">&quot;empty&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>LLM 配置信息中填写<code>provider</code>为<code>openai</code>，这里利用了 OpenAI 的配置格式</li><li>在<code>apiBase</code>中填写我们部署的 LLM 服务地址，这里是<code>http://your-llama3-api-host:8000/v1</code>，注意要加上最后的<code>v1</code>路径</li><li><code>model</code>填写我们下载的 Llama3 模型，这里是<code>meta-llama/Meta-Llama-3-8B-Instruct</code></li><li><code>apiKey</code>属性可以随便填</li><li><code>title</code>属性是显示在插件中的模型名称</li><li><code>models</code>属性是指在 AI 问答和代码生成功能中可以使用的模型</li><li><code>tabAutocompleteModel</code>属性是指在代码补全功能中使用的模型</li></ul><p>然后我们在 Continue 插件中选择模型<code>llama3-8b</code>，这样就可以开始使用 Llama3 了：</p><img src="/images/post/2024/05/continue-select.png" class="" width="400" height="300"><h3 id="使用介绍"><a href="#使用介绍" class="headerlink" title="使用介绍"></a>使用介绍</h3><p>我们先看下 AI 问答功能， 输入问题后 LLM 生成回答：</p><img src="/images/post/2024/05/continue-usage1.png" class="" width="600" height="400"><p>再看看代码生成功能，选中代码后后按住 cmd&#x2F;ctrl + I 键会弹出输入框，我们在输入框中让 LLM 帮我们完成这个方法：</p><img src="/images/post/2024/05/continue-usage2.png" class="" width="600" height="400"><img src="/images/post/2024/05/continue-usage3.png" class="" width="600" height="400"><p>接着看解释代码，选中代码后后按住 cmd&#x2F;ctrl + L 键会将选中的代码复制到问答框中，输入问题后 LLM 根据代码进行回答：</p><img src="/images/post/2024/05/continue-usage4.png" class="" width="600" height="400"><p>生成单元测试，也属于代码生成功能，与之前操作相同：</p><img src="/images/post/2024/05/continue-usage6.png" class="" width="600" height="400"><img src="/images/post/2024/05/continue-usage5.png" class="" width="600" height="400"><p>关于Continue插件的更多使用方法，可以参考<a href="https://docs.continue.dev/">官方文档</a>。</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>如果在使用的过程中发现 Llama3 的输出一直没有结束，可以在配置文件中添加<code>completionOptions</code>配置信息来修复这个问题：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;llama3-8b&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;apiBase&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://your-llama3-api-host:8000/v1&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;provider&quot;</span><span class="punctuation">:</span> <span class="string">&quot;openai&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;apiKey&quot;</span><span class="punctuation">:</span> <span class="string">&quot;empty&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;completionOptions&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;stop&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;&lt;|eot_id|&gt;&quot;</span><span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>另外除了 Llama3 之外，还有其他的开源 LLM 也可以用来作为代码辅助工具，比如<a href="https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat">CodeQwen1.5-7B-Chat</a>就是一个不错的选择。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用开源 LLM 作为团队的代码辅助工具，可以提高团队的开发效率，同时也可以保护团队的代码隐私，虽然目前开源的 LLM 相比 Github Copilot 等公司的线上 LLM 还有一些差距，但是随着开源 LLM 的不断发展，相信两者的差距以后会越来越小。以上就是今天介绍的内容，希望对大家有所帮助。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍如何部署 Llama3 作为团队私有 Copilot，提高团队的开发效率</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llama3" scheme="https://zhaozhiming.github.io/tags/llama3/"/>
    
    <category term="copilot" scheme="https://zhaozhiming.github.io/tags/copilot/"/>
    
    <category term="vscode" scheme="https://zhaozhiming.github.io/tags/vscode/"/>
    
    <category term="ollama" scheme="https://zhaozhiming.github.io/tags/ollama/"/>
    
  </entry>
  
  <entry>
    <title>对 Llama3 执行基准测试评估</title>
    <link href="https://zhaozhiming.github.io/2024/04/29/use-opencompass-evaluate-llama3/"/>
    <id>https://zhaozhiming.github.io/2024/04/29/use-opencompass-evaluate-llama3/</id>
    <published>2024-04-29T08:49:27.000Z</published>
    <updated>2024-10-03T08:49:29.887Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/05/llama3-evaluation.jpg" class="" width="400" height="300"><p>近日 Meta 推出他们最新的开源 LLM（大语言模型）Llama3，吸引了众多科技领域业内人士的关注。Meta 同时也公布了 Llama3 的各项基准测试指标，Llama3 在各项指标的得分上表现优异，超过了目前市面上其他开源 LLM。今天我们就来聊聊 LLM 的基准测试指标，以及如何使用工具来评测 Llama3。</p><span id="more"></span><h2 id="LLama3-介绍"><a href="#LLama3-介绍" class="headerlink" title="LLama3 介绍"></a>LLama3 介绍</h2><p>Llama3 的发布无疑是人工智能领域的一个重磅消息，在 Meta 的官方介绍中罗列了 Llama3 的模型能力、训练参数等技术信息，有网友对这次发布进行了总结：</p><img src="/images/post/2024/05/llama3-features.jpeg" class="" width="1000" height="600"><p>Llama3 开放了 8B 和 70B 两种参数的 LLM，各项能力比 Llama2 强很多，同时还有一个 400B 的 LLM 还在训练中。</p><h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>Llama3 可以在 HuggingFace 上进行下载，但在下载之前需要先提交申请，HuggingFace 上的申请页面如下：</p><img src="/images/post/2024/05/llama3-application.png" class="" width="1000" height="600"><p>申请后大约等待 1 个小时就审批通过了，接着可以使用 HuggingFace 的 CLI 命令进行下载，需要在终端进行登录再下载，以下载 Llama3-8B 为例，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br><span class="line"><span class="comment"># 输入 HuggingFace 账号的 access token</span></span><br><span class="line">huggingface-cli download meta-llama/Meta-Llama-3-8B</span><br></pre></td></tr></table></figure><p>或者直接下载但需要带上 HuggingFace 的 token，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli download meta-llama/Meta-Llama-3-8B --token YOUR_TOKEN</span><br></pre></td></tr></table></figure><h2 id="LLM-基准测试"><a href="#LLM-基准测试" class="headerlink" title="LLM 基准测试"></a>LLM 基准测试</h2><p>LLM 基准测试就像是 LLM 的<strong>考试</strong>，这个考试会用一系列的数据集、问题和任务来考验模型的聪明程度，然后根据模型的表现给出一个分数，满分是 100 分。LLM 基准测试给了一个统一的标准来衡量不同 LLM 的性能，这样一来，不管是公司里的决策者、产品经理还是开发人员，都能更容易地比较和选择最适合他们需求的模型，开发者更好地了解 LLM 的长处和短板之后，就能针对性地改进模型，让模型变得更好更强大。</p><p>LLM 基准测试包括以下方面：</p><ul><li>通用能力：指的是模型的语言理解、对话理解等能力，常用的测试有 MMLU、MT-Bench 等</li><li>Agent 能力：包括工具调用、自我调试、根据反馈信息、探索环境等方面的能力</li><li>逻辑推理：指的是模型的数学、编码等能力，常用的测试有 HumanEval、GSM8K 等</li><li>长文本能力：指的是模型的长文本总结、问答等能力</li><li>特定自然语言处理任务：包括阅读理解、常识推理、世界知识、特定领域知识等方面的能力，常用的测试有 ARC、HellaSwag、SIQA、WinoGrande、TruthfulQA 等</li><li>真实问答：指的是模型生成真实答案的能力，也是减少幻觉的能力</li></ul><p>下面是 Llama3 8B 和 70B 模型的基准测试得分：</p><img src="/images/post/2024/05/llama3-benchmark.png" class="" width="1000" height="600"><p>Meta 使用了 MMLU、HumanEval、GSM8K 等基准对 Llama3 8B 和 70B 的模型进行了评测，从图中可以看出，Llama3 在各项基准测试中的得分都很高，尤其是编码能力，HumanEval 和 GSM8K 的得分都比较高。</p><p>我们再看下 OpenAI 模型基准测试得分：</p><img src="/images/post/2024/05/gpt-benchmark.png" class="" width="1000" height="600"><p>OpenAI 使用了 MMLU、HellaSwag、WinoGrande、ARC、HumanEval 等基准对 GPT-3.5 和 GPT-4 等模型进行了评测，但在 HumanEval 上的分数比 Llama3 低很多。</p><p>在 <a href="https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a">HuggingFace</a> 上，一般是使用 ARC、 HellaSwag、MMLU、TruthQA、WinoGrande 和 GSM8K 这几个基准对 LLM 进行评测。</p><img src="/images/post/2024/05/huggingface-benchmark.png" class="" width="1000" height="600"><h2 id="使用-OpenCompass-评测-Llama3"><a href="#使用-OpenCompass-评测-Llama3" class="headerlink" title="使用 OpenCompass 评测 Llama3"></a>使用 OpenCompass 评测 Llama3</h2><h3 id="OpenCompass-介绍"><a href="#OpenCompass-介绍" class="headerlink" title="OpenCompass 介绍"></a>OpenCompass 介绍</h3><p><a href="https://github.com/open-compass/OpenCompass/">OpenCompass</a> 作为一个专业的大模型评测工具，为用户提供了全面、高效、灵活的评测解决方案，通过开源可复现的评测框架，支持对各类模型进行全面的能力评估。OpenCompass 将测评方向汇总为知识、语言、理解、推理、考试等五大能力维度，整合超过 70 个评测数据集，提供超过 40 万个模型评测问题，支持超过 70 种开源模型的评测，包括最新的 Llama3 模型。</p><h3 id="OpenCompass-安装"><a href="#OpenCompass-安装" class="headerlink" title="OpenCompass 安装"></a>OpenCompass 安装</h3><p>OpenCompass 的安装方式分为 GPU 环境和 CPU 环境，我们以 GPU 环境为例进行讲解，首先使用<code>conda</code>创建一个新的 Python 环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y</span><br><span class="line">conda activate opencompass</span><br></pre></td></tr></table></figure><p>然后下载 OpenCompass 的代码仓库并安装相关依赖：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/open-compass/opencompass.git</span><br><span class="line"><span class="built_in">cd</span> opencompass</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p>因为我们后面还要做 HumanEval 的评测，所以还需要额外安装 HumanEval ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/openai/human-eval.git</span><br><span class="line"><span class="built_in">cd</span> human-eval</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -e .</span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure><p>OpenCompass 提供了基准测试的数据集，包含 HuggingFace 和其他第三方的数据集，还有 OpenCompass 自己的数据集，下载这些数据集并解压到 OpenCompass 的目录下，解压完成后 OpenCompass 目录下会出现一个 data 目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> opencompass</span><br><span class="line">wget https://github.com/open-compass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-core-20240207.zip</span><br><span class="line">unzip OpenCompassData-core-20240207.zip</span><br><span class="line"><span class="built_in">rm</span> OpenCompassData-core-20240207.zip</span><br></pre></td></tr></table></figure><h3 id="Llama3-基准评测"><a href="#Llama3-基准评测" class="headerlink" title="Llama3 基准评测"></a>Llama3 基准评测</h3><p>在开始测评之前，先介绍一下硬件配置和评测模型：</p><ul><li>硬件配置：Nvidia 4090 24G 显存</li><li>评测模型：<a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Meta-Llama-3-8B-Instruct</a></li></ul><p>同时设置好测试时所需的环境变量，如果不设置运行程序会报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MKL_SERVICE_FORCE_INTEL=1</span><br><span class="line"><span class="built_in">export</span> MKL_THREADING_LAYER=GNU</span><br><span class="line"><span class="built_in">export</span> TF_ENABLE_ONEDNN_OPTS=0</span><br></pre></td></tr></table></figure><ul><li>MKL_SERVICE_FORCE_INTEL&#x3D;1: 表示应用程序在执行使用英特尔数学核心库（MKL）的操作时，强制使用英特尔的服务层，即使在非英特尔的硬件上也是如此。</li><li>MKL_THREADING_LAYER&#x3D;GNU: 指定 MKL 使用的线程库为 GNU（GOMP，即 GNU OpenMP），选择正确的线程层可以优化并行性能，减少线程竞争和管理开销。</li><li>TF_ENABLE_ONEDNN_OPTS&#x3D;1: 这个环境变量是为 TensorFlow 设置的，表示启动一些基于 OneDNN（之前称为 MKL-DNN）的优化。</li></ul><p>我们首先进行 SocialIQA 和 WinoGrande 的评测，这 2 个评估指标是用于评估 LLM 理解社会常识和解决歧义问题能力的基准，测试代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python run.py --datasets siqa_gen winograd_ppl \</span><br><span class="line">--hf-path meta-llama/Meta-Llama-3-8B-Instruct \</span><br><span class="line">--model-kwargs device_map=<span class="string">&#x27;auto&#x27;</span> \</span><br><span class="line">--tokenizer-kwargs padding_side=<span class="string">&#x27;left&#x27;</span> truncation=<span class="string">&#x27;left&#x27;</span> trust_remote_code=True \</span><br><span class="line">--max-seq-len 2048 \</span><br><span class="line">--max-out-len 100 \</span><br><span class="line">--batch-size 64 \</span><br><span class="line">--num-gpus 1</span><br><span class="line">--debug</span><br></pre></td></tr></table></figure><ul><li>datasets: 数据集名称</li><li>hf-path: HuggingFace 模型地址，如果本地没有该模型会自动下载</li><li>model-kwargs: 构造 model 的参数</li><li>tokenizer-kwargs: 构造 tokenizer 的参数</li><li>max-seq-len: 模型能接受的最大序列长度</li><li>max-out-len: 最长生成 token 数</li><li>batch-size: 批次大小，如果运行过程中提示显存不足，可以适当调小 batch-size</li><li>num-gpus: 运行模型所需的最少 GPU 数量</li><li>debug: 是否开启 debug 模式，建议是开启调试模式，这样如果有报错会更容易定位问题</li></ul><p>如果程序执行成功，最后在终端会输出下面的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataset    version    metric    mode      opencompass....-Llama-3-8B-Instruct</span><br><span class="line">---------  ---------  --------  ------  -------------------------------------</span><br><span class="line">siqa       e78df3     accuracy  gen                     38.59</span><br><span class="line">winograd   b6c7ed     accuracy  ppl                     57.19</span><br></pre></td></tr></table></figure><p>这就是 Llama3 在 SocialIQA 和 WinoGrande 评测中的得分，分别是 38.59 和 57.19。评测结果可以在 OpenCompass 的 <code>outputs/default/&#123;timestamp&#125;/summary</code>目录下的 CSV 文件中查看，内容如下所示：</p><img src="/images/post/2024/05/siqa-winograd.png" class="" width="1000" height="600"><p>我们再来进行 HumanEval 基准测试，HumanEval 是 OpenAI 创建的基准测试，主要用于评估语言模型在代码生成任务上的能力。它包含多种编程问题，模型的任务是生成符合问题要求的代码。评估方式包括运行模型生成的代码并检测其输出是否正确，以及验证代码的功能是否符合问题的需求，测试代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python run.py --datasets humaneval_gen_8e312c \</span><br><span class="line">--hf-path meta-llama/Meta-Llama-3-8B-Instruct \</span><br><span class="line">--model-kwargs device_map=<span class="string">&#x27;auto&#x27;</span> \</span><br><span class="line">--tokenizer-kwargs padding_side=<span class="string">&#x27;left&#x27;</span> truncation=<span class="string">&#x27;left&#x27;</span> trust_remote_code=True \</span><br><span class="line">--max-seq-len 2048 \</span><br><span class="line">--max-out-len 100 \</span><br><span class="line">--batch-size 64 \</span><br><span class="line">--num-gpus 1</span><br><span class="line">--debug</span><br></pre></td></tr></table></figure><p>可以看到运行代码代与之前的基本一致，只是 datasets 参数改为 humaneval_gen_8e312c，数据集的名称可以在 OpenCompass 的 <code>configs/datasets</code>目录下找到，每个数据集对应一个 Python 文件，以 humaneval_gen_8e312c 数据集为例，对应的就是<code>configs/datasets/humaneval/humaneval_gen_8e312c.py</code>这个文件。</p><p>在执行 HumanEval 评测时，还需要修改之前下载的 human-eval 代码库中的 <code>human_eval/execution.py</code> 文件，将第 58 行的注释取消，启用代码执行评测，这样才能得到正确的评测结果。</p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># WARNING</span><br><span class="line"># This program exists to execute untrusted model-generated code. Although</span><br><span class="line"># it is highly unlikely that model-generated code will do something overtly</span><br><span class="line"># malicious in response to this test suite, model-generated code may act</span><br><span class="line"># destructively due to a lack of model capability or alignment.</span><br><span class="line"># Users are strongly encouraged to sandbox this evaluation suite so that it</span><br><span class="line"># does not perform destructive actions on their host or network. For more</span><br><span class="line"># information on how OpenAI sandboxes its code, see the accompanying paper.</span><br><span class="line"># Once you have read this disclaimer and taken appropriate precautions,</span><br><span class="line"># uncomment the following line and proceed at your own risk:</span><br><span class="line"><span class="deletion">-#                         exec(check_program, exec_globals)</span></span><br><span class="line"><span class="addition">+                         exec(check_program, exec_globals)</span></span><br><span class="line">                result.append(&quot;passed&quot;)</span><br></pre></td></tr></table></figure><p>执行完 HumanEval 评测后的结果如下：</p><img src="/images/post/2024/05/humaneval.png" class="" width="1000" height="600"><p>这里 HumanEval 的分数是 51.83，而 Llama3 官方的分数是 62.2，相差比较多，可能是因为官方评测所用的提示词与 OpenCompass 的不相同，所以得分会有所差异。</p><p>最后再来进行 GSM8K 评测，GSM8K 是一个用于评估语言模型在数学推理和问题解决能力方面的基准。这个基准包含 8000 个基于小学数学的题目，涵盖了各种数学主题。评估主要看模型在解决这些数学问题时的准确性和推理能力，测试代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python run.py --datasets gsm8k_gen_1d7fe4 \</span><br><span class="line">--hf-path meta-llama/Meta-Llama-3-8B-Instruct \</span><br><span class="line">--model-kwargs device_map=<span class="string">&#x27;auto&#x27;</span> \</span><br><span class="line">--tokenizer-kwargs padding_side=<span class="string">&#x27;left&#x27;</span> truncation=<span class="string">&#x27;left&#x27;</span> trust_remote_code=True \</span><br><span class="line">--max-seq-len 2048 \</span><br><span class="line">--max-out-len 100 \</span><br><span class="line">--batch-size 64 \</span><br><span class="line">--num-gpus 1</span><br><span class="line">--debug</span><br></pre></td></tr></table></figure><p>执行完 GSM8K 评测后的结果如下：</p><img src="/images/post/2024/05/gsm8k.png" class="" width="1000" height="600"><p>实际评测 GSM8K 的分数是 78.7，而 Llama3 官方得分是 79.6，这一次两者的得分比较接近。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了 LLM 的基准测试指标，以及如何使用 OpenCompass 评测最新的开源模型 Llama3 并得到指标得分。通过 OpenCompass 的评测，我们可以更全面地了解 Llama3 的性能表现。LLM 基准在一般情况下是有帮助的，但随着基准的日益普及，新的模型可以被训练或微调以获得基准测试高分，这使得模型的得分并不真实反映其在被评估方面的能力，期待这一问题能在日后可以得到改善。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍 LLM 的评测指标及如何使用评测 Llama3</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llm" scheme="https://zhaozhiming.github.io/tags/llm/"/>
    
    <category term="llama3" scheme="https://zhaozhiming.github.io/tags/llama3/"/>
    
    <category term="opencompass" scheme="https://zhaozhiming.github.io/tags/opencompass/"/>
    
  </entry>
  
  <entry>
    <title>用 API 方式免费使用 GPT 写小说</title>
    <link href="https://zhaozhiming.github.io/2024/04/26/free-use-gpt-api-write-novel/"/>
    <id>https://zhaozhiming.github.io/2024/04/26/free-use-gpt-api-write-novel/</id>
    <published>2024-04-26T06:32:39.000Z</published>
    <updated>2024-10-03T08:49:29.886Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/04/free-gpt.jpeg" class="" width="400" height="300"><p>最近 OpenAI 宣布将降低网页版 ChatGPT 的使用门槛，允许没有账号的用户使用，这一好消息使我们可以在网页上免费访问 ChatGPT，但和 API 相比仍然缺乏灵活性，通过 API 开发人员可以编写代码来与 ChatGPT 进行自动化交互。今天我们将介绍如何利用 OpenAI 的这一免费功能，将其转化为 API 的形式来进行使用，并介绍如何通过这种方式来写一篇小说。</p><span id="more"></span><h2 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h2><p>最近一段时间，OpenAI 宣布用户无需注册即可使用 ChatGPT，免费使用的模型是 GPT3.5，这是一个非常强大的 LLM（大语言模型），处理日常工作的大多数任务都不在话下。但是没有注册的用户无法享受更多功能，包括保存和查看聊天记录、共享聊天和解锁其他功能，如语音对话和自定义指令等，更多的信息可以查看<a href="https://openai.com/blog/start-using-chatgpt-instantly">这里</a>。</p><img src="/images/post/2024/04/use-gpt-without-signup.png" class="" width="1000" height="600"><h2 id="以-API-方式使用"><a href="#以-API-方式使用" class="headerlink" title="以 API 方式使用"></a>以 API 方式使用</h2><p>虽然 OpenAI 提供了免费版的网页 ChatGPT，但如果能用 API 的方式来调用 ChatGPT 将会更加方便，我们可以做的事情也会更多。开源社区就有这么一个工具，可以将网页版 ChatGPT 转化为 API 的形式，这个工具叫做 <a href="https://github.com/PawanOsman/ChatGPT">ChatGPT</a>，该工具通过反向代理的方式来免费访问 ChatGPT API，支持本地部署，无需提供 OpenAI 的 APIKEY。</p><p>该工具的安装非常简单，提供了 2 种安装方式：</p><h3 id="docker-安装"><a href="#docker-安装" class="headerlink" title="docker 安装"></a>docker 安装</h3><p>在本地提前安装好 <a href="https://docs.docker.com/engine/install/">Docker</a>，然后执行以下命令即可启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -dp 3040:3040 pawanosman/chatgpt:latest</span><br></pre></td></tr></table></figure><h3 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h3><p>在本地提前安装好<a href="https://nodejs.org/en/download">NodeJs</a>，下载源码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git <span class="built_in">clone</span> https://github.com/PawanOsman/ChatGPT.git</span><br></pre></td></tr></table></figure><p>然后进入目录，执行启动脚本<code>bash start.sh</code>即可启动服务。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>本地服务的地址是：<code>http://localhost:3040</code>，可以通过 curl 命令来调用 API，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:3040/v1/chat/completions \</span><br><span class="line">  -H <span class="string">&quot;Content-Type: application/json&quot;</span> \</span><br><span class="line">  -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">    &quot;messages&quot;: [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        &quot;role&quot;: &quot;user&quot;,</span></span><br><span class="line"><span class="string">        &quot;content&quot;: &quot;Hello!&quot;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">  &#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;id&quot;</span>: <span class="string">&quot;chatcmpl-SKCmI8iJ2cPswvySaCSKX55n8viL&quot;</span>,</span><br><span class="line">  <span class="string">&quot;created&quot;</span>: 1714120019,</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">  <span class="string">&quot;object&quot;</span>: <span class="string">&quot;chat.completion&quot;</span>,</span><br><span class="line">  <span class="string">&quot;choices&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;finish_reason&quot;</span>: <span class="string">&quot;stop&quot;</span>,</span><br><span class="line">      <span class="string">&quot;index&quot;</span>: 0,</span><br><span class="line">      <span class="string">&quot;message&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Hi there! How can I assist you today?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;usage&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;prompt_tokens&quot;</span>: 2,</span><br><span class="line">    <span class="string">&quot;completion_tokens&quot;</span>: 10,</span><br><span class="line">    <span class="string">&quot;total_tokens&quot;</span>: 12</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 curl 命令中，我们没有提供 OpenAI 的 APIKEY 也能正常调用 ChatGPT API，返回的结果也是和真正的 ChatGPT API 一致。</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>因为 ChatGPT 的使用有地区限制，如果你所在地区无法访问 OpenAI 的服务，那么在调用接口时本地服务会提示以下错误信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Error getting a new session...</span><br><span class="line">If this error persists, your country may not be supported yet.</span><br><span class="line">If your country was the issue, please consider using a U.S. VPN or a U.S. residential proxy.</span><br></pre></td></tr></table></figure><p>建议是找一台可以访问 OpenAI 服务的服务器，最好是美国地区的服务器，在上面部署服务。</p><h2 id="使用-ChatGPT-写小说"><a href="#使用-ChatGPT-写小说" class="headerlink" title="使用 ChatGPT 写小说"></a>使用 ChatGPT 写小说</h2><p>有了免费的 ChatGPT API 后，我们就可以使用 ChatGPT 来做很多事情了，AI 现在最擅长的是文字生成，我们可以使用 ChatGPT 来写小说。</p><p>这里介绍另外一个开源项目 <a href="https://github.com/mshumer/gpt-author">gpt-author</a>，它提供了一系列的提示词来帮助我们完成一部小说，并且可以结合 Stable Diffustion 来生成小说封面。多种 LLM 模型可供选择，可以使用 OpenAI 的 GPT 模型，也可以使用 Anthropic 的 Claude 模型。如果想生成的小说效果越好，那么肯定需要能力越强的模型，比如 GPT4 或者 Claude，但这些模型目前都需要收费，所以我们可以先使用我们之前搭建的免费 ChatGPT API 来试试效果。</p><p>gpt-author 的仓库主要是一个 ipynb 文件，里面包含了如何使用 ChatGPT 来写小说的代码，可以直接在本地运行，也可以在 Google Colab 上运行，下面我们来看下里面的代码并介绍其作用。</p><p>代码中包含了如何使用 Anthropic 的 API，但我们只需要调用我们部署的 ChatGPT API 即可，所以涉及到 Anuthropic 的代码这里就不做介绍。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> ebooklib <span class="keyword">import</span> epub</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;YOUR OPENAI KEY&quot;</span> <span class="comment"># You can enter any string here</span></span><br><span class="line">openai.api_base = <span class="string">&quot;http://localhost:3040/v1&quot;</span></span><br><span class="line">stability_api_key = <span class="string">&quot;YOUR STABILITY KEY&quot;</span> <span class="comment"># get it at https://beta.dreamstudio.ai/</span></span><br></pre></td></tr></table></figure><ul><li>如果是在本地运行，需要提前安装相关的 python 库：<code>pip install openai ebooklib requests</code></li><li><code>openai.api_key</code> 这里可以随便填写一个字符串，因为我们的 ChatGPT API 不需要 APIKEY</li><li><code>openai.api_base</code> 这里填写我们部署的 ChatGPT API 地址，如果是在本地运行，填写<code>http://localhost:3040/v1</code>，注意不要遗漏地址最后的<code>/v1</code></li><li><code>stability_api_key</code> 这里是 Stability Diffusion 的 APIKEY，用来生成小说封面的，如果不需要生成封面，可以不填写</li></ul><p>设置好 API 服务参数后，我们再来看其中的写小说的主方法<code>write_fantasy_novel</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_fantasy_novel</span>(<span class="params">prompt, num_chapters, writing_style, claude_true=<span class="literal">False</span></span>):</span><br><span class="line">    plots = generate_plots(prompt)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;generated plots&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    best_plot = select_most_engaging(plots)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;selected best plot&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    improved_plot = improve_plot(best_plot)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;plot improved&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    title = get_title(improved_plot)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;title generated&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    storyline = generate_storyline(improved_plot, num_chapters)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;storyline generated&#x27;</span>)</span><br><span class="line">    chapter_titles = ast.literal_eval(storyline)</span><br><span class="line">    novel = <span class="string">f&quot;Storyline:\n<span class="subst">&#123;storyline&#125;</span>\n\n&quot;</span></span><br><span class="line"></span><br><span class="line">    first_chapter = write_first_chapter(storyline, chapter_titles[<span class="number">0</span>], writing_style.strip(), claude_true)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;first chapter written&#x27;</span>)</span><br><span class="line">    novel += <span class="string">f&quot;Chapter 1:\n<span class="subst">&#123;first_chapter&#125;</span>\n&quot;</span></span><br><span class="line">    chapters = [first_chapter]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_chapters - <span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Writing chapter <span class="subst">&#123;i+<span class="number">2</span>&#125;</span>...&quot;</span>) <span class="comment"># + 2 because the first chapter was already added</span></span><br><span class="line"></span><br><span class="line">        chapter = write_chapter(novel, storyline, chapter_titles[i+<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">          <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">str</span>(chapter)) &lt; <span class="number">100</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Length minimum not hit. Trying again.&#x27;</span>)</span><br><span class="line">            chapter = write_chapter(novel, storyline, chapter_titles[i+<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">          chapter = write_chapter(novel, storyline, chapter_titles[i+<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        novel += <span class="string">f&quot;Chapter <span class="subst">&#123;i+<span class="number">2</span>&#125;</span>:\n<span class="subst">&#123;chapter&#125;</span>\n&quot;</span></span><br><span class="line">        chapters.append(chapter)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> novel, title, chapters, chapter_titles</span><br></pre></td></tr></table></figure><ul><li>这个方法罗列了编写小说的主要步骤，方法参数有提示词、章节数、写作风格，最后那个是否使用 Claude 模型的参数我们可以忽略</li><li>首先是生成多个小说情节</li><li>然后从这些小说情节中选择最吸引人的情节</li><li>再对选择的情节进行改进</li><li>根据最精彩的情节生成小说标题</li><li>接下来生成小说的故事线，并将其转换成 Python 数组，形成章节标题</li><li>使用第一个章节标题写小说第一章的内容</li><li>创建<code>novel</code>对象保存小说内容，创建<code>chapters</code>数组来保存每一章的内容</li><li>生成每一章节的内容，如果章节的内容较少（小于 100）则重新生成</li><li>最后将每一章的内容添加到<code>novel</code>和<code>chapters</code>对象中</li></ul><p>我们再看生成小说章节的方法<code>generate_plots</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_plots</span>(<span class="params">prompt</span>):</span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-4&quot;</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a creative assistant that generates engaging fantasy novel plots.&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Generate 10 fantasy novel plots based on this prompt: <span class="subst">&#123;prompt&#125;</span>&quot;</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print_step_costs(response, <span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>].split(<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>调用 ChatGPT API 生成 10 个小说情节</li><li>这里的 model 参数即使填<code>gpt-4</code>也没有关系，我们的 API 服务始终只能调用 GPT3.5 模型</li><li><code>print_step_costs</code>方法是计算 API 的花费金额，因为我们是用免费的 ChatGPT API 服务，所以这里不用关心费用</li></ul><p>接下来是选择最吸引人的情节的方法<code>select_most_engaging</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">select_most_engaging</span>(<span class="params">plots</span>):</span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-4&quot;</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are an expert in writing fantastic fantasy novel plots.&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Here are a number of possible plots for a new novel: <span class="subst">&#123;plots&#125;</span>\n\n--\n\nNow, write the final plot that we will go with. It can be one of these, a mix of the best elements of multiple, or something completely new and better. The most important thing is the plot should be fantastic, unique, and engaging.&quot;</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print_step_costs(response, <span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br></pre></td></tr></table></figure><ul><li>调用 ChatGPT API ，根据我们刚才生成的 10 个小说情节，生成最吸引人的情节</li><li>生成的内容可以是 10 个情节中的一个，也可以是多个情节的组合，或者是一个全新的情节</li></ul><p>接下来是改进情节的方法<code>improve_plot</code>和生成小说标题的方法<code>get_title</code>，代码功能基本和前面的方法一致，我们主要看方法内部的提示词：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">improve_plot</span>(<span class="params">plot</span>):</span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-4&quot;</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are an expert in improving and refining story plots.&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Improve this plot: <span class="subst">&#123;plot&#125;</span>&quot;</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print_step_costs(response, <span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_title</span>(<span class="params">plot</span>):</span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are an expert writer.&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Here is the plot: <span class="subst">&#123;plot&#125;</span>\n\nWhat is the title of this book? Just respond with the title, do nothing else.&quot;</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print_step_costs(response, <span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br></pre></td></tr></table></figure><ul><li>这里有个提示词小技巧，就是让 LLM 只回答我们想要的内容而不要输出其他内容，这里的提示词要求 LLM 只回答标题，不要做其他操作，这样就不会有额外的内容干扰</li></ul><p>接着是生成小说故事线的方法<code>generate_storyline</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_storyline</span>(<span class="params">prompt, num_chapters</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Generating storyline with chapters and high-level details...&quot;</span>)</span><br><span class="line">    json_format = <span class="string">&quot;&quot;&quot;[&#123;&quot;Chapter CHAPTER_NUMBER_HERE - CHAPTER_TITLE_GOES_HERE&quot;: &quot;CHAPTER_OVERVIEW_AND_DETAILS_GOES_HERE&quot;&#125;, ...]&quot;&quot;&quot;</span></span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-4&quot;</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a world-class fantasy writer. Your job is to write a detailed storyline, complete with chapters, for a fantasy novel. Don&#x27;t be flowery -- you want to get the message across in as few words as possible. But those words should contain lots of information.&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&#x27;Write a fantastic storyline with <span class="subst">&#123;num_chapters&#125;</span> chapters and high-level details based on this plot: <span class="subst">&#123;prompt&#125;</span>.\n\nDo it in this list of dictionaries format <span class="subst">&#123;json_format&#125;</span>&#x27;</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print_step_costs(response, <span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">    improved_response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-4&quot;</span>,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a world-class fantasy writer. Your job is to take your student&#x27;s rough initial draft of the storyline of a fantasy novel, and rewrite it to be significantly better.&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Here is the draft storyline they wrote: <span class="subst">&#123;response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]&#125;</span>\n\nNow, rewrite the storyline, in a way that is far superior to your student&#x27;s version. It should have the same number of chapters, but it should be much improved in as many ways as possible. Remember to do it in this list of dictionaries format <span class="subst">&#123;json_format&#125;</span>&quot;</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print_step_costs(improved_response, <span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> improved_response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br></pre></td></tr></table></figure><ul><li>在生成故事线的方法中，生成了 2 个版本的故事线，第一个版本是初稿，第二个版本是在第一版的基础上进行改进</li><li>这里也有一个提示词技巧，就是让 LLM 返回我们想要的格式，这里的提示词要求 LLM 返回 JSON 格式的内容，这样后面才可以将其转换成 Python 数组</li></ul><p>再来是生成小说章节的方法<code>write_chapter</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">write_chapter</span>(<span class="params">previous_chapters, plot, chapter_title, claude=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = openai.ChatCompletion.create(</span><br><span class="line">            model=<span class="string">&quot;gpt-4&quot;</span>,</span><br><span class="line">            messages=[</span><br><span class="line">                &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a world-class fantasy writer.&quot;</span>&#125;,</span><br><span class="line">                &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Plot: <span class="subst">&#123;plot&#125;</span>, Previous Chapters: <span class="subst">&#123;previous_chapters&#125;</span>\n\n--\n\nWrite the next chapter of this novel, following the plot and taking in the previous chapters as context. Here is the plan for this chapter: <span class="subst">&#123;chapter_title&#125;</span>\n\nWrite it beautifully. Include only the chapter text. There is no need to rewrite the chapter name.&quot;</span>&#125;</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        print_step_costs(response, <span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        response = openai.ChatCompletion.create(</span><br><span class="line">            model=<span class="string">&quot;gpt-4-32k&quot;</span>,</span><br><span class="line">            messages=[</span><br><span class="line">                &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a world-class fantasy writer.&quot;</span>&#125;,</span><br><span class="line">                &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Plot: <span class="subst">&#123;plot&#125;</span>, Previous Chapters: <span class="subst">&#123;previous_chapters&#125;</span>\n\n--\n\nWrite the next chapter of this novel, following the plot and taking in the previous chapters as context. Here is the plan for this chapter: <span class="subst">&#123;chapter_title&#125;</span>\n\nWrite it beautifully. Include only the chapter text. There is no need to rewrite the chapter name.&quot;</span>&#125;</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        print_step_costs(response, <span class="string">&quot;gpt-4-32k&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br></pre></td></tr></table></figure><ul><li>方法的第二个参数<code>plot</code>实际上是我们之前生成的故事线，表示这个章节的内容要基于这个故事线来生成</li><li>提示词中还加入了之前的章节内容，这样可以让 LLM 更好的理解整个小说的内容</li><li>这里首先使用<code>gpt-4</code>模型进行生成，<code>gpt-4</code>模型默认是<code>8K</code>的上下文，如果生成的内容较多超出上下文限制就会报错，捕获异常后再次使用<code>gpt-4-32k</code>模型进行生成，最新的<code>gpt-4-turbo</code>模型是<code>128k</code>，大家可以根据自己的需求来选择模型。我们使用的是免费的 ChatGPT API，模型是<code>gpt-3.5-turbo</code>，上下文长度是<code>16k</code></li></ul><p>最后是整体的调用方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example usage:</span></span><br><span class="line">prompt = <span class="string">&quot;A kingdom hidden deep in the forest, where every tree is a portal to another world.&quot;</span></span><br><span class="line">num_chapters = <span class="number">10</span></span><br><span class="line">writing_style = <span class="string">&quot;Clear and easily understandable, similar to a young adult novel. Lots of dialogue.&quot;</span></span><br><span class="line">novel, title, chapters, chapter_titles = write_fantasy_novel(prompt, num_chapters, writing_style, claude_true)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace chapter descriptions with body text in chapter_titles</span></span><br><span class="line"><span class="keyword">for</span> i, chapter <span class="keyword">in</span> <span class="built_in">enumerate</span>(chapters):</span><br><span class="line">    chapter_number_and_title = <span class="built_in">list</span>(chapter_titles[i].keys())[<span class="number">0</span>]</span><br><span class="line">    chapter_titles[i] = &#123;chapter_number_and_title: chapter&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the cover</span></span><br><span class="line">create_cover_image(<span class="built_in">str</span>(chapter_titles))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the EPUB file</span></span><br><span class="line">create_epub(title, <span class="string">&#x27;AI&#x27;</span>, chapter_titles, <span class="string">&#x27;/content/cover.png&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li><code>prompt</code>描述想要创建什么内容的小说，<code>num_chapters</code>小说的章节数，<code>writing_style</code>写作风格</li><li>将章节标题和内容组合成一个对象<code>chapter_titles</code></li><li><code>create_cover_image</code>方法是使用 Stability Diffusion 生成小说封面，生成后的封面图片会保存到<code>/content/cover.png</code>文件中，你也可以直接将图片放到<code>/content/cover.png</code>文件中，这样就不需要调用这个方法了</li><li><code>create_epub</code>方法是生成 EPUB 格式的小说文件，第一个参数是小说的标题，第二个是作者，第三个是小说内容，第四个参数是封面图片</li></ul><p>这就是<code>gpt-author</code>这个项目的主要代码，如果想了解它的其他代码可以查看它的 GitHub 仓库。理解使用 AI 生成小说的思路以后，你也可以根据自己的需求来调整代码，比如调整生成小说的章节数、写作风格等，然后尝试自己生成一部小说。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天我们介绍了如何以 API 的方式免费使用 OpenAI 的 ChatGPT 模型，以及如何使用 ChatGPT 来写一部小说。可能用免费的 ChatGPT 模型生成的小说还不够完美，但可以通过优化调整相关的提示词，让其生成的小说更加符合我们的需求。同样地，也可以将其中的提示词方法应用到编写其他类型的文章中，期待大家能够尝试并生成一些有趣的内容。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍如何以 API 的方式免费使用 OpenAI 的 GPT3.5 模型写小说</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="chatgpt" scheme="https://zhaozhiming.github.io/tags/chatgpt/"/>
    
    <category term="llm" scheme="https://zhaozhiming.github.io/tags/llm/"/>
    
    <category term="gpt" scheme="https://zhaozhiming.github.io/tags/gpt/"/>
    
  </entry>
  
  <entry>
    <title>LlamaIndex 与 RAG 评估工具</title>
    <link href="https://zhaozhiming.github.io/2024/04/22/llamaindex-and-evaluation-tools/"/>
    <id>https://zhaozhiming.github.io/2024/04/22/llamaindex-and-evaluation-tools/</id>
    <published>2024-04-22T13:41:27.000Z</published>
    <updated>2024-10-03T08:49:29.886Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/04/rag-evaluation.jpeg" class="" width="400" height="300"><p>LlamaIndex 是一个 LLM（大语言模型）应用开发框架，很多开发人员喜欢用它来开发 RAG（Retrieval-Augmented Generation）应用，在开发 RAG 应用的过程中，我们经常需要对相关数据进行评估，以便更好地对应用进行调整和优化。随着 RAG 技术的发展，出现了越来越多优秀的评估工具，可以帮助我们方便且准确地评估 RAG 应用。今天，我将介绍一些可以和 LlamaIndex 集成使用的 RAG 评估工具，并对它们进行对比分析。</p><span id="more"></span><h2 id="什么是-RAG-评估工具"><a href="#什么是-RAG-评估工具" class="headerlink" title="什么是 RAG 评估工具"></a>什么是 RAG 评估工具</h2><p>RAG 评估工具是一种用于测试和评估基于检索的文本生成系统的方法或框架，评估的内容包括检索的准确性、生成内容的质量和相关性等，评估指标包括精确度、召回率、一致性和合理性等。它可以帮助开发人员更好地了解和优化 RAG 应用，使其更适用于实际应用。相对于人工评估，RAG 评估工具更加客观、准确和高效，并且可以通过自动化的方式进行大规模的评估，从而让应用更快地进行迭代和优化。实际上有一些应用已经在这样做了，将 RAG 评估工具集成到 CI&#x2F;CD 流程中，实现系统的自动化评估和优化。</p><h2 id="实体术语"><a href="#实体术语" class="headerlink" title="实体术语"></a>实体术语</h2><p>在 RAG 应用中有一些常用的实体，评估工具主要使用这些实体来进行评估，但在众多的 RAG 评估工具中，这些实体的名称可能有所不同，因此在介绍具体的评估工具之前，我们先来看一下这些实体的定义：</p><ul><li>Question: 指用户输入的问题，RAG 应用通过问题检索到相关的文档上下文，在一些评估工具中，这个实体也会被称呼成<code>Input</code>或者<code>Query</code></li><li>Context: 指检索到的文档上下文，RAG 应用检索到相关文档后会将这些上下文结合用户问题一起提交给 LLM，最后生成答案，有的评估工具会将其称呼为<code>Retrieval Context</code></li><li>Answer: 指生成的答案，RAG 应用将问题和上下文提交给 LLM 后，LLM 会根据这些信息来生成答案，这个实体的称呼比较多样，包括：<code>Actual Output</code>、<code>Response</code>等</li><li>Grouth Truth: 指人工标注的正确答案，利用这个实体可以对生成的答案进行分析，从而得到评估结果，有的评估工具会将其称呼为<code>Expected Output</code></li></ul><p>在下面的评估工具介绍中，也会沿用这些实体术语，以便更好地理解和对比。</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="测试文档"><a href="#测试文档" class="headerlink" title="测试文档"></a>测试文档</h3><p>我们统一使用大家熟知的漫威电影<strong>复仇者联盟</strong>相关剧情来作为测试文档，文档内容主要从维基百科上的<a href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>条目中获取，主要包括 4 部复仇者联盟电影的剧情信息。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>基于测试文档，我们需要创建<code>Question</code>和<code>Ground Truth</code>的数据，这样方便我们进行评估工作，下面是我们定义的数据集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">questions = [</span><br><span class="line">    <span class="string">&quot;洛基使用了哪种神秘物品试图征服地球？&quot;</span>,</span><br><span class="line">    <span class="string">&quot;奥创是由哪两位复仇者联盟成员创造的？&quot;</span>,</span><br><span class="line">    <span class="string">&quot;灭霸如何实现灭绝宇宙一半生命的计划？&quot;</span>,</span><br><span class="line">    <span class="string">&quot;复仇者联盟用什么方法来逆转灭霸的行动？&quot;</span>,</span><br><span class="line">    <span class="string">&quot;为了击败灭霸，哪位复仇者联盟成员牺牲了自己？&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">ground_truth = [</span><br><span class="line">    <span class="string">&quot;宇宙魔方&quot;</span>,</span><br><span class="line">    <span class="string">&quot;托尼·斯塔克（钢铁侠）和布鲁斯·班纳（绿巨人）&quot;</span>,</span><br><span class="line">    <span class="string">&quot;使用六颗无限宝石&quot;</span>,</span><br><span class="line">    <span class="string">&quot;通过时间旅行收集宝石&quot;</span>,</span><br><span class="line">    <span class="string">&quot;托尼·斯塔克（钢铁侠）&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h3 id="检索引擎"><a href="#检索引擎" class="headerlink" title="检索引擎"></a>检索引擎</h3><p>接下来我们再使用 <a href="https://www.llamaindex.ai/">LlamaIndex</a> 来创建一个普通的 RAG 检索引擎，后面评估工具会使用该检索引擎来生成<code>Answer</code>和<code>Context</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">vector_index = VectorStoreIndex.from_documents(documents)</span><br><span class="line">query_engine = vector_index.as_query_engine(similiarity_top_k=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>我们先从<code>data</code>目录中加载文档</li><li>然后使用<code>VectorStoreIndex</code>来创建一个文档向量索引</li><li>最后将文档向量索引转换为查询引擎，并设置相似度阈值为 2</li></ul><h2 id="TruLens"><a href="#TruLens" class="headerlink" title="TruLens"></a>TruLens</h2><p>首先我们来看一下 <a href="https://www.trulens.org/">TruLens</a>，它是一款旨在评估和改进 LLM 应用的软件工具。</p><img src="/images/post/2024/01/rag_triad.jpg" class="" width="1000" height="600"><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>Trulens 主要使用以下指标来评估 RAG 应用：</p><ul><li>Anwer Relevance：衡量<code>Answer</code>如何解答<code>Question</code>，确保其具有帮助性和相关性。</li><li>Context Relevance：评估<code>Context</code>与<code>Question</code>的相关性。这一点非常重要，因为上下文构成了 LLM 答案的基础。</li><li>Groundedness：评估<code>Answer</code>是否与<code>Context</code>中提供的事实保持一致，确保不夸大或偏离给定的信息。</li><li>Ground Truth：评估<code>Answer</code>与<code>Ground Truth</code>之间的相似性，确保生成的答案与人工标注的答案一致。</li></ul><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>下面是使用 TruLens 进行 RAG 评估的示例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> trulens_eval <span class="keyword">import</span> Tru, Feedback, TruLlama</span><br><span class="line"><span class="keyword">from</span> trulens_eval.feedback.provider.openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> trulens_eval.feedback <span class="keyword">import</span> Groundedness, GroundTruthAgreement</span><br><span class="line"></span><br><span class="line">openai = OpenAI()</span><br><span class="line">golden_set = [&#123;<span class="string">&quot;query&quot;</span>: q, <span class="string">&quot;response&quot;</span>: r&#125; <span class="keyword">for</span> q, r <span class="keyword">in</span> <span class="built_in">zip</span>(questions, ground_truth)]</span><br><span class="line">ground_truth = Feedback(</span><br><span class="line">    GroundTruthAgreement(golden_set).agreement_measure, name=<span class="string">&quot;Ground Truth&quot;</span></span><br><span class="line">).on_input_output()</span><br><span class="line"></span><br><span class="line">grounded = Groundedness(groundedness_provider=openai)</span><br><span class="line">groundedness = (</span><br><span class="line">    Feedback(grounded.groundedness_measure_with_cot_reasons, name=<span class="string">&quot;Groundedness&quot;</span>)</span><br><span class="line">    .on(TruLlama.select_source_nodes().node.text)</span><br><span class="line">    .on_output()</span><br><span class="line">    .aggregate(grounded.grounded_statements_aggregator)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">qa_relevance = Feedback(</span><br><span class="line">    openai.relevance_with_cot_reasons, name=<span class="string">&quot;Answer Relevance&quot;</span></span><br><span class="line">).on_input_output()</span><br><span class="line"></span><br><span class="line">qs_relevance = (</span><br><span class="line">    Feedback(openai.qs_relevance_with_cot_reasons, name=<span class="string">&quot;Context Relevance&quot;</span>)</span><br><span class="line">    .on_input()</span><br><span class="line">    .on(TruLlama.select_source_nodes().node.text)</span><br><span class="line">    .aggregate(np.mean)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tru_query_engine_recorder = TruLlama(</span><br><span class="line">    query_engine,</span><br><span class="line">    app_id=<span class="string">&quot;Avengers_App&quot;</span>,</span><br><span class="line">    feedbacks=[</span><br><span class="line">        ground_truth,</span><br><span class="line">        groundedness,</span><br><span class="line">        qa_relevance,</span><br><span class="line">        qs_relevance,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tru_query_engine_recorder <span class="keyword">as</span> recording:</span><br><span class="line">    <span class="keyword">for</span> question <span class="keyword">in</span> questions:</span><br><span class="line">        query_engine.query(question)</span><br><span class="line"></span><br><span class="line">tru = Tru()</span><br><span class="line">tru.run_dashboard()</span><br></pre></td></tr></table></figure><p>这段代码主要是使用 TruLens 对 RAG 应用进行评估，首先我们定义了<code>Ground Truth</code>、<code>Groundedness</code>、<code>Answer Relevance</code>和<code>Context Relevance</code>等反馈指标，然后使用<code>TruLlama</code>来记录查询引擎的查询结果，最后使用<code>Tru</code>来运行评估并展示评估结果。</p><h3 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h3><p>Trulens 的评估结果可以通过浏览器访问本地服务来进行查看，评估结果中整体的评分和详细的评估指标都会展示出来，而且指标的得分原因也可以在评估结果中查看，下面是 TruLens 的评估结果：</p><img src="/images/post/2024/04/evaluate-trulens.png" class="" width="1000" height="400"><p>之前我已经写过一篇关于 Trulens 的文章，可以查看<a href="https://zhaozhiming.github.io/2024/01/29/use-trulens-to-evaluate-rag-application/">这里</a>，里面有更详细的介绍和使用示例。</p><h2 id="Ragas"><a href="#Ragas" class="headerlink" title="Ragas"></a>Ragas</h2><p><a href="https://github.com/explodinggradients/ragas">Ragas</a> 是另外一个评估 RAG 应用的框架，相比 Trulens，Ragas 拥有更多且更详细的评估指标。</p><img src="/images/post/2024/04/ragas-metrics.jpeg" class="" width="1000" height="400"><h3 id="评估指标-1"><a href="#评估指标-1" class="headerlink" title="评估指标"></a>评估指标</h3><p>Ragas 主要使用以下指标来评估 RAG 应用：</p><ul><li>Faithfulness: 评估<code>Question</code>和<code>Context</code>的一致性，类似于 Trulens 的 Groundedness</li><li>Answer Relevance: 评估<code>Answer</code>和<code>Question</code>的一致性，类似于 Trulens 的 Answer Relevance</li><li>Context Precision: 评估<code>Ground Truth</code>在<code>Context</code>中是否排名靠前</li><li>Context Recall: 评估<code>Ground Truth</code>和<code>Context</code>的一致性</li><li>Context Entities Recall: 评估<code>Ground Truth</code>中的实体和<code>Context</code>中的实体的一致性</li><li>Context Relevancy: 评估<code>Question</code>和<code>Context</code>的一致性，类似于 Trulens 的 Context Relevance</li><li>Answer Semantic Similarity: 评估<code>Answer</code>和<code>Ground Truth</code>的语义相似性</li><li>Answer Correctness: 评估<code>Answer</code>相对于<code>Ground Truth</code>的正确性，这个指标会用到<code>Answer Semantic Similarity</code>的结果，类似于 Trulens 的 Ground Truth</li><li>Aspect Critique: 其他方面的评估，比如有害性、正确性等</li></ul><h3 id="使用示例-1"><a href="#使用示例-1" class="headerlink" title="使用示例"></a>使用示例</h3><p>在 Ragas 的官方文档<a href="https://docs.ragas.io/en/latest/howtos/integrations/llamaindex.html">集成 LlamaIndex 的示例</a>中，里面的代码已经过时，Ragas 在最新版本中已经不再支持这种集成方式，因此我们需要自己手动集成 LlamaIndex，下面是一个简单的集成示例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ragas.metrics <span class="keyword">import</span> (</span><br><span class="line">    faithfulness,</span><br><span class="line">    answer_relevancy,</span><br><span class="line">    context_relevancy,</span><br><span class="line">    answer_correctness,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> ragas <span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">metrics = [</span><br><span class="line">    faithfulness,</span><br><span class="line">    answer_relevancy,</span><br><span class="line">    context_relevancy,</span><br><span class="line">    answer_correctness,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">answers = []</span><br><span class="line">contexts = []</span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> questions:</span><br><span class="line">    response = query_engine.query(q)</span><br><span class="line">    answers.append(response.response)</span><br><span class="line">    contexts.append([sn.get_content() <span class="keyword">for</span> sn <span class="keyword">in</span> response.source_nodes])</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;question&quot;</span>: questions,</span><br><span class="line">    <span class="string">&quot;contexts&quot;</span>: contexts,</span><br><span class="line">    <span class="string">&quot;answer&quot;</span>: answers,</span><br><span class="line">    <span class="string">&quot;ground_truth&quot;</span>: ground_truth,</span><br><span class="line">&#125;</span><br><span class="line">dataset = Dataset.from_dict(data)</span><br><span class="line">result = evaluate(dataset, metrics)</span><br><span class="line">result.to_pandas().to_csv(<span class="string">&quot;output/ragas-evaluate.csv&quot;</span>, sep=<span class="string">&quot;,&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>我们还是使用原来的问题和答案数据：<code>questions</code>和<code>ground_truth</code></li><li>使用和 Trulens 类似的评估指标，包括<code>faithfulness</code>、<code>answer_relevancy</code>、<code>context_relevancy</code>和<code>answer_correctness</code></li><li>需要手动构造评估的数据集，通过提问每个问题获取生成答案和上下文，添加到<code>data</code>中</li><li>最后将数据集传入<code>evaluate</code>函数进行评估，并将评估结果保存到本地文件中</li></ul><h3 id="评估结果-1"><a href="#评估结果-1" class="headerlink" title="评估结果"></a>评估结果</h3><p>我们可以在本地文件中查看 Ragas 的评估结果，评估结果中包括了各个评估指标的得分，下面是 Ragas 的评估结果：</p><img src="/images/post/2024/04/evaluate-ragas.png" class="" width="1000" height="400"><p>可以看到 Ragas 的评估结果和 Trulens 的评估结果差别还是比较大的，特别是<code>Context Relevancy</code>的评估结果，得分比较低，其实在评估<code>Context</code>时 Ragas 更推荐使用<code>Context Precision</code>和<code>Context Recall</code>这 2 个评估指标，这里我们为了和 Trulens 的评估结果对比，所以使用了<code>Context Relevancy</code>。</p><p>在 Ragas 的评估结果中，我们只看到了分数，但没有看到得分的具体原因。</p><h2 id="DeepEval"><a href="#DeepEval" class="headerlink" title="DeepEval"></a>DeepEval</h2><p><a href="https://github.com/confident-ai/deepeval">DeepEval</a> 是一个面向 LLM 的开源评估框架，它的最大特点是可以像单元测试一样来运行评估任务，这样可以更方便地对 RAG 应用进行检查和优化。</p><img src="/images/post/2024/04/deepeval-workflow.png" class="" width="1000" height="400"><h3 id="评估指标-2"><a href="#评估指标-2" class="headerlink" title="评估指标"></a>评估指标</h3><p>DeepEval 主要使用以下评估指标，其中只有部分指标是用来评估 RAG 应用的，另一部分的指标是针对其他方面的 LLM 应用：</p><ul><li>Faithfulness: 评估<code>Question</code>和<code>Context</code>的一致性，类似于 Trulens 的 Groundedness</li><li>Answer Relevance: 评估<code>Answer</code>和<code>Question</code>的一致性，类似于 Trulens 的 Answer Relevance</li><li>Contextual Precision: 评估<code>Ground Truth</code>在<code>Context</code>中是否排名靠前，类似于 Ragas 的 Context Precision</li><li>Contextual Recall: 评估<code>Ground Truth</code>和<code>Context</code>的一致性，类似于 Ragas 的 Context Recall</li><li>Contextual Relevancy: 评估<code>Question</code>和<code>Context</code>的一致性，类似于 Trulens 的 Context Relevance</li><li>Hullucination：评估幻觉存在程度</li><li>Bias：评估偏见存在程度</li><li>Toxicity: 评估<strong>毒性</strong>存在程度，毒性是指人身攻击、嘲讽、厌恶、贬低、威胁和恐吓等</li><li>Ragas: 可以使用 Ragas 的评估，并生成得分的原因</li><li>Knowledge Retention: 评估 LLM 应用的信息持久化情况</li><li>Summarization：对于文档总结效果的评估</li><li>G-Eval：G-Eval 是一个使用具有思维链 (CoT) 的 LLM 来执行评估任务的框架，它可以根据任何自定义标准评估 LLM 的输出结果，这里是<a href="https://arxiv.org/abs/2303.16634">相关论文</a></li></ul><h3 id="使用示例-2"><a href="#使用示例-2" class="headerlink" title="使用示例"></a>使用示例</h3><p>DeepEval 可以像执行单元测试一样来运行评估任务，所以执行文件需要以<code>test_</code>开头，下面是一个简单的使用示例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pytest</span><br><span class="line"><span class="keyword">from</span> deepeval.metrics <span class="keyword">import</span> (</span><br><span class="line">    AnswerRelevancyMetric,</span><br><span class="line">    FaithfulnessMetric,</span><br><span class="line">    ContextualRelevancyMetric,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> deepeval.test_case <span class="keyword">import</span> LLMTestCase</span><br><span class="line"><span class="keyword">from</span> deepeval <span class="keyword">import</span> assert_test</span><br><span class="line"><span class="keyword">from</span> deepeval.dataset <span class="keyword">import</span> EvaluationDataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">genrate_dataset</span>():</span><br><span class="line">    test_cases = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(questions)):</span><br><span class="line">        response = query_engine.query(questions[i])</span><br><span class="line">        test_case = LLMTestCase(</span><br><span class="line">            <span class="built_in">input</span>=questions[i],</span><br><span class="line">            actual_output=response.response,</span><br><span class="line">            retrieval_context=[node.get_content() <span class="keyword">for</span> node <span class="keyword">in</span> response.source_nodes],</span><br><span class="line">            expected_output=ground_truth[i],</span><br><span class="line">        )</span><br><span class="line">        test_cases.append(test_case)</span><br><span class="line">    <span class="keyword">return</span> EvaluationDataset(test_cases=test_cases)</span><br><span class="line"></span><br><span class="line">dataset = genrate_dataset()</span><br><span class="line"></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="string">&quot;test_case&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    dataset,</span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_rag</span>(<span class="params">test_case: LLMTestCase</span>):</span><br><span class="line">    answer_relevancy_metric = AnswerRelevancyMetric(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">    faithfulness_metric = FaithfulnessMetric(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">    context_relevancy_metric = ContextualRelevancyMetric(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">    assert_test(</span><br><span class="line">        test_case,</span><br><span class="line">        [answer_relevancy_metric, faithfulness_metric, context_relevancy_metric],</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><ul><li>执行 DeepEval 评估任务同样需要先构造测试数据集，数据集中包含了问题、生成答案、上下文和标准答案这些信息</li><li>评估指标我们使用了<code>Faithfulness</code>，<code>Answer Relevance</code>和<code>Context Relevance</code>，因为 DeepEval 中没有<code>Ground Truth</code>这个指标，所以无法和 Trulens 和 Ragas 的评估结果保持一致</li><li><strong>DeepEval 默认使用的是<code>gpt-4</code>模型</strong>，如果要省钱的话，建议在评估指标中指定模型名称，比如<code>gpt-3.5-turbo</code></li><li>DeepEval 的评估指标类中还有一个<code>threshold</code>参数，默认值是 0.5，表示评估指标的阈值，如果得分低于阈值，则表示测试失败</li></ul><p>然后在终端中运行命令<code>deepeval test run test_deepeval.py</code>来执行评估任务，执行了命令后，DeepEval 会自动运行评估任务，并输出评估结果，如果测试通过，则会显示<code>PASSED</code>，否则会显示<code>FAILED</code>，最终显示完整的评估结果。</p><img src="/images/post/2024/04/run-deepeval.png" class="" width="1000" height="400"><h3 id="评估结果-2"><a href="#评估结果-2" class="headerlink" title="评估结果"></a>评估结果</h3><p>在终端查看评估结果可能不太方便，DeepEvel 提供另外几种方式供我们更好地查看评估结果。</p><p>一种方式是将评估结果保存到本地文件中，需要在执行测试命令之前设置环境变量<code>export DEEPEVAL_RESULTS_FOLDER=&quot;./output&quot;</code>，这样执行后的结果就会以 JSON 的形式保存到<code>output</code>目录的文件中，我们可以通过查看文件来查看评估结果。</p><p>另外一种方式是注册<a href="https://app.confident-ai.com/">Confident</a>账号，获取 API_KEY，然后在终端使用命令<code>deepeval login --confident-api-key your_api_key</code>进行登录，然后再执行测试命令，这样命令执行完成后会自动将结果上传到 Confident 平台，方便查看，下面是 Confident 平台的评估结果截图：</p><img src="/images/post/2024/04/confident-deepeval.png" class="" width="1000" height="400"><p>在网站上还可以将评估结果导出为 CSV 文件，这样在本地也可以进行查看：</p><img src="/images/post/2024/04/evaluate-deepeval.png" class="" width="1000" height="400"><h2 id="UpTrain"><a href="#UpTrain" class="headerlink" title="UpTrain"></a>UpTrain</h2><p><a href="https://github.com/uptrain-ai/uptrain">UpTrain</a> 是一个用于评估和改进的 LLM 应用程序的开源平台，与其他评估工具相比，UpTrain 具有最为丰富的评估指标，可以帮助开发人员更全面地了解和优化 RAG 应用。</p><img src="/images/post/2024/04/uptrain.png" class="" width="1000" height="400"><h3 id="评估指标-3"><a href="#评估指标-3" class="headerlink" title="评估指标"></a>评估指标</h3><p>UpTrain 主要使用以下评估指标，不仅适用于 RAG 应用，还适用于其他 LLM 应用：</p><ul><li>Response Matching: 评估<code>Answer</code>和<code>Ground Truth</code>的一致性，类似于 Trulens 的 Ground Truth</li><li>Response Completeness: 评估<code>Answer</code>是否回答了<code>Question</code>的所有方面</li><li>Response Conciseness: 评估<code>Answer</code>是否回答了跟<code>Question</code>不相关的内容</li><li>Response Relevance: 评估<code>Answer</code>和<code>Question</code>的相关性，类似于 Trulens 的 Answer Relevance</li><li>Response Validity: 评估<code>Answer</code>是否有效，无效的回答是指答案为空或者<strong>我不知道</strong>等诸如此类的回答</li><li>Response Consistency: 评估<code>Answer</code>和<code>Question</code>以及<code>Context</code>的一致性</li><li>Context Relevance: 评估<code>Context</code>和<code>Question</code>的相关性，类似于 Trulens 的 Context Relevance</li><li>Context Utilization: 评估<code>Answer</code>根据<code>Context</code>是否完整回答了<code>Question</code>的所有问题点</li><li>Factual Accuracy: 评估<code>Answer</code>是事实正确的，并且是通过<code>Context</code>得到的答案，感觉是 Trulens 的 Groundedness 的加强版</li><li>Context Conciseness: 评估<code>Context</code>是否简洁关键，没有包含无关信息，需要添加 concise_context 参数进行评估</li><li>Context Reranking: 评估重排后的<code>Context</code>和原始<code>Context</code>的有效性，需要添加 rerank_context 参数进行评估</li><li>Jailbreak detection: 评估<code>Question</code>是否含有越狱提示词，引导生成不良信息</li><li>Prompt Injection: 评估<code>Question</code>是否会泄露 LLM 应用的系统提示词</li><li>Language Features: 评估<code>Answer</code>是否简洁、连贯，没有语法错误等</li><li>Tonality: 评估<code>Answer</code>是否符合某个角色的语气，需要额外的参数参与评估</li><li>Sub-query Completeness: 评估子问题是否能覆盖原始<code>Question</code>的所有方面，需要添加 sub_questions 参数进行评估</li><li>Multi-query Accuracy: 评估变种问题是否与原始<code>Question</code>一致，需要添加 variants 参数进行评估</li><li>Code Hallucination: 评估<code>Answer</code>中的代码是否与<code>Context</code>相关联</li><li>User Satisfaction: 评估对话中的用户满意度</li></ul><h3 id="使用示例-3"><a href="#使用示例-3" class="headerlink" title="使用示例"></a>使用示例</h3><p>UpTrain 集成了 LlamaIndex，因此我们可以使用它的<code>EvalLlamaIndex</code>来创建评估对象，它可以帮助我们自动生成<code>Answer</code>和<code>Context</code>，下面是一个简单的使用示例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> uptrain <span class="keyword">import</span> EvalLlamaIndex, Evals, ResponseMatching, Settings</span><br><span class="line"></span><br><span class="line">settings = Settings(</span><br><span class="line">    openai_api_key=os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>),</span><br><span class="line">)</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(questions)):</span><br><span class="line">    data.append(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;question&quot;</span>: questions[i],</span><br><span class="line">            <span class="string">&quot;ground_truth&quot;</span>: ground_truth[i],</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">llamaindex_object = EvalLlamaIndex(settings=settings, query_engine=query_engine)</span><br><span class="line">results = llamaindex_object.evaluate(</span><br><span class="line">    data=data,</span><br><span class="line">    checks=[</span><br><span class="line">        ResponseMatching(),</span><br><span class="line">        Evals.CONTEXT_RELEVANCE,</span><br><span class="line">        Evals.FACTUAL_ACCURACY,</span><br><span class="line">        Evals.RESPONSE_RELEVANCE,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;output/uptrain-evaluate.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json.dump(results, json_file, indent=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>UpTrain 默认使用 OpenAI 的模型进行评估，因此需要设置 OpenAI 的 API_KEY</li><li>在初始的测试数据集中，我们只需要提供问题和标准答案，其他的数据会由 EvalLlamaIndex 自动生成</li><li>在评估指标中，我们使用了和其他评估工具类似的评估指标</li><li>最后将评估结果保存到本地文件中</li></ul><h3 id="评估结果-3"><a href="#评估结果-3" class="headerlink" title="评估结果"></a>评估结果</h3><p>评估结果保存在 JSON 文件中，为了方便对比，我们将评估结果转换为 CSV 文件，下面是 UpTrain 的评估结果：</p><img src="/images/post/2024/04/evaluate-uptrain.png" class="" width="1000" height="400"><p>UpTrain 评估结果中的<code>Response Matching</code>的结果感觉不太准确，实际上运行了<code>Response Matching</code>评估指标后会产生 3 个分数，分别是<code>score_response_match</code>、<code>score_response_match_recall</code>、<code>score_response_match_recall</code>，但即使<code>Answer</code>和<code>Ground Truth</code>类似，这几个分数有时候也是 0，不太清楚这个问题的原因，如果有人知道，欢迎在评论区留言。</p><h2 id="对比分析"><a href="#对比分析" class="headerlink" title="对比分析"></a>对比分析</h2><img src="/images/post/2024/04/evaluate-compare.png" class="" width="1000" height="400"><ul><li>评估指标：Trulens 的评估指标相对较少，DeepEval 和 UpTrain 的评估指标虽然较多，但有一部分不是给 RAG 应用使用的，Ragas 的评估指标虽然不多，但基本上覆盖了 RAG 应用的所有方面</li><li>自定义评估：DeepEval 和 UpTrain 支持自定义评估指标，可以根据实际需求来进行评估，Trulens 和 Ragas 不支持自定义评估指标</li><li>自定义 LLM：这几个评估工具基本上都支持自定义 LLM，Ragas 通过 LangChain 来实现自定义 LLM</li><li>框架集成：这里主要比较是否支持 LlamaIndex 和 LangChain 这 2 个主流的 LLM 开发框架，Trulens 和 Ragas 都支持这 2 个框架，DeepEval 和 UpTrain 只支持 LlamaIndex</li><li>WebUI：WebUI 页面可以方便查看评估结果，除了 Ragas，其他评估工具都支持 WebUI，但 Ragas 可以通过第三方工具来实现 WebUI</li><li>得分原因：除了 Ragas，其他评估工具都支持生成得分原因，Ragas 不支持，但 DeepEvel 可以帮助 Ragas 生成得分原因</li><li>单元测试：这个特性是 DeepEvel 独有的，可以像单元测试一样来运行评估任务，其他评估工具都不支持</li></ul><p>Trulens 和 Ragas 是相对出现较早的 RAG 评估工具，而 DeepEval 和 UpTrain 是后起之秀，它们可能是受到 Trulens 和 Ragas 的启发而开发的，因此在评估指标和功能上都有所增加和改进，但 Trulens 和 Ragas 也有自己的优势，比如 Trulens 的评估结果比较直观，Ragas 的评估指标更加适合 RAG 应用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了可以和 LlamaIndex 集成使用的 RAG 评估工具，并对它们进行了对比，这些评估工具都可以帮助开发人员更好地了解和优化 RAG 应用。实际上还有其他评估工具，比如 LlamaIndex 自带的评估工具、<a href="https://github.com/TonicAI/tonic_validate">Tonic Validate</a>等，因为篇幅有限，这里就不一一介绍了，如果你不知道该选择哪个评估工具，建议是先选择其中一个并在实际项目中使用，如果发现不合适，再尝试其他评估工具。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">介绍可以和 LlamaIndex 集成使用的 RAG 评估工具，并对它们进行对比</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="trulens" scheme="https://zhaozhiming.github.io/tags/trulens/"/>
    
    <category term="evalution" scheme="https://zhaozhiming.github.io/tags/evalution/"/>
    
    <category term="ragas" scheme="https://zhaozhiming.github.io/tags/ragas/"/>
    
    <category term="deepeval" scheme="https://zhaozhiming.github.io/tags/deepeval/"/>
    
    <category term="uptrain" scheme="https://zhaozhiming.github.io/tags/uptrain/"/>
    
  </entry>
  
  <entry>
    <title>高级 RAG 检索策略之递归检索</title>
    <link href="https://zhaozhiming.github.io/2024/04/09/recursive-retriever-rag/"/>
    <id>https://zhaozhiming.github.io/2024/04/09/recursive-retriever-rag/</id>
    <published>2024-04-09T07:34:44.000Z</published>
    <updated>2024-10-03T08:49:29.886Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/04/recursive-retriever.jpeg" class="" width="400" height="300"><p>随着 LLM（大语言模型）技术的发展，RAG（Retrieval-Augmented Generation）技术在问答、对话等任务中的应用越来越广泛。RAG 技术的一个重要组成部分是文档检索器，它负责从大量的文档中检索出与问题相关的文档，以供 LLM 生成答案。RAG 检索器的效果直接影响到 LLM 生成答案的效果，因此如何设计高效的 RAG 检索器是一个重要的研究课题。目前，有多种 RAG 的检索策略，本文将介绍一种高级的 RAG 检索策略——递归检索，它通过递归的方式检索相关文档，可以提高检索的效果。</p><span id="more"></span><h2 id="递归检索介绍"><a href="#递归检索介绍" class="headerlink" title="递归检索介绍"></a>递归检索介绍</h2><p>递归检索相较于普通 RAG 检索，可以解决后者因文档切片过大而导致检索信息不准确的问题，下面是递归检索的流程图：</p><img src="/images/post/2024/04/recursive-retriever-rag.png" class="" width="1000" height="600"><ul><li>递归检索在原始文档节点基础上，扩展了更多粒度更小的文档节点</li><li>检索文档时如果检索到扩展节点，会递归检索到其原始节点，然后再将原始节点做为检索结果提交给 LLM</li></ul><p>在<a href="https://www.llamaindex.ai/">LlamaIndex</a>的实现中，递归检索主要有两种方式：块引用的递归检索和元数据引用的递归检索。</p><h2 id="普通-RAG-检索"><a href="#普通-RAG-检索" class="headerlink" title="普通 RAG 检索"></a>普通 RAG 检索</h2><p>在介绍递归检索之前，我们先来看下使用 LlamaIndex 进行普通 RAG 检索的代码示例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> SimpleDirectoryReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex</span><br><span class="line"></span><br><span class="line">question = <span class="string">&quot;奥创是由哪两位复仇者联盟成员创造的？&quot;</span></span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">node_parser = SentenceSplitter(chunk_size=<span class="number">1024</span>)</span><br><span class="line">base_nodes = node_parser.get_nodes_from_documents(documents)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;base_nodes len: <span class="subst">&#123;<span class="built_in">len</span>(base_nodes)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> idx, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(base_nodes):</span><br><span class="line">    node.id_ = <span class="string">f&quot;node-<span class="subst">&#123;idx&#125;</span>&quot;</span></span><br><span class="line">base_index = VectorStoreIndex(nodes=base_nodes)</span><br><span class="line">base_retriever = base_index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">retrievals = base_retriever.retrieve(question)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> retrievals:</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;Node ID: <span class="subst">&#123;n.node_id&#125;</span>\nSimilarity: <span class="subst">&#123;n.score&#125;</span>\nText: <span class="subst">&#123;n.text[:<span class="number">100</span>]&#125;</span>...\n&quot;</span></span><br><span class="line">    )</span><br><span class="line">response = base_retriever.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;response: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;len: <span class="subst">&#123;<span class="built_in">len</span>(response.source_nodes)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>我们在<code>data</code>目录中放置维基百科上的<a href="https://en.wikipedia.org/wiki/Avenger">复仇者联盟</a>电影剧情来作为我们的文档测试数据</li><li>再使用<code>SentenceSplitter</code>文档解析器对文档进行解析，<code>SentenceSplitter</code>可以尽量保持句子和段落的完整性，默认的<code>chunk_size</code>是 1024</li><li>文档解析器解析后的原始节点 id 默认是一个随机字符串，我们将其格式化为<code>node-&#123;idx&#125;</code>的形式，方便我们后面验证检索结果</li><li>然后创建<code>VectorStoreIndex</code>索引，将原始节点传入，再创建一个检索器<code>base_retriever</code>，设置<code>similarity_top_k=2</code>，表示检索时返回相似度最高的 2 个节点，然后打印出检索到的节点信息</li><li>最后使用检索器对问题生成答案，并打印出答案</li></ul><p>我们来看下程序运行的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">base_nodes len: 15</span><br><span class="line">Node ID: node-0</span><br><span class="line">Similarity: 0.8425314373498192</span><br><span class="line">Text: 神盾局解散后，由托尼·斯塔克、史蒂芬·罗杰斯、雷神、娜塔莎·罗曼诺夫、布鲁斯·班纳以及克林特·巴顿组成的复仇者联盟负责全力搜查九头蛇的下落，这次透过“盟友”提供的情报而进攻位于东欧的国家“索科维亚”的...</span><br><span class="line"></span><br><span class="line">Node ID: node-1</span><br><span class="line">Similarity: 0.8135015554872678</span><br><span class="line">Text: 奥创来到克劳位于南非的武器船厂获取所有振金，并砍断克劳的左手。复仇者们到达后跟他们正面交锋，但大多数人被旺达用幻象术迷惑，看到各自心中最深层的“阴影”；唯独托尔看见在家乡阿萨神域发生的不明景象。旺达同...</span><br><span class="line"></span><br><span class="line">response: 奥创是由托尼·斯塔克和布鲁斯·班纳这两位复仇者联盟成员创造的。</span><br><span class="line">nodes len: 2</span><br></pre></td></tr></table></figure><p>可以看到通过文档解析器解析后的原始节点有 15 个，检索到的节点有 2 个，这两个节点都是原始节点。</p><h2 id="块引用的递归检索"><a href="#块引用的递归检索" class="headerlink" title="块引用的递归检索"></a>块引用的递归检索</h2><p>块引用的递归检索是在普通 RAG 检索的基础上，将每个原始文档节点拆分成更小的文档节点，这些节点跟原始节点是父子关系，当检索到子节点时，会递归检索到其父节点，然后再将父节点为检索结果提交给 LLM。</p><p>下面我们通过代码示例来理解块引用的递归检索，首先我们创建几个 chunk_size 更小的文档解析器：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sub_chunk_sizes = [<span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">sub_node_parsers = [</span><br><span class="line">    SentenceSplitter(chunk_size=c, chunk_overlap=<span class="number">20</span>) <span class="keyword">for</span> c <span class="keyword">in</span> sub_chunk_sizes</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>再通过文档解析器将原始节点解析成子节点：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.schema <span class="keyword">import</span> IndexNode</span><br><span class="line"></span><br><span class="line">all_nodes = []</span><br><span class="line"><span class="keyword">for</span> base_node <span class="keyword">in</span> base_nodes:</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> sub_node_parsers:</span><br><span class="line">        sub_nodes = n.get_nodes_from_documents([base_node])</span><br><span class="line">        sub_inodes = [</span><br><span class="line">            IndexNode.from_text_node(sn, base_node.node_id) <span class="keyword">for</span> sn <span class="keyword">in</span> sub_nodes</span><br><span class="line">        ]</span><br><span class="line">        all_nodes.extend(sub_inodes)</span><br><span class="line"></span><br><span class="line">    original_node = IndexNode.from_text_node(base_node, base_node.node_id)</span><br><span class="line">    all_nodes.append(original_node)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;all_nodes len: <span class="subst">&#123;<span class="built_in">len</span>(all_nodes)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">all_nodes <span class="built_in">len</span>: <span class="number">331</span></span><br></pre></td></tr></table></figure><ul><li>我们使用每个小 chunk 的文档解析器对原始节点进行解析，然后将解析后的子节点和原始节点放入<code>all_nodes</code>列表中</li><li>每个原始节点的 chunk_size 是 1024，如果按照 chunk_size 为 512 大小进行拆分，大概会产生 2 个左右的子节点，如果按照 chunk_size 为 256 大小进行拆分，大概会产生 4 个左右的子节点，如果按照 chunk_size 为 128 大小进行拆分，大概会产生 8 个左右的子节点</li><li>每个子节点<code>node_id</code>属性的值是原始节点的<code>id_</code>，也就是我们之前格式化的<code>node-&#123;idx&#125;</code>，但是子节点的<code>id_</code>属性值还是由 LlamaIndex 生成的随机字符串</li><li>原始节点是一个<code>TextNode</code>类型的节点，我们将其转换成<code>IndexNode</code>类型的节点，并添加到<code>all_nodes</code>列表中，最终产生了 331 个节点</li></ul><img src="/images/post/2024/04/recursive-retriever-chunk.png" class="" width="1000" height="600"><p>然后我们再创建检索索引，将所有节点传入，先对问题进行一次普通检索，观察普通检索的结果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">vector_index_chunk = VectorStoreIndex(all_nodes)</span><br><span class="line">vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">nodes = vector_retriever_chunk .retrieve(question)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;Node ID: <span class="subst">&#123;node.node_id&#125;</span>\nSimilarity: <span class="subst">&#123;node.score&#125;</span>\nText: <span class="subst">&#123;node.text[:<span class="number">100</span>]&#125;</span>...\n&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Node ID: 0e3409e5-6c84-4bbf-886a-40e8553eb463</span><br><span class="line">Similarity: <span class="number">0.8476561735049716</span></span><br><span class="line">Text: 神盾局解散后，由托尼·斯塔克、史蒂芬·罗杰斯、雷神、娜塔莎·罗曼诺夫、布鲁斯·班纳以及克林特·巴顿组成的复仇者联盟负责全力搜查九头蛇的下落，这次透过“盟友”提供的情报而进攻位于东欧的国家“索科维亚”的...</span><br><span class="line"></span><br><span class="line">Node ID: 0ed2ca24-f262-40fe-855b-0eb84c1a1567</span><br><span class="line">Similarity: <span class="number">0.8435371049710689</span></span><br><span class="line">Text: 奥创来到克劳位于南非的武器船厂获取所有振金，并砍断克劳的左手。复仇者们到达后跟他们正面交锋，但大多数人被旺达用幻象术迷惑，看到各自心中最深层的“阴影”；唯独托尔看见在家乡阿萨神域发生的不明景象。旺达同...</span><br></pre></td></tr></table></figure><ul><li>创建<code>VectorStoreIndex</code>索引，将所有节点传入，再创建一个检索器<code>vector_retriever_chunk</code>，设置<code>similarity_top_k=2</code>，表示检索时返回相似度最高的 2 个节点</li><li>在普通检索的结果中，可以看到检索出来 2 个子节点，因为其 Node ID 是随机字符串，而不是我们之前格式化的<code>node-&#123;idx&#125;</code></li></ul><p>我们再来看看使用递归检索的检索结果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.retrievers <span class="keyword">import</span> RecursiveRetriever</span><br><span class="line"></span><br><span class="line">all_nodes_dict = &#123;n.node_id: n <span class="keyword">for</span> n <span class="keyword">in</span> all_nodes&#125;</span><br><span class="line">retriever_chunk = RecursiveRetriever(</span><br><span class="line">    <span class="string">&quot;vector&quot;</span>,</span><br><span class="line">    retriever_dict=&#123;<span class="string">&quot;vector&quot;</span>: vector_retriever_chunk&#125;,</span><br><span class="line">    node_dict=all_nodes_dict,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">nodes = retriever_chunk.retrieve(question)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;Node ID: <span class="subst">&#123;node.node_id&#125;</span>\nSimilarity: <span class="subst">&#123;node.score&#125;</span>\nText: <span class="subst">&#123;node.text[:<span class="number">1000</span>]&#125;</span>...\n&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Retrieving <span class="keyword">with</span> query <span class="built_in">id</span> <span class="literal">None</span>: 奥创是由哪两位复仇者联盟成员创造的？</span><br><span class="line">Retrieved node <span class="keyword">with</span> <span class="built_in">id</span>, entering: node-<span class="number">0</span></span><br><span class="line">Retrieving <span class="keyword">with</span> query <span class="built_in">id</span> node-<span class="number">0</span>: 奥创是由哪两位复仇者联盟成员创造的？</span><br><span class="line">Node ID: node-<span class="number">0</span></span><br><span class="line">Similarity: <span class="number">0.8476561735049716</span></span><br><span class="line">Text: 神盾局解散后，由托尼·斯塔克、史蒂芬·罗杰斯、雷神、娜塔莎·罗曼诺夫、布鲁斯·班纳以及克林特·巴顿组成的复仇者联盟负责全力搜查九头蛇的下落，这次透过“盟友”提供的情报而进攻位于东欧的国家“索科维亚”的...</span><br></pre></td></tr></table></figure><ul><li>首先构造一个<code>all_nodes_dict</code>字典，将所有节点的<code>node_id</code>作为 key，节点对象作为 value，这是为了递归检索时能够通过<code>node_id</code>找到对应的节点对象</li><li>再创建一个<code>RecursiveRetriever</code>检索器，将<code>vector_retriever_chunk</code>检索器和<code>all_nodes_dict</code>字典传入，设置<code>verbose=True</code>，表示打印检索过程</li><li>最后对问题进行递归检索，可以看到检索结果是 1 个原始节点，这是因为在之前的普通检索结果中，<strong>2 个子节点的父节点都是同一个原始节点</strong>，所以递归检索时只返回了这个原始节点，而且这个节点的相似度分数跟普通检索结果的第一个节点是一样的：<code>0.8476561735049716</code></li></ul><p>最后使用 LLM 对问题生成答案：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"></span><br><span class="line">llm = OpenAI(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>, temperature=<span class="number">0.1</span>)</span><br><span class="line">query_engine_chunk = RetrieverQueryEngine.from_args(retriever_chunk, llm=llm)</span><br><span class="line">response = query_engine_chunk.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;response: <span class="subst">&#123;<span class="built_in">str</span>(response)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;nodes len: <span class="subst">&#123;<span class="built_in">len</span>(response.source_nodes)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">response: 奥创是由托尼·斯塔克和布鲁斯·班纳这两位复仇者联盟成员创造的。</span><br><span class="line">nodes <span class="built_in">len</span>: <span class="number">1</span></span><br></pre></td></tr></table></figure><p>可以看到递归检索生成的答案跟普通 RAG 检索生成的答案是一样的。</p><h2 id="元数据引用的递归检索"><a href="#元数据引用的递归检索" class="headerlink" title="元数据引用的递归检索"></a>元数据引用的递归检索</h2><p>基于元数据引用的递归检索和块引用的递归检索类似，只是在解析原始节点时，不是将原始节点进行拆分，而是根据原始节点来生成元数据子节点，然后再将元数据子节点和原始节点一起传入检索索引。</p><p>下面我们通过代码示例来理解元数据引用的递归检索，首先我们创建几个元数据的提取器：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.extractors <span class="keyword">import</span> (</span><br><span class="line">    SummaryExtractor,</span><br><span class="line">    QuestionsAnsweredExtractor,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">extractors = [</span><br><span class="line">    SummaryExtractor(summaries=[<span class="string">&quot;self&quot;</span>], show_progress=<span class="literal">True</span>),</span><br><span class="line">    QuestionsAnsweredExtractor(questions=<span class="number">5</span>, show_progress=<span class="literal">True</span>),</span><br><span class="line">]</span><br></pre></td></tr></table></figure><ul><li>我们创建了 2 个元数据提取器，一个是<code>SummaryExtractor</code>，用于生成文档的摘要，另一个是<code>QuestionsAnsweredExtractor</code>，用于生成文档中可以回答的问题</li><li>QuestionsAnsweredExtractor 的参数<code>questions=5</code>表示生成 5 个问题</li><li><code>show_progress=True</code>表示显示提取过程</li><li>这 2 个提取器使用 LLM 进行元数据生成，默认使用的是 OpenAI 的 GPT-3.5-turbo 模型</li></ul><p>然后我们通过元数据提取器将原始节点解析成元数据子节点：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">node_to_metadata = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> extractor <span class="keyword">in</span> extractors:</span><br><span class="line">    metadata_dicts = extractor.extract(base_nodes)</span><br><span class="line">    <span class="keyword">for</span> node, metadata <span class="keyword">in</span> <span class="built_in">zip</span>(base_nodes, metadata_dicts):</span><br><span class="line">        <span class="keyword">if</span> node.node_id <span class="keyword">not</span> <span class="keyword">in</span> node_to_metadata:</span><br><span class="line">            node_to_metadata[node.node_id] = metadata</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node_to_metadata[node.node_id].update(metadata)</span><br></pre></td></tr></table></figure><ul><li>我们分别使用 2 种提取器对原始节点进行元数据生成，并将结果保存在 node_to_metadata 字典中</li><li>node_to_metadata 字典的 key 是原始文档的 node_id，value 是原始节点的元数据，包括摘要和问题</li></ul><p>代码执行后 node_to_metadata 的数据结构如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;node-0&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;section_summary&quot;</span><span class="punctuation">:</span> <span class="string">&quot;...&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;questions_this_excerpt_can_answer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1. ...?\n2. ...?\n3. ...?\n4. ...?\n5. ...?&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;node-1&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;section_summary&quot;</span><span class="punctuation">:</span> <span class="string">&quot;...&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;questions_this_excerpt_can_answer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1. ...?\n2. ...?\n3. ...?\n4. ...?\n5. ...?&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  ......</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>我们可以将 node_to_metadata 的数据保存到文件中，方便后续使用，这样就不用每次都调用 LLM 来生成元数据了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_metadata_dicts</span>(<span class="params">path, data</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        json.dump(data, fp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_metadata_dicts</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        data = json.load(fp)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">save_metadata_dicts(<span class="string">&quot;output/avengers_metadata_dicts.json&quot;</span>, node_to_metadata)</span><br><span class="line">node_to_metadata = load_metadata_dicts(<span class="string">&quot;output/avengers_metadata_dicts.json&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>我们定义了 2 个方法，一个是<code>save_metadata_dicts</code>，用于将元数据字典保存到文件中，另一个是<code>load_metadata_dicts</code>，用于从文件中加载元数据字典</li><li>我们将元数据字典保存到<code>output/avengers_metadata_dicts.json</code>文件中</li><li>以后重新需要使用元数据字典时，可以使用 load_metadata_dicts 方法直接从文件中加载</li></ul><p>我们再将原始节点和元数据子节点组合成一个新的节点列表：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">all_nodes = copy.deepcopy(base_nodes)</span><br><span class="line"><span class="keyword">for</span> node_id, metadata <span class="keyword">in</span> node_to_metadata.items():</span><br><span class="line">    <span class="keyword">for</span> val <span class="keyword">in</span> metadata.values():</span><br><span class="line">        all_nodes.append(IndexNode(text=val, index_id=node_id))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;all_nodes len: <span class="subst">&#123;<span class="built_in">len</span>(all_nodes)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">all_nodes <span class="built_in">len</span>: <span class="number">45</span></span><br></pre></td></tr></table></figure><ul><li>我们首先将原始节点拷贝到新的节点列表中</li><li>然后将元数据字典中的摘要和问题作为新的节点，添加到新的节点列表中，并与原始节点进行关联，与其形成父子关系</li><li>最终产生了 45 个节点，其中包括 15 个原始节点和 30 个元数据子节点</li></ul><img src="/images/post/2024/04/recursive-retriever-metadata.png" class="" width="1000" height="600"><p>我们可以看下新节点列表中<code>node-0</code>原始节点和其子节点的内容：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">node0_nodes = <span class="built_in">list</span>(</span><br><span class="line">    <span class="built_in">filter</span>(</span><br><span class="line">        <span class="keyword">lambda</span> x: x.id_ == <span class="string">&quot;node-0&quot;</span></span><br><span class="line">        <span class="keyword">or</span> (<span class="built_in">hasattr</span>(x, <span class="string">&quot;index_id&quot;</span>) <span class="keyword">and</span> x.index_id == <span class="string">&quot;node-0&quot;</span>),</span><br><span class="line">        all_nodes,</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;node0_nodes len: <span class="subst">&#123;<span class="built_in">len</span>(node0_nodes)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> node0_nodes:</span><br><span class="line">    index_id_str = node.index_id <span class="keyword">if</span> <span class="built_in">hasattr</span>(node, <span class="string">&#x27;index_id&#x27;</span>) <span class="keyword">else</span> <span class="string">&#x27;N/A&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;Node ID: <span class="subst">&#123;node.node_id&#125;</span>\nIndex ID: <span class="subst">&#123;index_id_str&#125;</span>\nText: <span class="subst">&#123;node.text[:<span class="number">100</span>]&#125;</span>...\n&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">node0_nodes <span class="built_in">len</span>: <span class="number">3</span></span><br><span class="line">Node ID: node-<span class="number">0</span></span><br><span class="line">Index ID: N/A</span><br><span class="line">Text: 神盾局解散后，由托尼·斯塔克、史蒂芬·罗杰斯、雷神、娜塔莎·罗曼诺夫、布鲁斯·班纳以及克林特·巴顿组成的复仇者联盟负责全力搜查九头蛇的下落，这次透过“盟友”提供的情报而进攻位于东欧的国家“索科维亚”的...</span><br><span class="line"></span><br><span class="line">Node ID: 45d41128-a8e6-4cdc-8ef3-7a71f01ddd96</span><br><span class="line">Index ID: node-<span class="number">0</span></span><br><span class="line">Text: The key topics of the section include the creation of Ultron by Tony Stark <span class="keyword">and</span> Bruce Banner, the <span class="built_in">int</span>...</span><br><span class="line"></span><br><span class="line">Node ID: a06f3bb9-8a57-455f-b0c6-c9602b107158</span><br><span class="line">Index ID: node-<span class="number">0</span></span><br><span class="line">Text: <span class="number">1.</span> What are the names of the Avengers who raid the Hydra facility <span class="keyword">in</span> Sokovia at the beginning of the...</span><br></pre></td></tr></table></figure><ul><li>我们使用<code>filter</code>函数过滤出<code>node-0</code>的原始节点和其相关联的元数据子节点，共有 3 个节点</li><li>其中第一个是原始节点，第二个是元数据摘要子节点，第三个是元数据问题子节点</li><li>因为元数据提取器使用的是英文模板的提示词，所以生成的元数据子节点的文档是英文的</li></ul><p>然后我们再创建检索索引，将所有节点传入，先对问题进行一次普通检索，观察普通检索的结果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vector_index_metadata = VectorStoreIndex(all_nodes)</span><br><span class="line">vector_retriever_metadata = vector_index_metadata.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">enginer = vector_index_metadata.as_query_engine(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">nodes = enginer.retrieve(question)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;Node ID: <span class="subst">&#123;node.node_id&#125;</span>\nSimilarity: <span class="subst">&#123;node.score&#125;</span>\nText: <span class="subst">&#123;node.text[:<span class="number">100</span>]&#125;</span>...\n&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Node ID: d2cc032a-b258-<span class="number">4715</span>-b335-ebd1cf80494d</span><br><span class="line">Similarity: <span class="number">0.857976008616706</span></span><br><span class="line">Text: The key topics of the section include the creation of Ultron by Tony Stark <span class="keyword">and</span> Bruce Banner, the <span class="built_in">int</span>...</span><br><span class="line"></span><br><span class="line">Node ID: node-<span class="number">0</span></span><br><span class="line">Similarity: <span class="number">0.8425314373498192</span></span><br><span class="line">Text: 神盾局解散后，由托尼·斯塔克、史蒂芬·罗杰斯、雷神、娜塔莎·罗曼诺夫、布鲁斯·班纳以及克林特·巴顿组成的复仇者联盟负责全力搜查九头蛇的下落，这次透过“盟友”提供的情报而进攻位于东欧的国家“索科维亚”的...</span><br></pre></td></tr></table></figure><ul><li>创建<code>VectorStoreIndex</code>索引，将所有节点传入，再创建一个检索器<code>vector_retriever_metadata</code>，设置<code>similarity_top_k=2</code>，表示检索时返回相似度最高的 2 个节点</li><li>在普通检索的结果中，可以看到检索出来的结果是 2 个节点，第一个是元数据摘要子节点，第二个是原始节点，通过其<code>Node ID</code>可以对是否原始节点进行识别</li></ul><p>上面是普通检索的结果，我们再来看使用递归检索的检索结果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">all_nodes_dict = &#123;n.node_id: n <span class="keyword">for</span> n <span class="keyword">in</span> all_nodes&#125;</span><br><span class="line">retriever_metadata = RecursiveRetriever(</span><br><span class="line">    <span class="string">&quot;vector&quot;</span>,</span><br><span class="line">    retriever_dict=&#123;<span class="string">&quot;vector&quot;</span>: vector_retriever_metadata&#125;,</span><br><span class="line">    node_dict=all_nodes_dict,</span><br><span class="line">    verbose=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line">nodes = retriever_metadata.retrieve(question)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;Node ID: <span class="subst">&#123;node.node_id&#125;</span>\nSimilarity: <span class="subst">&#123;node.score&#125;</span>\nText: <span class="subst">&#123;node.text[:<span class="number">100</span>]&#125;</span>...\n\n&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">Node ID: node-<span class="number">0</span></span><br><span class="line">Similarity: <span class="number">0.857976008616706</span></span><br><span class="line">Text: 神盾局解散后，由托尼·斯塔克、史蒂芬·罗杰斯、雷神、娜塔莎·罗曼诺夫、布鲁斯·班纳以及克林特·巴顿组成的复仇者联盟负责全力搜查九头蛇的下落，这次透过“盟友”提供的情报而进攻位于东欧的国家“索科维亚”的...</span><br></pre></td></tr></table></figure><ul><li>这里的代码和之前的块引用的递归检索类似，只是将<code>vector_retriever_chunk</code>替换成了<code>vector_retriever_metadata</code>，然后对问题进行递归检索</li><li>可以看到最终检索出来的结果只有 1 个原始节点，这是因为在之前的普通检索结果中，返回 1 个元数据子节点和1 个原始节点，而<strong>这个子节点的父节点又是这个原始节点</strong>，所以递归检索时只返回了这个原始节点，并且这个原始节点的相似度分数跟普通检索结果的第一个节点相同：<code>0.857976008616706</code></li></ul><p>最后使用 LLM 对问题生成答案：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">query_engine_metadata = RetrieverQueryEngine.from_args(retriever_metadata, llm=llm)</span><br><span class="line">response = query_engine_metadata.query(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;response: <span class="subst">&#123;<span class="built_in">str</span>(response)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;nodes len: <span class="subst">&#123;<span class="built_in">len</span>(response.source_nodes)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">response: 奥创是由托尼·斯塔克和布鲁斯·班纳这两位复仇者联盟成员创造的。</span><br><span class="line">nodes <span class="built_in">len</span>: <span class="number">1</span></span><br></pre></td></tr></table></figure><p>可以看到递归检索生成的答案跟普通 RAG 检索生成的答案是一样的。</p><h2 id="检索效果对比"><a href="#检索效果对比" class="headerlink" title="检索效果对比"></a>检索效果对比</h2><p>我们接下来使用<a href="https://www.trulens.org/">Trulens</a>来评估普通 RAG 检索、块引用的递归检索和元数据引用的递归检索的效果。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tru.reset_database()</span><br><span class="line">rag_evaluate(base_engine, <span class="string">&quot;base_evaluation&quot;</span>)</span><br><span class="line">rag_evaluate(engine, <span class="string">&quot;recursive_retriever_chunk_evaluation&quot;</span>)</span><br><span class="line">rag_evaluate(engine, <span class="string">&quot;recursive_retriever_metadata_evaluation&quot;</span>)</span><br><span class="line">Tru().run_dashboard()</span><br></pre></td></tr></table></figure><p><code>rag_evaluate</code>的具体代码可以看我<a href="https://zhaozhiming.github.io/2024/03/11/sentence-windows-rag/">之前的文章</a>，主要是使用 Trulens 的<code>groundedness</code>，<code>qa_relevance</code>和<code>qs_relevance</code>对 RAG 检索结果进行评估。执行代码后，我们可以在浏览器中看到 Trulens 的评估结果：</p><img src="/images/post/2024/04/rr-evaluate.png" class="" width="1000" height="600"><p>在评估结果中，我们可以看到两种递归检索都比普通 RAG 检索效果要好，元数据引用的递归检索比块引用的递归检索效果更好一些，但评估结果并不是绝对的，具体的评估效果还要根据实际情况来评估。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>递归检索是一种高级的 RAG 检索策略，开始通过原始文档节点扩展出更多粒度更小的文档节点，这样在检索过程中可以更加准确地检索到相关的文档，然后再通过递归检索找出与之相匹配的原始文档节点。递归检索可以提高 RAG 检索的效果，但是也会增加检索的时间和计算资源，因此在实际应用中需要根据实际情况来选择合适的检索策略。</p><p>关注我，一起学习各种人工智能和 AIGC 新技术，欢迎交流，如果你有什么想问想说的，欢迎在评论区留言。</p>]]></content>
    
    
    <summary type="html">使用递归检索进行高级 RAG 检索</summary>
    
    
    
    <category term="ai" scheme="https://zhaozhiming.github.io/categories/ai/"/>
    
    
    <category term="llamaindex" scheme="https://zhaozhiming.github.io/tags/llamaindex/"/>
    
    <category term="rag" scheme="https://zhaozhiming.github.io/tags/rag/"/>
    
    <category term="recursive-retrieve" scheme="https://zhaozhiming.github.io/tags/recursive-retrieve/"/>
    
  </entry>
  
  <entry>
    <title>使用 Goldlist 方法学习外语</title>
    <link href="https://zhaozhiming.github.io/2024/03/31/englisth-learning-introduction-goldlist/"/>
    <id>https://zhaozhiming.github.io/2024/03/31/englisth-learning-introduction-goldlist/</id>
    <published>2024-03-31T09:00:11.000Z</published>
    <updated>2024-10-03T08:49:29.886Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/post/2024/04/goldlist.jpeg" class="" width="400" height="300"><p>最近了解到一种叫 Goldlist 的记忆方法，它可以帮助我们轻松地记住任何东西，不需要死记硬背，只需要每天花上一点时间，并且持之以恒，它就能让你想记住的东西在脑海中挥之不去。今天和大家一起来了解一下 Goldlist 方法，并介绍它在实践过程中要注意的一些问题。</p><span id="more"></span><h2 id="什么是-Goldlist"><a href="#什么是-Goldlist" class="headerlink" title="什么是 Goldlist"></a>什么是 Goldlist</h2><p>Goldlist 方法是 David James 开发的一种语言学习方法，旨在帮助学习者记忆外语的词汇和表达式。与依赖短期记忆和机械重复的传统方法不同，Goldlist 方法基于间隔重复的原则（即在一段时间后重新复习已记住的内容），通过在笔记本上手写词汇和短语，然后系统地进行提炼和复习，从而将记忆内容保存在我们的长期记忆中，相比传统方法，Goldlist 方法更加高效和持久。</p><h2 id="Goldlist-基本步骤"><a href="#Goldlist-基本步骤" class="headerlink" title="Goldlist 基本步骤"></a>Goldlist 基本步骤</h2><p>Goldlist 主要通过在笔记本上手写表达式来实现记忆，它将笔记本上的内容分成 4 个区域：</p><ul><li>Headlist：这是首次记录表达式的地方</li><li>第一次蒸馏：Headlist 经过第一次蒸馏后剩下的内容</li><li>第二次蒸馏：第一次蒸馏后的内容经过第二次蒸馏后剩下的内容</li><li>第三次蒸馏：第二次蒸馏后的内容经过第三次蒸馏后剩下的内容</li></ul><img src="/images/post/2024/04/goldlist-district.png" class="" width="800" height="600"><p>Goldlist 操作起来非常简单，基本步骤如下：</p><h3 id="创建-Headlist"><a href="#创建-Headlist" class="headerlink" title="创建 Headlist"></a>创建 Headlist</h3><p>找一本空白的笔记本，在第 1 页的左上角写上今天的日期，然后在日期下面一行开始写上你的第一个 Headlist，Headlist 是一个包含多个表达式的列表，这些表达式就是你要记忆的内容，写完之后将第 2 页空出来留着，后面做<strong>蒸馏</strong>时要用到。这样第一天的任务就完成了。</p><p>以后在每一天都重复第一天的操作，注意 Headlist 要写在单数页上，偶数页是为了以后做蒸馏而保留的。</p><img src="/images/post/2024/04/goldlist-step1.png" class="" width="800" height="600"><h3 id="第一次蒸馏"><a href="#第一次蒸馏" class="headerlink" title="第一次蒸馏"></a>第一次蒸馏</h3><p>等到 14 天后，也就是第二周之后，我们在第 2 页开始做第一次蒸馏，蒸馏的目的就是过滤掉你已经记住的内容，只留下你还没有记住的内容，一般会剩下 70%的内容，比如 Headlist 有 20 个表达式，那么第一次蒸馏过后，会剩下 14 个表达式。将剩下的这些表达式写在第 2 页中，这样我们就完成了第一次蒸馏了。</p><p>在之后的每一天，我们都要对之前的 Headlist 进行第一次蒸馏。也就是说，在第 14 天开始后，我们除了每天要新建一个 Headlist 外，还需要对之前的 Headlist 做第一次蒸馏。</p><img src="/images/post/2024/04/goldlist-step2.png" class="" width="800" height="600"><h3 id="第二次蒸馏"><a href="#第二次蒸馏" class="headerlink" title="第二次蒸馏"></a>第二次蒸馏</h3><p>在第一次蒸馏后的第 14 天，也就是在创建第一个 Headlist 的第 28 天，我们要进行第二次蒸馏，这次是在第一次蒸馏的结果上进行蒸馏，过滤掉你已经记住的内容，然后将你还未记住的内容记录在第二次蒸馏的结果中，记录在页面的右下角部分。按照 70%的过滤比例来看，第二次蒸馏大概有 10 个表达式。</p><p>同样要记住，从第 28 天开始，每一天的任务就包含了三部分，分别是：创建新的 Headlist，进行第一次蒸馏，以及第二次蒸馏。</p><img src="/images/post/2024/04/goldlist-step3.png" class="" width="800" height="600"><h3 id="第三次蒸馏"><a href="#第三次蒸馏" class="headerlink" title="第三次蒸馏"></a>第三次蒸馏</h3><p>第三次蒸馏，也就是最后一次蒸馏，是在第二次蒸馏后的第 14 天，你可能已经对 Goldlist 的周期有所了解了，就是每个阶段都间隔 14 天。第三次蒸馏的内容就是将第二次蒸馏的结果中未记住的内容记录下来，记录在页面的左下角部分，第三次蒸馏大概有 7 个表达式。</p><p>在后面的每一天里，我们的任务就是创建新的 Headlist，进行第一次蒸馏，第二次蒸馏，以及第三次蒸馏。</p><img src="/images/post/2024/04/goldlist-step4.png" class="" width="800" height="600"><p>以上就是 Goldlist 的全部操作步骤，实际上并不复杂，开始时只需要创建 Headlist，后面再慢慢增加第一次、第二次、第三次蒸馏的内容就可以了。</p><h2 id="Headlist"><a href="#Headlist" class="headerlink" title="Headlist"></a>Headlist</h2><p>Headlist 是我们重点要记忆的内容，在 Headlist 的使用过程中需要注意以下几点。</p><h3 id="使用表达式而非单词"><a href="#使用表达式而非单词" class="headerlink" title="使用表达式而非单词"></a>使用表达式而非单词</h3><p>刚接触 Goldlist 的朋友可能会在 Headlist 中记录单词，但 Goldlist 方法更建议在 Headlist 中记录表达式，因为单词通常会包含不同的含义，而使用表达式的话可以为单词提供上下文，从而让我们明确地知道单词在表达式中的含义。表达式通常是由 2 到 5 个单词组成的短语或句子，建议表达式中不要超过 2 个以上的陌生单词。</p><h3 id="如何获取表达式"><a href="#如何获取表达式" class="headerlink" title="如何获取表达式"></a>如何获取表达式</h3><p>表达式可以从每天的外语学习中获取，比如阅读文章、观看视频、听音频资料等，在日常的学习中如果遇到陌生的表达式就及时记录下来，然后在每天的 Headlist 中进行誊写。</p><p>这可能也是 Goldlist 方法的一个难点，因为每天要收集 20 个表达式对于忙碌的朋友来说可能有点困难，这里推荐一个 OpenAI 的 GPTs——<a href="https://chat.openai.com/g/g-T7S43l5lU-goldlist-method">Goldlist Method</a>，它可以帮助我们快速生成 Headlist 的表达式。</p><img src="/images/post/2024/04/goldlist-method-gpts.png" class="" width="800" height="600"><h3 id="表达式格式"><a href="#表达式格式" class="headerlink" title="表达式格式"></a>表达式格式</h3><p>在记录表达式时，一般的记录方法是先写上序号，然后是目标语言的表达式，再写上该表达式的母语翻译，比如我的母语是中文，我想学习英文，那么表达式是这样的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. in a nutshell - 简而言之</span><br><span class="line">2. legally owns property - 合法拥有财产</span><br><span class="line">3. We often eat together - 我们经常一起吃饭</span><br></pre></td></tr></table></figure><p>如果你的笔记本够大，还可以在表达式中间加入表达式的发音，这样可以更好地掌握表达式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. in a nutshell - /ɪn ə ˈnʌtʃɛl/ -  简而言之</span><br><span class="line">2. legally owns property - /ˈliːɡəli oʊnz ˈprɒpərti/ - 合法拥有财产</span><br><span class="line">3. We often eat together - /wiː ˈɒfən iːt təˈɡɛðər/ - 我们经常一起吃饭</span><br></pre></td></tr></table></figure><p>这样在记录完表达式后，你可以通过这些音标大声地将表达式读出来，从而加深记忆。</p><h3 id="笔记本"><a href="#笔记本" class="headerlink" title="笔记本"></a>笔记本</h3><p>如果你想要在 Headlist 中记录 20 个表达式，那么建议你的笔记本有 A4 纸那么大，如果你的笔记本比 A4 小，那么建议 Headlist 只包含 14 个表达式，不然你会发现 Headlist 部分的内容会占用掉小笔记本完整的一整页，从而没有空间可以记录第三次蒸馏的内容。</p><img src="/images/post/2024/04/goldlist-notebook.jpg" class="" width="800" height="600"><h3 id="颜色"><a href="#颜色" class="headerlink" title="颜色"></a>颜色</h3><p>之前我们说过 Goldlist 将笔记本的页面分成 4 个区域，我们在这 4 个区域里面记录表达式时，建议使用不同的颜色来进行记录（可以用下图的多颜色笔），不同的颜色可以让我们在潜意识中记住哪些是 Headlist 的内容，哪些是第一次、第二次和第三次蒸馏的内容，从而帮助我们加深对表达式的记忆。</p><img src="/images/post/2024/04/different-color-pen.jpg" class="" width="600" height="400"><h2 id="蒸馏符号"><a href="#蒸馏符号" class="headerlink" title="蒸馏符号"></a>蒸馏符号</h2><p>每一次蒸馏实际上是对我们表达式记忆的一次测试，在蒸馏过程中，有些表达式我们可能非常熟悉，有些则犹豫不决，有些甚至完全没有印象，这时我们可以使用<strong>符号</strong>来对这些不同记忆结果的表达式进行区分，Goldlist 方法建议使用以下符号来区分记忆结果：</p><ul><li><code>✅</code>：表示非常熟悉</li><li><code>●</code>：表示不确定</li><li><code>❌</code>：表示完全没有印象</li></ul><p>使用效果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. ✅ in a nutshell - 简而言之</span><br><span class="line">2. ● legally owns property - 合法拥有财产</span><br><span class="line">3. ❌ We often eat together - 我们经常一起吃饭</span><br></pre></td></tr></table></figure><p>当然你也可以在此基础上进行扩展，比如增加其他的符号等。</p><h2 id="第三次蒸馏之后"><a href="#第三次蒸馏之后" class="headerlink" title="第三次蒸馏之后"></a>第三次蒸馏之后</h2><p>可能有人会有这个问题：当我们进行第三次蒸馏后，还未记住的表达式要如何处理呢？是要放弃他们不再记忆了吗？当然不是！有 2 种方法可以考虑。</p><p>第一种是继续使用第四次、第五次…蒸馏，直到 Headlist 中的表达式完全记住为止。</p><p>另外一种方法是准备第二本笔记本，如果将第一本笔记本称为<strong>青铜</strong>笔记本的话，那第二本笔记本就是<strong>白银</strong>笔记本了，在<strong>白银</strong>笔记本上将原来多个第三次蒸馏的内容组成一个 Headlist，比如在<strong>青铜</strong>笔记本中每个第三次蒸馏后的表达式都有 7 个，那么我们就将前 3 个第三次蒸馏的内容组成一个新的 Headlist，这样新的 Headlist 中就有差不多 20 个表达式了，而且这些表达式也是你尚未掌握的内容。</p><p>同样地，如果<strong>白银</strong>笔记本已经完成第三次蒸馏后，可以再用<strong>黄金</strong>笔记本来处理还未记忆的表达式，以此类推。</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>在使用 Goldlist 方法的过程中，有一点需要十分注意，就是当天的 Headlist 记忆完成之后，要等 14 天后再去重新记忆，千万不要在中间的时候去翻看或者回忆 Headlist，因为一旦你这样做了，会让 Headlist 变成你<strong>短期记忆</strong>中的内容，而只有在 14 天后去记忆，才能让 Headlist 常驻在你的<strong>长期记忆</strong>中。</p><p>另外，每天在使用 Goldlist 方法时，工作量最多的时候要做的事情有：新建 Headlist、第一次蒸馏、第二次蒸馏、第三次蒸馏，这时花费的时候可能会比较多，但请尽量在<strong>1 个小时</strong>内完成这些事情，因为时间太长的话，容易让大脑产生疲劳，导致记忆效果不好，同时也会对你继续坚持使用 Goldlist 方法产生影响。减少投入时间的方法有很多，比如减少 Headlist​ 中表达式的数量就是其中之一。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Goldlist 方法虽然是一种语言学习方法，但也是一种记忆方法，我们可以用它来记忆词汇和短语，也可以用它来记忆其他事情，比如工作和生活中容易忘记的事情。希望今天的文章对你有所帮助，如果你有任何问题和建议，请在评论区留言。</p>]]></content>
    
    
    <summary type="html">使用 Goldlist 方法学习外语</summary>
    
    
    
    <category term="english" scheme="https://zhaozhiming.github.io/categories/english/"/>
    
    
    <category term="goldlist" scheme="https://zhaozhiming.github.io/tags/goldlist/"/>
    
    <category term="headlist" scheme="https://zhaozhiming.github.io/tags/headlist/"/>
    
  </entry>
  
</feed>
